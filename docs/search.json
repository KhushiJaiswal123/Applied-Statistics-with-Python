[
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course is an introduction to using R/Rstudio to learn from data. It aims to teach useful skills: importing, data filtering and manipulation, visualization, inferential statistics, and data modelling.\nBusiness decisions are often too complex to be made by intuition alone. We need to communicate the structure of our reasoning, defend it to adversarial challenge and deliver presentations that show we have done a thorough analysis. We also need to understand and make use of various sources of data, organise the inputs of experts and colleagues, and use R/RStudio to provide analytical support to our reasoning. The overall objective of this course is to equip you with analytical thinking and techniques that help you be more effective in these tasks. The goal is to teach you how to perform data analysis to support decision-making, build simple but powerful models that test your intuitive reasoning, develop managerial thinking and facilitate the communication of your recommendations.\nBy the end of the course you should be able to identify the areas where data analytics can add the most value, select appropriate types of analyses and apply them in a small-scale, quick-turnaround but high-impact fashion.\n\nLearning Objectives\n\nArticulate, extract and analyse valuable information from data\nUnderstand and quantify the accuracy of sample evidence\nBuild regression models to describe and predict complicated outcomes\nCommunicate quantitative analysis and recommendations effectively with RMarkdown\nBe able to use R and R studio effectively for data analysis and decision making\n\n\n\nRequired Texts or Readings\nWe will be drawing on the following online Textbooks:\n\nModernDrive: Statistical Interface Via Data Science\nR for Data Science\nGetting Used to R, RStudio, and RMarkdown\n\n\n\nAssessment Policy\nGrades will be based on the following:\n\n\n\n\n\n\n\n\n\nAssessment Type\nDue\nWeight\nGroup/Individual\n\n\n\n\nHomework in R Markdown (3*5%)\n29th Aug, 3rd Sep, 8th Sep\n15%\nGroup\n\n\nGroup Project\n11 Sep 2024\n30%\nGroup\n\n\nFinal Exam, online ?, timed\n20 Sep 2024\n55%\nIndividual\n\n\n\nAssignments are due by 11:59 PM UTC on the day they are due.\n\n\nDetailed Course Structure\nHere is a detailed schedule for this class.\n\n\nWords of Encouragement\nLearning R can be challenging at first— it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2—made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors.\n # The 15 Minute rule\nIt’s good practice to follow the 15 minute rule. If you encounter a problem in your work, spend 15 minutes troubleshooting the problem on your own; Google, RStudio Support, and StackOverflow are good places to look for answers. So if you google your error message, you will find that 99% of the time someone has had the same error message and the solution is on stackoverflow.\nHowever, if after 15 minutes you still cannot solve the problem, please ask for help – post a question on Slack, email me, reach out to a friend.\n\n\n15 min rule: when stuck, you HAVE to try on your own for 15 min; after 15 min, you HAVE to ask for help. - Brain AMA\n\n— Rachel Thomas (@math_rachel), August 15, 2016"
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Resources",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Examples.html",
    "href": "Examples.html",
    "title": "Examples",
    "section": "",
    "text": "Contents\n\n\n\n\n\n\nExploratory Data Analysis\nData Modelling\n\n\n\n\nThis section contains worked examples, most of them with fully annotated Python code that you can use as reference.\n\n\nBefore starting any analysis, we first need to import a dataset, understand its variables, visualize it, and manipulate it systematically using tools like pandas, matplotlib, and seaborn. This might seem like a tedious step, but it’s a critical foundation that must precede any form of statistical modelling.\n\n\n\nData modelling allows us to move beyond describing individual variables — instead, we use models to learn from data. At the core of this is understanding the relationship:\n# General form of a predictive model\noutcome = f(features) + error\nWe begin with Exploratory Data Analysis tailored for modelling, and then proceed with three key approaches:\n\nEDA for Modelling After importing and cleaning the data (using pandas), we start looking at summary statistics and plots that will be useful in framing our modelling approach\nTesting for Differences in Means across samples: How do we know whether there is a statistically significant difference between two groups A and B? E.g., between those who took a drug versus those than a placebo? Or whether there is a difference in the percentage of people who approve of Donald Trump is lower than those who disapprove of him?\nFitting a Linear Regression Model To understand which features are associated with a numerical outcome Y, we use Linear Regression from scikit-learn. We try to explain the effect that specific explanatory variables, X, have on Y\nFitting a Binary Classification Model where the difference is that the outcome variable, Y, is binary (0/1). Again we want to use our model primarily for explanation, e.g., what is the effect of different explanatory variables X’s on e.g., the probability that someone with Covid-19 will die?"
  },
  {
    "objectID": "Examples.html#import-data",
    "href": "Examples.html#import-data",
    "title": "Examples",
    "section": "1. Import Data",
    "text": "1. Import Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\n\nImporting CSV files: pandas.read_csv()\nImporting CSV files saved locally\n\nNeed for speed: dask and vaex\nOther data formats\nNever work directly on the raw data\nother Links\n\n\n\n\n\nLearning Objectives\n\nLoad external data from a .csv file into a data frame.\nDescribe what a data frame is.\nUse indexing to subset specific portions of data frames.\nDescribe what a factor is.\nReorder and rename factors.\nFormat dates.\n\n\n\nOverview\nWhen working with Python, data importation is generally achieved using libraries like pandas, which provides powerful tools for data manipulation, including importing data from various file formats.\n\n\n\nImporting CSV files: pandas.read_csv()\nCSV files can be imported using the read_csv() function from the pandas library. This function is fast and user-friendly, allowing you to read flat data without complex configurations.\nimport pandas as pd\n\n# Importing CSV file from a URL\nurl = \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\"\nweather = pd.read_csv(url, skiprows=1, na_values=\"***\")\nOptions Used: * skiprows=1: This skips the first row, assuming the actual data starts from the second row. * na_values= \"***\" : This treats astriks’ as missing values, converting them to NaN.\nTo view the structure of the dataframe, you can use:\nprint(weather.info())\nprint(weather.head())\n\n\n\nImporting CSV files saved locally\nTo import a CSV file saved locally, simply provide the file path to the read_csv() function:\nweather_local = pd.read_csv(\"path/to/your/localfile.csv\")\n\n\n\nNeed for speed: dask and vaex\nFor handling large datasets, libraries like dask and vaex can be used, which offer faster data processing capabilities compared to pandas.\nimport dask.dataframe as dd\n\nweather_large = dd.read_csv(\"path/to/largefile.csv\")\n\n\n\nOther data formats\nPython offers several libraries for reading and writing various data formats:\n\nExcel files: pandas.read_excel()\nJSON: json library\nWeb APIs: requests library\nDatabases: sqlite3\nBig Data: pyspark\n\n\n\n\nNever work directly on the raw data\nIn 2012 Cecilia Giménez, an 83-year-old widow and amateur painter, attempted to restore a century-old fresco of Jesus crowned with thorns in her local church in Borja, Spain. The restoration didn’t go very well, but, surprisingly, the botched restoration of Jesus fresco miraculously saved the Spanish Town.\n\nAs a most important rule, please do not work on the raw data; it’s unlikely you will have Cecilia Giménez’s good fortune to become (in)famous for your not-so-brilliant work.\nMake sure you always work on a copy of your raw data. Use Python’s data manipulation libraries to clean and transform your data, and save the results to a new file, ensuring the original data remains intact.\nweather_cleaned = weather.dropna()  # Example of cleaning data\nweather_cleaned.to_csv(\"cleaned_data.csv\", index=False)\n\n\n\nOther Links\n\nFor Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights\nData Wrangling with pandas: Pandas Documentation\nBig Data Processing with dask and vaex: Dask Documentation, Vaex Documentation"
  },
  {
    "objectID": "Examples.html#inspect-data",
    "href": "Examples.html#inspect-data",
    "title": "Examples",
    "section": "2. Inspect Data",
    "text": "2. Inspect Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\nViewing Data\nDetailed Inspection\nKey Questions\n\n\n\n\n\n\nOverview\nOnce you have loaded your dataset into Python, it’s essential to inspect and understand the data. Typically, you want to know:\n\nThe dimensions of the dataset (number of rows and columns).\nThe types of variables (integer, string, boolean, etc.).\nThe number of missing values.\nSummary statistics for numeric data.\n\n\n\n\nViewing Data\nIn Python, you can use pandas to view and inspect your data. The pandas library provides several functions to achieve this.\npandas.DataFrame.info() and pandas.DataFrame.describe()\nThese functions help you understand the structure and summary statistics of your data.\nimport pandas as pd\n\n# Load data\ngapminder = pd.read_csv(\"path/to/gapminder.csv\")\n\n# View the structure of the dataframe\nprint(gapminder.info())\n\n# Summary statistics\nprint(gapminder.describe())\nUsing info(): Provides the number of rows, columns, and data types of each column. Using describe() : Offers summary statistics for numeric columns, including count, mean, standard deviation, min, and max values.\n\n\n\nDetailed Inspection\nFor a detailed inspection, you can use the pandas_profiling library, which provides an extensive report on your dataframe.\nfrom pandas_profiling import ProfileReport\n\n# Generate a report\nprofile = ProfileReport(gapminder)\nprofile.to_file(\"gapminder_report.html\")\nExample Analysis on London Bikes Data\nbikes = pd.read_csv(\"path/to/londonBikes.csv\")\n\n# Use pandas_profiling for a detailed report\nbikes_profile = ProfileReport(bikes)\nbikes_profile.to_file(\"london_bikes_report.html\")\n\n\n\nKey Questions\nusing the London Bikes Data from above\n\nWhat kind of variable is date?\n\ndate is typically a string or datetime variable. You can convert it using pd.to_datetime().\n\nbikes['date'] = pd.to_datetime(bikes['date'])\nWhat kind of variable is season?\n\nseason is likely a categorical variable. You can convert it using pd.Categorical().\n\nbikes['season'] = pd.Categorical(bikes['season'])\nHow often does it rain in London?\n\nCount the occurrences in the rain column.\n\nrain_count = bikes['rain'].sum()\nprint(f\"It rains {rain_count} times in the dataset.\")\nWhat is the average annual temperature (in degrees C)?\n\nCalculate the mean of the temperature columns.\n\navg_temp = bikes[['max_temp', 'min_temp', 'avg_temp']].mean().mean()\nprint(f\"The average annual temperature is {avg_temp:.2f} degrees C.\")\nWhat is the maximum rainfall?\n\nFind the maximum value in the rainfall_mm column.\n\nmax_rainfall = bikes['rainfall_mm'].max()\nprint(f\"The maximum rainfall recorded is {max_rainfall} mm.\")"
  },
  {
    "objectID": "Examples.html#clean-data",
    "href": "Examples.html#clean-data",
    "title": "Examples",
    "section": "3. Clean Data",
    "text": "3. Clean Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\nCleaning Variable Names with pandas\nCode Quality\nOther Links\n\n\n\n\n\n\nOverview\nWhen creating data files, it’s common to use variable names and formats that are human-readable but not ideal for computational processing. In Python, the pandas library can be used to clean and standardise variable names for easier manipulation.\n\n\n\nCleaning Variable Names with pandas\nIn Python, we can use pandas and custom code to read data and clean column names to make them more suitable for analysis.\nimport pandas as pd\n\n# Load Excel file\nroster = pd.read_excel(\"path/to/dirty_data.xlsx\")\n\n# Clean column names using pandas string methods\nroster.columns = (\n    roster.columns\n        .str.strip()  # Remove leading and trailing spaces\n        .str.lower()  # Convert to lowercase\n        .str.replace(' ', '_')  # Replace spaces with underscores\n        .str.replace('%', 'percent')  # Replace '%' with 'percent'\n        .str.replace('[^a-zA-Z0-9_]', '', regex=True)  # Remove special characters\n)\n\n# Inspect cleaned dataframe\nprint(roster.head())\n\nThe custom code directly modifies the column names using pandas string methods. It removes spaces, converts to lowercase, replaces spaces with underscores, and removes special characters using a regular expression.\nRegular Expression: [^a-zA-Z0-9_] is used to remove any character that is not alphanumeric or an underscore.\npandas.read_excel(): Used to read Excel files into a DataFrame.\n\n\n\n\nCode Quality\nAccording to Phil Karlton, there are only two hard things in Computer Science: cache invalidation and naming things. It’s crucial to write code that is not only functional but also maintainable and readable. Use meaningful names for variables and dataframes, and include comments to explain complex logic.\n\n\n\nOther Links\n\nFor Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights"
  },
  {
    "objectID": "Examples.html#visualise-data",
    "href": "Examples.html#visualise-data",
    "title": "Examples",
    "section": "4. Visualise Data",
    "text": "4. Visualise Data\ngapminder Dataset"
  },
  {
    "objectID": "Examples.html#manipulate-data",
    "href": "Examples.html#manipulate-data",
    "title": "Examples",
    "section": "5. Manipulate Data",
    "text": "5. Manipulate Data"
  },
  {
    "objectID": "Examples.html#reshape-data",
    "href": "Examples.html#reshape-data",
    "title": "Examples",
    "section": "6. Reshape Data",
    "text": "6. Reshape Data"
  },
  {
    "objectID": "Examples.html#eda-for-modelling",
    "href": "Examples.html#eda-for-modelling",
    "title": "Examples",
    "section": "7. EDA for Modelling",
    "text": "7. EDA for Modelling"
  },
  {
    "objectID": "Assignments.html",
    "href": "Assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment details\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nWeekly Homeworks\nFinal Project\nFinal Exam\n\n\n\n\n\n\nWeekly Homeworks\nTo practice writing R code, exploring datasets, and running statistical models you will complete a series of weekly homeworks.\nThere are 3 weekly homeworks to be undertaken with your group and these will be graded using a check system:\n\n5/5: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often.\n3/5: Problem set is 70–99% complete and most answers are correct. This is the expected level of performance.\n1/5: Problem set is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often.\n\n\n\n\nFinal Project\nAt the end of the course, you will demonstrate your knowledge by working on a group assignment… Complete details for the final project will be made available here.\n\n\n\nFinal Exam\nThere will be a final exam that will cover: * 1. Programming in R and the tidyverse (about a third of total marks) * the core statistical tools of inferential statistics and linear models.\nYou will take the final exam in-person in a lecture theatre. This is a closed-book, 1.5-hour exam. A formula sheet will be provided, but you will not have online access to Google.\n\n\n\n\nHomeworks\n\n\nHomework 1\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nhomework1.Rmd\nInstruction\nRubric\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nhomework1.Rmd\nInstruction\nRubric\n\n\n\n\n\n\n\nHomework 3\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nhomework1.Rmd\nInstruction\nRubric\n\n\n\n\n\n\n\n\nProjects\n\n\nFinal Group Project\n\n\n\n\n\n\nContents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Exam\n\n\n\n\n\n\nContents\n\n\n\n\n\n\n\n\n\n\n\n\nDue by 11:59 PM on Friday, September 20, 2024"
  },
  {
    "objectID": "Content.html",
    "href": "Content.html",
    "title": "Content",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before the lecture or working through the lesson.\nI have included a set of questions that might guide your reflection response. You should not try to respond to all of these (or any of them if you don’t want to)– they’ll just help you know what to look for and think about as you read."
  },
  {
    "objectID": "Content.html#workshop-1-import-visualise-and-manipulate-data",
    "href": "Content.html#workshop-1-import-visualise-and-manipulate-data",
    "title": "Content",
    "section": "Workshop 1: Import, visualise, and manipulate data",
    "text": "Workshop 1: Import, visualise, and manipulate data\n\nRead before class on Wednesday, August 28, 2024\n\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nReadings\n\nRecommended\n\nQuestions to Reflect on\nSession Files\n\n\n\n\n\nReadings\n\nHans Rosling, “200 Countries, 200 Years, 4 Minutes”\nChapter 2 in Claus Wilke, Fundamentals of Data Visualization\nChapter 3 in Kieran Healy, Data Visualization\n\n\nRecommended\n\nThe fullest look yet at the racial inequality of Coronavirus\n\n\n\n\nQuestions to Reflect on\n\nWhat data was mapped to which aesthetics in Rosling’s video?\nWhat data would you need to create the bar plot in NYT’s article?\n\n\n\nSession Files\n\nSlides for today’s session are available on Canvas.\nYou can download all session files (data, code, etc.) by pulling from course Github repo.\n\ngit clone https://github.com/kostis-christodoulou/am01.git\nAlternatively, download the zipped files directly from here. Then unzip and open them in your Python IDE (e.g., Jupyter, VS Code, or Spyder)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics with Python",
    "section": "",
    "text": "Instructor\n\n\n\n\nCourse details\n\n\n\n\nContact me"
  },
  {
    "objectID": "Schedule.html",
    "href": "Schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the course\n\nContent (): This contains the readings, slides, data files, etc. for each session. These will also be added on Canvas on the day of each session. It helps to read the material before each session.\nExample () : This page contains worked examples of fully annotated R code that you can use as a reference. This is only a reference page—you don’t have to necessarily do anything here.\nExercise (): These are interactive exercises where you have to provide R code in your browser to solve a problem, much like Datacamp. These are not graded, but are always there for your reference.\nAssignment (): This page contains instructions for the three workshop exercises (3-4 brief tasks plus a challenge), for the individual portfolio website project, and the final group project.\n\nAssignments are due by 11:59 PM UTC on the day they’re listed.\n\n\n\nWeek\nDate\nSession\nContent\nExample\nExercise\nAssignment\n\n\n\n\nFoundations: EDA and Intro to Data Science\n\n\n\n\n\n\n\n\n1\n28th Aug\nLecture 1: Exploratory Data Analysis\n()\n()\n()\n-\n\n\n2\n28th Aug\nWorkshop 1: Import, visualise, and manipulate data\n()\n()\n()\n-\n\n\n\n29th Aug\nHomework 1 due\n-\n-\n-\n()\n\n\nInferential Statistics\n\n\n\n\n\n\n\n\n3\n30th Aug\nLecture 2: Sampling and Probability Distributions\n()\n()\n()\n-\n\n\n4\n30th Aug\nWorkshop 2: Confidence Intervals; reshape data\n()\n()\n()\n-\n\n\n\n03rd Sep\nHomework 2 due\n-\n-\n-\n()\n\n\n5\n04th Sep\nLecture 3: Hypothesis Testing; there is only one test\n()\n()\n()\n-\n\n\n6\n04th Sep\nWorkshop 3: Hypothesis testing; A/B testing; simulating\n()\n()\n-\n-\n\n\n\n08th Sep\nHomework 3 due\n-\n-\n-\n()\n\n\nRegression Modelling\n\n\n\n\n\n\n\n\n7\n09th Sep\nLecture 4: Introduction to regression models\n()\n()\n-\n-\n\n\n8\n09th Sep\nWorkshop 4: Workshop on regression\n()\n()\n-\n-\n\n\n9\n11th Sep\nLecture 5: Further regression; regression diagnostics\n()\n()\n-\n-\n\n\n10\n11th Sep\nFinal group project\n()\n-\n-\n-\n\n\n\n11th Sep\nFinal Group Project due\n-\n-\n-\n()\n\n\n\n18th Sep\nRevision Session\n-\n-\n-\n-\n\n\n\n20th Sep\nFinal Exam\n-\n-\n-\n()"
  }
]