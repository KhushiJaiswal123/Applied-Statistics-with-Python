[
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course is an introduction to using R/Rstudio to learn from data. It aims to teach useful skills: importing, data filtering and manipulation, visualization, inferential statistics, and data modelling.\nBusiness decisions are often too complex to be made by intuition alone. We need to communicate the structure of our reasoning, defend it to adversarial challenge and deliver presentations that show we have done a thorough analysis. We also need to understand and make use of various sources of data, organise the inputs of experts and colleagues, and use R/RStudio to provide analytical support to our reasoning. The overall objective of this course is to equip you with analytical thinking and techniques that help you be more effective in these tasks. The goal is to teach you how to perform data analysis to support decision-making, build simple but powerful models that test your intuitive reasoning, develop managerial thinking and facilitate the communication of your recommendations.\nBy the end of the course you should be able to identify the areas where data analytics can add the most value, select appropriate types of analyses and apply them in a small-scale, quick-turnaround but high-impact fashion.\n\nLearning Objectives\n\nArticulate, extract and analyse valuable information from data\nUnderstand and quantify the accuracy of sample evidence\nBuild regression models to describe and predict complicated outcomes\nCommunicate quantitative analysis and recommendations effectively with RMarkdown\nBe able to use R and R studio effectively for data analysis and decision making\n\n\n\nRequired Texts or Readings\nWe will be drawing on the following online Textbooks:\n\nModernDrive: Statistical Interface Via Data Science\nR for Data Science\nGetting Used to R, RStudio, and RMarkdown\n\n\n\nAssessment Policy\nGrades will be based on the following:\n\n\n\n\n\n\n\n\n\nAssessment Type\nDue\nWeight\nGroup/Individual\n\n\n\n\nHomework in R Markdown (3*5%)\n29th Aug, 3rd Sep, 8th Sep\n15%\nGroup\n\n\nGroup Project\n11 Sep 2024\n30%\nGroup\n\n\nFinal Exam, online ?, timed\n20 Sep 2024\n55%\nIndividual\n\n\n\nAssignments are due by 11:59 PM UTC on the day they are due.\n\n\nDetailed Course Structure\nHere is a detailed schedule for this class.\n\n\nWords of Encouragement\nLearning R can be challenging at first— it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2—made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors.\n # The 15 Minute rule\nIt’s good practice to follow the 15 minute rule. If you encounter a problem in your work, spend 15 minutes troubleshooting the problem on your own; Google, RStudio Support, and StackOverflow are good places to look for answers. So if you google your error message, you will find that 99% of the time someone has had the same error message and the solution is on stackoverflow.\nHowever, if after 15 minutes you still cannot solve the problem, please ask for help – post a question on Slack, email me, reach out to a friend.\n\n\n15 min rule: when stuck, you HAVE to try on your own for 15 min; after 15 min, you HAVE to ask for help. - Brain AMA\n\n— Rachel Thomas (@math_rachel), August 15, 2016"
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Resources",
    "section": "",
    "text": "Contents\n\n\n\n\n\n\nInstalling Python\nInstalling vs Code\nOther tools"
  },
  {
    "objectID": "Resources.html#installing",
    "href": "Resources.html#installing",
    "title": "Resources",
    "section": "Installing Python",
    "text": "Installing Python\nTo install Python:\n\nDownload from the official website: python.org\nRecommended version: 3.10 or higher\nFor Mac: Consider using Homebrew\n\nFor Windows: Use the Microsoft Store or installer"
  },
  {
    "objectID": "Resources.html#code",
    "href": "Resources.html#code",
    "title": "Resources",
    "section": "Installing VS Code",
    "text": "Installing VS Code\nSteps to install VS Code:\n\nDownload from: code.visualstudio.com\nRecommended extensions:\n\nPython\nQuarto\nGitLens\nJupyter"
  },
  {
    "objectID": "Resources.html#tools",
    "href": "Resources.html#tools",
    "title": "Resources",
    "section": "Other Tools",
    "text": "Other Tools\nComing soon…"
  },
  {
    "objectID": "Resources.html#github",
    "href": "Resources.html#github",
    "title": "Resources",
    "section": "Using GitHub",
    "text": "Using GitHub\nTo get started with GitHub:\n\nSign up at github.com\nLearn basic commands:\n\ngit clone\ngit add\ngit commit\ngit push\n\nTry GitHub Desktop if you’re not comfortable with the command line interface (CLI)"
  },
  {
    "objectID": "Resources.html#textbooks",
    "href": "Resources.html#textbooks",
    "title": "Resources",
    "section": "Textbooks",
    "text": "Textbooks\n\ntest"
  },
  {
    "objectID": "Resources.html#videos",
    "href": "Resources.html#videos",
    "title": "Resources",
    "section": "Videos",
    "text": "Videos\n\ntest"
  },
  {
    "objectID": "Resources.html#finance",
    "href": "Resources.html#finance",
    "title": "Resources",
    "section": "Finance Data",
    "text": "Finance Data\n\nYahoo Finance: finance.yahoo.com\nAlpha Vantage: alphavantage.co\nQuandl: quandl.com"
  },
  {
    "objectID": "Resources.html#worldBank",
    "href": "Resources.html#worldBank",
    "title": "Resources",
    "section": "World Bank",
    "text": "World Bank\n\nWorld Bank Open Data: data.worldbank.org\nWDI R package: WDI on CRAN"
  },
  {
    "objectID": "Examples.html",
    "href": "Examples.html",
    "title": "Examples",
    "section": "",
    "text": "Contents\n\n\n\n\n\n\nThe Basics\nExploratory Data Analysis\nData Modelling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nVariables and Data Types\nBasic Operations\nData Structures\nControl flow\n\n\n\n\nThis section covers fundamental Python concepts that are essential for working with data and understanding the examples in the rest of this course. If you’re new to Python, spend some time here to get comfortable with the syntax and common operations.\n\n\n\n\nWhat are variables? (Containers for storing data)\nCommon Data Types:\n\nint (Integers: 5, -10)\nfloat (Floating-point numbers: 3.14, -0.5)\nstr (Strings: 'hello', \"Python Course\")\nbool (Booleans: True, False)\n\nHow to assign variables: my_number = 10, my_text = \"Data\"\nChecking data type: type(my_number)\n\n# Example\nage = 30\ntemperature = 25.5\nname = \"Alice\"\nis_student = True\n\nprint(f\"Age: {age}, Type: {type(age)}\")\nprint(f\"Temperature: {temperature}, Type: {type(temperature)}\")\nprint(f\"Name: {name}, Type: {type(name)}\")\nprint(f\"Is Student: {is_student}, Type: {type(is_student)}\")\n\n\n\n\nArithmetic Operators: +, -, *, /, ** (exponentiation), % (modulo)\nComparison Operators: ==, !=, &lt;, &gt;, &lt;=, &gt;=\nLogical Operators: and, or, not\n\n# Example\nresult = 10 + 5 * 2\nprint(f\"Calculation: {result}\") # Output: 20\n\nis_greater = (result &gt; 15) and (age &lt; 40)\nprint(f\"Is greater and age within range: {is_greater}\")\n\nThis section next section contains worked examples, most of them with fully annotated Python code that you can use as reference.\n\n\n\n\nBefore starting any analysis, we first need to import a dataset, understand its variables, visualize it, and manipulate it systematically using tools like pandas, matplotlib, and seaborn. This might seem like a tedious step, but it’s a critical foundation that must precede any form of statistical modelling.\n\n\n\nData modelling allows us to move beyond describing individual variables — instead, we use models to learn from data. At the core of this is understanding the relationship:\n# General form of a predictive model\noutcome = f(features) + error\nWe begin with Exploratory Data Analysis tailored for modelling, and then proceed with three key approaches:\n\nEDA for Modelling After importing and cleaning the data (using pandas), we start looking at summary statistics and plots that will be useful in framing our modelling approach\nTesting for Differences in Means across samples: How do we know whether there is a statistically significant difference between two groups A and B? E.g., between those who took a drug versus those than a placebo? Or whether there is a difference in the percentage of people who approve of Donald Trump is lower than those who disapprove of him?\nFitting a Linear Regression Model To understand which features are associated with a numerical outcome Y, we use Linear Regression from scikit-learn. We try to explain the effect that specific explanatory variables, X, have on Y\nFitting a Binary Classification Model where the difference is that the outcome variable, Y, is binary (0/1). Again we want to use our model primarily for explanation, e.g., what is the effect of different explanatory variables X’s on e.g., the probability that someone with Covid-19 will die?"
  },
  {
    "objectID": "Examples.html#import-data",
    "href": "Examples.html#import-data",
    "title": "Examples",
    "section": "1. Import Data",
    "text": "1. Import Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\n\nImporting CSV files: pandas.read_csv()\nImporting CSV files saved locally\n\nNeed for speed: dask and vaex\nOther data formats\nNever work directly on the raw data\nother Links\n\n\n\n\n\nLearning Objectives\n\nLoad external data from a .csv file into a data frame.\nDescribe what a data frame is.\nUse indexing to subset specific portions of data frames.\nDescribe what a factor is.\nReorder and rename factors.\nFormat dates.\n\n\n\nOverview\nWhen working with Python, data importation is generally achieved using libraries like pandas, which provides powerful tools for data manipulation, including importing data from various file formats.\n\n\n\nImporting CSV files: pandas.read_csv()\nCSV files can be imported using the read_csv() function from the pandas library. This function is fast and user-friendly, allowing you to read flat data without complex configurations.\nimport pandas as pd\n\n# Importing CSV file from a URL\nurl = \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\"\nweather = pd.read_csv(url, skiprows=1, na_values=\"***\")\nOptions Used: * skiprows=1: This skips the first row, assuming the actual data starts from the second row. * na_values= \"***\" : This treats astriks’ as missing values, converting them to NaN.\nTo view the structure of the dataframe, you can use:\nprint(weather.info())\nprint(weather.head())\n\n\n\nImporting CSV files saved locally\nTo import a CSV file saved locally, simply provide the file path to the read_csv() function:\nweather_local = pd.read_csv(\"path/to/your/localfile.csv\")\n\n\n\nNeed for speed: dask and vaex\nFor handling large datasets, libraries like dask and vaex can be used, which offer faster data processing capabilities compared to pandas.\nimport dask.dataframe as dd\n\nweather_large = dd.read_csv(\"path/to/largefile.csv\")\n\n\n\nOther data formats\nPython offers several libraries for reading and writing various data formats:\n\nExcel files: pandas.read_excel()\nJSON: json library\nWeb APIs: requests library\nDatabases: sqlite3\nBig Data: pyspark\n\n\n\n\nNever work directly on the raw data\nIn 2012 Cecilia Giménez, an 83-year-old widow and amateur painter, attempted to restore a century-old fresco of Jesus crowned with thorns in her local church in Borja, Spain. The restoration didn’t go very well, but, surprisingly, the botched restoration of Jesus fresco miraculously saved the Spanish Town.\n\nAs a most important rule, please do not work on the raw data; it’s unlikely you will have Cecilia Giménez’s good fortune to become (in)famous for your not-so-brilliant work.\nMake sure you always work on a copy of your raw data. Use Python’s data manipulation libraries to clean and transform your data, and save the results to a new file, ensuring the original data remains intact.\nweather_cleaned = weather.dropna()  # Example of cleaning data\nweather_cleaned.to_csv(\"cleaned_data.csv\", index=False)\n\n\n\nOther Links\n\nFor Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights\nData Wrangling with pandas: Pandas Documentation\nBig Data Processing with dask and vaex: Dask Documentation, Vaex Documentation"
  },
  {
    "objectID": "Examples.html#inspect-data",
    "href": "Examples.html#inspect-data",
    "title": "Examples",
    "section": "2. Inspect Data",
    "text": "2. Inspect Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\nViewing Data\nDetailed Inspection\nKey Questions\n\n\n\n\n\n\nOverview\nOnce you have loaded your dataset into Python, it’s essential to inspect and understand the data. Typically, you want to know:\n\nThe dimensions of the dataset (number of rows and columns).\nThe types of variables (integer, string, boolean, etc.).\nThe number of missing values.\nSummary statistics for numeric data.\n\n\n\n\nViewing Data\nIn Python, you can use pandas to view and inspect your data. The pandas library provides several functions to achieve this.\npandas.DataFrame.info() and pandas.DataFrame.describe()\nThese functions help you understand the structure and summary statistics of your data.\nimport pandas as pd\n\n# Load data\ngapminder = pd.read_csv(\"path/to/gapminder.csv\")\n\n# View the structure of the dataframe\nprint(gapminder.info())\n\n# Summary statistics\nprint(gapminder.describe())\nUsing info(): Provides the number of rows, columns, and data types of each column. Using describe() : Offers summary statistics for numeric columns, including count, mean, standard deviation, min, and max values.\n\n\n\nDetailed Inspection\nFor a detailed inspection, you can use the pandas_profiling library, which provides an extensive report on your dataframe.\nfrom pandas_profiling import ProfileReport\n\n# Generate a report\nprofile = ProfileReport(gapminder)\nprofile.to_file(\"gapminder_report.html\")\nExample Analysis on London Bikes Data\nbikes = pd.read_csv(\"path/to/londonBikes.csv\")\n\n# Use pandas_profiling for a detailed report\nbikes_profile = ProfileReport(bikes)\nbikes_profile.to_file(\"london_bikes_report.html\")\n\n\n\nKey Questions\nusing the London Bikes Data from above\n\nWhat kind of variable is date?\n\ndate is typically a string or datetime variable. You can convert it using pd.to_datetime().\n\nbikes['date'] = pd.to_datetime(bikes['date'])\nWhat kind of variable is season?\n\nseason is likely a categorical variable. You can convert it using pd.Categorical().\n\nbikes['season'] = pd.Categorical(bikes['season'])\nHow often does it rain in London?\n\nCount the occurrences in the rain column.\n\nrain_count = bikes['rain'].sum()\nprint(f\"It rains {rain_count} times in the dataset.\")\nWhat is the average annual temperature (in degrees C)?\n\nCalculate the mean of the temperature columns.\n\navg_temp = bikes[['max_temp', 'min_temp', 'avg_temp']].mean().mean()\nprint(f\"The average annual temperature is {avg_temp:.2f} degrees C.\")\nWhat is the maximum rainfall?\n\nFind the maximum value in the rainfall_mm column.\n\nmax_rainfall = bikes['rainfall_mm'].max()\nprint(f\"The maximum rainfall recorded is {max_rainfall} mm.\")"
  },
  {
    "objectID": "Examples.html#clean-data",
    "href": "Examples.html#clean-data",
    "title": "Examples",
    "section": "3. Clean Data",
    "text": "3. Clean Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\nCleaning Variable Names with pandas\nCode Quality\nOther Links\n\n\n\n\n\n\nOverview\nWhen creating data files, it’s common to use variable names and formats that are human-readable but not ideal for computational processing. In Python, the pandas library can be used to clean and standardise variable names for easier manipulation.\n\n\n\nCleaning Variable Names with pandas\nIn Python, we can use pandas and custom code to read data and clean column names to make them more suitable for analysis.\nimport pandas as pd\n\n# Load Excel file\nroster = pd.read_excel(\"path/to/dirty_data.xlsx\")\n\n# Clean column names using pandas string methods\nroster.columns = (\n    roster.columns\n        .str.strip()  # Remove leading and trailing spaces\n        .str.lower()  # Convert to lowercase\n        .str.replace(' ', '_')  # Replace spaces with underscores\n        .str.replace('%', 'percent')  # Replace '%' with 'percent'\n        .str.replace('[^a-zA-Z0-9_]', '', regex=True)  # Remove special characters\n)\n\n# Inspect cleaned dataframe\nprint(roster.head())\n\nThe custom code directly modifies the column names using pandas string methods. It removes spaces, converts to lowercase, replaces spaces with underscores, and removes special characters using a regular expression.\nRegular Expression: [^a-zA-Z0-9_] is used to remove any character that is not alphanumeric or an underscore.\npandas.read_excel(): Used to read Excel files into a DataFrame.\n\n\n\n\nCode Quality\nAccording to Phil Karlton, there are only two hard things in Computer Science: cache invalidation and naming things. It’s crucial to write code that is not only functional but also maintainable and readable. Use meaningful names for variables and dataframes, and include comments to explain complex logic.\n\n\n\nOther Links\n\nFor Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights"
  },
  {
    "objectID": "Examples.html#visualise-data",
    "href": "Examples.html#visualise-data",
    "title": "Examples",
    "section": "4. Visualise Data",
    "text": "4. Visualise Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\nLayers\nFaucetting\nAnimated Graphs\n\nmatplotlib\n\nWhy you should always plot your data\nFurther resources\n\n\n\n\n\n\nLearning Objectives\n\n\n\nProduce scatter plots, boxplots, and time series plots using matplotlib and seaborn.\nSet universal plot settings.\nDescribe what faceting is and apply faceting using seaborn\nModify the aesthetics of an existing plot (including axis labels and colour).\nBuild complex and customised plots from data in a DataFrame.\n\n\n\n\nOverview\n\n“Visualization is a powerful tool for understanding data. You can often discover patterns, spot anomalies, and develop an intuition for the data just by looking at it.”\n— Wes McKinney, creator of pandas\n\nWe will explore how to create insightful and aesthetically pleasing data visualisations using two powerful Python libraries: matplotlib and seaborn.\nWe will work through examples of various plot types - scatter plots, boxplots, and histograms - and learn to customise them for clarity and impact.\nIt may seem verbose and unwieldy, but the idea of building a plot on a layer-by-layer basis is very powerful.\n\nYou begin a plot by defining the dataset you will use.\nThen, specify aesthetics, namely (x, y) coordinates, colour, size, etc.\nFinally, choose the geometric shape to represent your data, and add more layers like legends, labels, facets, etc.\n\nFor example, using the Gapminder dataset with data on life expectancy (life_exp), human development index (hdi_index), and GDP (gdp) for a number of countries, we can build a graph that shows the relationship between GDP and life expectancy.\nAs we said, first we define the dataset we are using\nimport pandas as pd\ngapminder = pd.read_csv(r\"filepath-to-your-dataset\")\nThe next thing is to map aesthetics. In our case, we will map gdpPercap to the x-axis, and lifeExp to the y-axis.\n# Basic Plot Setup\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=gapminder, x='gdp', y='life_exp')\nplt.title('Life Expectancy vs GDP')\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.show()\n\n\nplt.figure(figsize=(10, 6)): Defines the size of the plot.\nsns.scatterplot(...): Uses Seaborn to draw a scatter plot from the gapminder dataset.\nplt.title(...), plt.xlabel(...), plt.ylabel(...): Add meaningful labels and a title to make the chart easy to understand.\nplt.show(): Renders the plot in your output window or notebook.\n\nWhat if we wanted to colour the points by the continent each country is in? For this we will need to use the “continent” column from our gapminder dataset. Seaborn makes this easy using the hue parameter.\n# Colored Scatter Plot by Continent\nplt.figure(figsize=(10, 6))  # Set figure size\nsns.scatterplot(data=gapminder, x='gdp', y='life_exp', hue='continent')  # Color points by continent\nplt.title('Life Expectancy vs GDP by Continent')  # Updated title\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='Continent')  # Add a legend with a title\nplt.show()\n\n\nhue='continent': Tells Seaborn to color the points based on the continent each country belongs to.\nplt.legend(...): Makes sure the legend is clear and labeled.\n\nWhat if instead of a scatter plot we wanted to create a line plot? For this we need to:\n\nchange the sns.scatterplot to sns.lineplot\n\n# Colored Line Plot by Continent\nplt.figure(figsize=(10, 6))  # Set figure size\nsns.lineplot(data=gapminder, x='gdp', y='life_exp', hue='continent')  # Color points by continent\nplt.title('Life Expectancy vs GDP by Continent')  # Updated title\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='Continent')  # Add a legend with a title\nplt.show()\n\nHowever, this is not a particularly useful plot, so let us go back to our scatter plot.\nWhat if we wanted to have the size of each point correspond to the HDI index of the country? For this we will :\n\nadd the size='hdi_index' parameter in the scatterplot() function\nadd the sizes=(20, 200) parameter in the scatterplot() function to specify the range of sizes of scatter points we want to use.\n\n# going back to scatter plot but now the size of points coresspond to the population of the country\n# Basic Plot Setup with size mapped to population\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=gapminder,\n    x='gdp',\n    y='life_exp',\n    size='hdi_index',                  # map size to population\n    sizes=(20, 200),             # scale point size (min, max)\n)\n\nplt.title('Life Expectancy vs GDP (Point Size = hdi_index)')\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))\nplt.tight_layout()\nplt.show()\n But the points above are overlapping with each other. We can set the alpha variable between 0 and 1 to specify how transparent each point will be. This will let us see all the points better.\n# Adding Alpha\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=gapminder,\n    x='gdp',\n    y='life_exp',\n    size='hdi_index',                  # map size to population\n    sizes=(20, 200),             # scale point size (min, max)\n    alpha=0.4                    # make points a bit transparent\n)\n\nplt.title('Life Expectancy vs GDP (Point Size = hdi_index)')\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))\nplt.tight_layout()\nplt.show()\n\nOur graph is still not very clear after adding the alpha this is because all the points are bunched up in only a small section of the graph. We can log the x-axis to make the points more spread out across the GDP axis.\n# Logging the axis\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=gapminder,\n    x='gdp',\n    y='life_exp',\n    size='hdi_index',                  # map size to population\n    sizes=(20, 200),             # scale point size (min, max)\n    alpha=0.4                    # make points a bit transparent\n)\n\nplt.title('Life Expectancy vs GDP (Point Size = hdi_index)')\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))\nplt.tight_layout()\nplt.xscale('log')\nplt.show()\n We will now add the colour by continent parameter back.\nFor this notice that we are splitting the legends’ code into 2 separate sections.\n# Adding color by continent parameter back \nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(\n    data=gapminder,\n    x='gdp',\n    y='life_exp',\n    size='hdi_index',                  # map size to population\n    sizes=(20, 200),             # scale point size (min, max)\n    alpha=0.4,                       # make points a bit transparent\n    hue='continent'\n)\nplt.title('Life Expectancy vs GDP (Point Size = hdi_index)')\nplt.xlabel('GDP per capita (USD)')\nplt.ylabel('Life Expectancy (years)')\n\n# Separate legends for hue and size\nplt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))\nsns.move_legend(scatter, title='Continent', loc='upper left', bbox_to_anchor=(0.7, 0.65))\n\nplt.tight_layout # making sure the legends are in the frame\nplt.xscale('log')\nplt.show()\n\n\n\n\nLayers\nOnce you define your data and aesthetics (such as (x, y) coordinates, colour, size, etc.), you can add more layers to modify and enhance your plots.\n\nGeometric Objects: These are the graphical objects to be drawn, such as histograms, boxplots, density plots, etc.\nStatistics: These can be applied to your data, like calculating density or fitting a regression line.\nPosition Adjustments: These modify how elements are placed, such as jittering points or stacking bars.\n\nExample: Creating a Base plot - a histogram\n# Histogram with Position Adjustments\nplt.figure(figsize=(10, 6))\nsns.histplot(data=gapminder, x='life_exp', hue='continent', element='step', fill=True, alpha=0.3)\nplt.title('Histogram of Life Expectancy by Continent')\nplt.xlabel('Life Expectancy')\nplt.ylabel('Frequency')\nplt.show()\n\nthe “element” attribute can be one of “step”, “poly” or “bars” depending of what is needed. Try it out yourself.\nWe will now plot a density plot, a smoothed version of a histogram using geom_density; its default position is identity and both plots are equivalent.\n\nA histogram displays the frequency of data within bins, while a density plot represents the probability density function of the data, providing a smoothed continuous curve.\nTo convert a histogram to a density plot, you typically use sns.kdeplot, which computes and plots the kernel density estimate.\n\nIn our code below, the kdeplot function is used to create a density plot, and the common_norm=False ensures that the densities for each continent are normalized separately, allowing for a fair comparison of the distribution shapes without being influenced by the differing sizes of the groups.\n# filled Density Plot\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=gapminder, x='life_exp', hue='continent', fill=True, common_norm=False, alpha=0.3)\nplt.title('Density Plot of Life Expectancy by Continent')\nplt.xlabel('Life Expectancy')\nplt.ylabel('Density')\nplt.show()\n\nThe “Multiple Parameter” The multiple Parameter can be set to any of these values:\n\nlayer - Overlays categories, allowing overlap.\nstack - Stacks categories cumulatively.\nfill - Stacks and scales categories to equal height.\ndodge - Positions categories side by side.\n\ne.g. using the dodge version:\nplt.figure(figsize=(10, 6))\nsns.histplot(data=gapminder, x='life_exp', hue='continent', multiple='dodge', alpha=0.3)\nplt.title('Life Expectancy Distribution by Continent')\nplt.xlabel('Life Expectancy')\nplt.ylabel('Frequency')\nplt.show()\n \n\n\nFaucetting\nFacetting is a powerful technique in data visualisation that allows you to split one plot into multiple plots based on a factor included in the dataset. Python’s seaborn library provides this functionality.\nIn the Gapminder scatterplot example, we can use faceting to produce one scatter plot for each continent separately, using FacetGrid.\n\nDefine the Core Scatter Plot\nFirst, let’s define the core scatter plot of life expectancy vs GDP and store it in an object for easy reuse:\n# Base graph\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=gapminder, x='gdp', y='life_exp', hue='continent', alpha=0.5)\nplt.xscale('log')\nplt.title('Life Expectancy vs GDP per capita, 1998-2002')\nplt.xlabel('GDP per capita')\nplt.ylabel('Life Expectancy')\nplt.show()\n\nNow, let’s add a new layer to our base plot using FacetGrid to facet by continent:\n# Ensure 'continent' is treated as a categorical variable\ngapminder['continent'] = gapminder['continent'].astype('category')\n\n# Facet the scatter plot by continent with hue\ng = sns.FacetGrid(gapminder, col='continent', col_wrap=3, height=4)\ng.map_dataframe(sns.scatterplot, x='gdp', y='life_exp', hue='continent', alpha=0.5)\ng.set_titles(\"{col_name}\")\ng.set_axis_labels('GDP per capita', 'Life Expectancy')\ng.add_legend()\nplt.show()\n\ng.map_dataframe(...): Maps the sns.scatterplot function to each facet, passing the data explicitly and ensuring that\nhue='continent' colours the points appropriately in each facet.\n\n\n\nFacetGrid: This is used to create a grid of plots, allowing you to facet by a specified variable (continent in this case).\ncol_wrap: This parameter controls the number of columns in the facet grid, making it adaptable to different screen sizes.\nmap: This method maps a plotting function (sns.scatterplot) to each facet.\n\nFinally, if you want to create a boxplot of life expectancy by continent instead of a scatter plot, you can use similar aesthetics with Python libraries like matplotlib and seaborn. The key difference is the type of plot you choose to represent your data.\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=gapminder, x='continent', y='life_exp', hue='continent')\n\n# Add labels and title\nplt.title(\"Life Expectancy among the continents, 1952-2007\")\nplt.xlabel(\" \")  # Empty, as the levels of the x-variable are the continents\nplt.ylabel(\"Life Expectancy\")\nplt.figtext(0.9, 0.01, \"Source: Gapminder\", horizontalalignment='right')\n\n# Apply a minimal theme\nsns.set_theme(style=\"whitegrid\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\nAnimated Graphs\nAnimated graphs are a powerful way to visualize how data changes over time. In Python, one of the most commonly used tools for creating animations is Matplotlib’s FuncAnimation class. This function updates a plot frame-by-frame, allowing you to illustrate dynamic processes such as moving trends, changing patterns, or simulations. The final animation can be displayed directly in a notebook or exported as a GIF or video using writers like pillow or ffmpeg. Keep in mind that generating animations may take a few seconds, especially for longer sequences, as each frame is rendered individually.\n\n\nmatplotlib.animation\nTo implement animations, we will need to understand the concept of functions in Python. These are reusable blocks of code that perform specific tasks and can be called with different inputs to produce different outputs.\nIn the context of animations, a function—like update() in the code below - is used to define how each frame of the animation should be rendered based on changing input (e.g., the year). This function is then repeatedly called by FuncAnimation, allowing us to dynamically update the plot for each time step.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n\n# Set up the figure and axis\nfig, ax = plt.subplots(figsize=(8, 6))\nplt.xticks(rotation=45)\n\n# Get unique years sorted\nyears = sorted(gapminder['year'].unique())\n\ndef update(year_index):\n    ax.clear()  # Clear previous frame\n    year = years[year_index]\n    yearly_data = gapminder[gapminder['year'] == year]\n\n    # Create boxplot grouped by continent\n    yearly_data.boxplot(column='life_exp', by='continent', ax=ax)\n\n    # Customize plot appearance\n    ax.set_title(f'Life Expectancy by Continent ({year})')\n    ax.set_xlabel('Continent')\n    ax.set_ylabel('Life Expectancy')\n    plt.suptitle('')  # Remove automatic title added by Pandas boxplot\n\n# Create animation\nani = animation.FuncAnimation(fig, update, frames=len(years), interval=800, repeat=True)\n\nplt.tight_layout()\nplt.show()\n\n# Optional: Save as GIF (requires 'pillow')\nani.save('life_expectancy_boxplot.gif', writer='pillow')\n\nSimilarly, we can create an animated visualisation showing the relationship between GDP and life expectancy over time for different countries, segmented by continent.\n\nX-axis (GDP): Logarithmic scale representing each country’s GDP.\nY-axis (Life Expectancy): Life expectancy of each country’s population.\nData Points\n\nEach point represents a country for a specific year.\nColour-coded by continent:\n\nAsia: Red\nEurope: Blue\nAfrica: Green\nAmericas: Yellow\nOceania: Purple\nUnknown: Grey\n\n\nAnimation\n\nDisplays changes over time, with each frame representing a different year.\nTitle updates dynamically to reflect the current year.\n\n\n# Fill NaN values in the 'continent' column with a default value\ngapminder['continent'] = gapminder['continent'].fillna('Unknown')\n\n# Set up the figure and axis\nfig, ax = plt.subplots(figsize=(6, 6))\nplt.xticks(rotation=45)\n\n# Get unique years sorted\nyears = sorted(gapminder['year'].unique())\n\ndef update(year_index):\n    ax.clear()  # Clear previous frame\n    year = years[year_index]\n    yearly_data = gapminder[gapminder['year'] == year]\n\n    # Ensure there are no NaN values in the data used for plotting\n    yearly_data = yearly_data.dropna(subset=['gdp', 'life_exp'])\n\n    # Map continents to colours, handling 'Unknown' as grey\n    continent_colors = yearly_data['continent'].map({\n        'Asia': 'red', 'Europe': 'blue', 'Africa': 'green', \n        'Americas': 'yellow', 'Oceania': 'purple', 'Unknown': 'grey'\n    }).fillna('grey')  # Fill any remaining NaN values with 'grey'\n\n    # Create scatter plot\n    scatter = ax.scatter(yearly_data['gdp'], yearly_data['life_exp'], \n                         c=continent_colors, alpha=0.5)\n\n    # Customize plot appearance\n    ax.set_title(f'Year: {year}')\n    ax.set_xlabel('GDP')\n    ax.set_ylabel('Life Expectancy')\n    ax.set_xscale('log')  # Use logarithmic scale for GDP\n    ax.set_xlim(gapminder['gdp'].min(), gapminder['gdp'].max())\n    ax.set_ylim(gapminder['life_exp'].min(), gapminder['life_exp'].max())\n\n# Create animation\nani = animation.FuncAnimation(fig, update, frames=len(years), interval=800, repeat=True)\n\nplt.tight_layout()\nplt.show()\n\n# Optional: Save as GIF (requires 'pillow')\nani.save('life_expectancy_vs_gdp.gif', writer='pillow', dpi=80)\n\n\n\n\n\nWhy you should always plot your data\nWe have touched on the basics of python visualisations, but in this section we wanted to discuss why one should always plot the data and not just rely on tables of summary statistics.\nLet us consider thirteen datasets all of which have 142 observations of (x,y) values. The table below shows the average value of X and Y, the standard deviation of X and Y, as well as the correlation coefficient between X and Y.\n\n\n\nid\nn\nmean_x\nmean_y\nsd_x\nsd_y\ncorrelation\n\n\n\n\n1\n142\n54.3\n47.8\n6.82\n6.9\n-0.064\n\n\n2\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n3\n142\n54.3\n47.8\n6.82\n6.9\n-0.068\n\n\n4\n142\n54.3\n47.8\n6.82\n6.9\n-0.064\n\n\n5\n142\n54.3\n47.8\n6.82\n6.9\n-0.060\n\n\n6\n142\n54.3\n47.8\n6.82\n6.9\n-0.062\n\n\n7\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n8\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n9\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n10\n142\n54.3\n47.8\n6.82\n6.9\n-0.063\n\n\n11\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n12\n142\n54.3\n47.8\n6.82\n6.9\n-0.067\n\n\n13\n142\n54.3\n47.8\n6.82\n6.9\n-0.066\n\n\n\nSince our datasets contain values for X and Y, we can estimate 13 regression models and plot the values for each of the 13 intercepts and slope for X.\n\nIf we just looked at either the summary statistics table, or the plots of intercepts and slopes, we may be tempted to conclude that the 13 datasets are either identical or very much alike. However, this is far from the truth, as this is what the 13 individual datasets look like.\n\nYou can read more about why you should never trust summary statistics alone and should always visualize your data.\n\n\n\nFurther resources\n\nOfficial seaborn Tutorial\nmatplotlib Pyplot Tutorial\nDataCamp - Data Visualization with Seaborn\nDatacamp - Introduction to Data Visualization with Matplotlib\nseaborn Gallery\nmatplotlib Examples"
  },
  {
    "objectID": "Examples.html#manipulate-data",
    "href": "Examples.html#manipulate-data",
    "title": "Examples",
    "section": "5. Manipulate Data",
    "text": "5. Manipulate Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nSelecting and Filtering Data\nHandling Missing Values\nCreating New Variables (Feature Engineering Basics)\nCombining Datasets\nReshaping Data (Pivoting and Melting)\n\n\n\n\nOnce you’ve imported and inspected your data, the next crucial step in any data science workflow is data manipulation. This involves transforming raw data into a clean, structured, and analysis-ready format. Think of it as refining raw materials before building something incredible – whether it’s an insightful visualization or a powerful predictive model.\nThis section will introduce you to essential techniques using pandas, your go-to library for efficient data manipulation in Python. We’ll cover how to select and filter data, handle missing values, create new variables, and combine datasets.\n\n\nSelecting and Filtering Data\nEffectively extracting specific subsets of your data is fundamental. This allows you to focus on relevant information, analyze particular groups, or prepare data for specific operations.\nYou can select one or multiple columns from a pandas DataFrame using single or double square brackets.\n\nSelecting a single column\n\nimport pandas as pd\n\n# Assuming 'gapminder' DataFrame is loaded from previous sections\n# (e.g., gapminder = pd.read_csv(\"filepath-to-your-dataset\"))\n\n# Select the 'life_exp' column\nlife_expectancy = gapminder['life_exp']\nprint(life_expectancy.head())\nprint(type(life_expectancy)) # Output will be &lt;class 'pandas.core.series.Series'&gt;\n\nSelecting multiple columns:\n\n# Select 'gdp' and 'life_exp' columns\ngdp_life_exp = gapminder[['gdp', 'life_exp']]\nprint(gdp_life_exp.head())\nprint(type(gdp_life_exp)) # Output will be &lt;class 'pandas.core.frame.DataFrame'&gt;\nNotice the difference in output type: a single column selection returns a Series, while multiple columns return a DataFrame."
  },
  {
    "objectID": "Examples.html#filter",
    "href": "Examples.html#filter",
    "title": "Examples",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\nFiltering rows, also known as subsetting, allows you to select data based on specific conditions. This is often done using boolean indexing.\n\nFiltering by a single condition: Let’s say we want to look at data only for the ‘Europe’ continent.\n\n# Filter for data where continent is 'Europe'\neurope_data = gapminder[gapminder['continent'] == 'Europe']\nprint(europe_data.head())\nprint(f\"Number of rows for Europe: {len(europe_data)}\")\n\nFiltering by multiple conditions: You can combine multiple conditions using logical operators (& for AND, | for OR, ~ for NOT). Remember to wrap each condition in parentheses.\n\n# Filter for data where continent is 'Asia' AND life expectancy is greater than 70\nasia_high_life_exp = gapminder[(gapminder['continent'] == 'Asia') & (gapminder['life_exp'] &gt; 70)]\nprint(asia_high_life_exp.head())\nprint(f\"Number of rows for Asia with high life expectancy: {len(asia_high_life_exp)}\")\nReal-world datasets are rarely perfect and often contain missing values (represented as NaN - Not a Number in pandas). Dealing with these is a critical step in data cleaning, as they can lead to errors or biased results in your analysis.\nYou’ve briefly touched on dropna() when saving a cleaned file, but here we’ll delve deeper into strategies.\n\nIdentifying Missing Values: Use isnull() or isna() to get a boolean DataFrame indicating missing values, and sum() to count them per column.\n\n# Check for missing values across the entire DataFrame\nprint(gapminder.isnull().sum())\n\n# Check for missing values in a specific column, e.g., 'gdp'\nprint(f\"Missing values in 'gdp' column: {gapminder['gdp'].isnull().sum()}\")\n\nDropping Missing Values: dropna() Removes rows or columns containing missing values.\n\n# Drop rows with ANY missing values\ndf_cleaned_rows = gapminder.dropna()\nprint(f\"Original rows: {len(gapminder)}, Rows after dropping NaNs: {len(df_cleaned_rows)}\")\n\n# Drop columns with ANY missing values\ndf_cleaned_cols = gapminder.dropna(axis=1) # axis=1 for columns\nprint(f\"Original columns: {gapminder.shape[1]}, Columns after dropping NaNs: {df_cleaned_cols.shape[1]}\")\n\n# Drop rows only if ALL values are missing\ndf_cleaned_all = gapminder.dropna(how='all')\nCaution: Be careful when dropping data, especially rows. This can lead to significant data loss and might not always be the best approach, particularly with small datasets or if missingness carries important information.\n\nImputing Missing Values (Filling): fillna() Replaces missing values with a specified value or method. This is generally preferred over dropping when you want to retain as much data as possible.\n\n# Fill missing 'gdp' values with the mean of the 'gdp' column\nmean_gdp = gapminder['gdp'].mean()\ngapminder_filled_mean = gapminder['gdp'].fillna(mean_gdp)\nprint(f\"GDP after filling with mean:\\n{gapminder_filled_mean.head()}\")\n\n# Fill missing values in 'continent' with 'Unknown' (as done in animation example)\ngapminder['continent'] = gapminder['continent'].fillna('Unknown')\nprint(f\"Continent column after filling NaNs:\\n{gapminder['continent'].value_counts()}\")\n\n# Forward fill (propagates last valid observation forward to next valid)\ngapminder_ffill = gapminder['life_exp'].fillna(method='ffill')\n\n# Backward fill (propagates next valid observation backward to next valid)\ngapminder_bfill = gapminder['life_exp'].fillna(method='bfill')\nChoosing an imputation strategy depends heavily on the nature of your data and the reason for the missingness. Common strategies include using the mean, median, mode, or more advanced techniques like interpolation.\n\n\nCreating New Variables (Feature Engineering Basics)\nFeature engineering is the process of creating new variables (features) from existing ones to improve the performance of predictive models or reveal new insights. This can be as simple as a mathematical transformation or as complex as extracting information from text. - Mathematical Operations: You can easily perform arithmetic operations on existing columns to create new ones.\n# Create a new 'gdp_per_person' column\ngapminder['gdp_per_person'] = gapminder['gdp'] / gapminder['population']\nprint(gapminder[['gdp', 'population', 'gdp_per_person']].head())\n\nApplying Functions: Use the .apply() method to apply a custom function to a Series or DataFrame.\n\n# Categorize countries into 'High' or 'Low' life expectancy based on a threshold\ndef categorize_life_exp(life_exp):\n    return 'High' if life_exp &gt; 70 else 'Low'\n\ngapminder['life_exp_category'] = gapminder['life_exp'].apply(categorize_life_exp)\nprint(gapminder[['life_exp', 'life_exp_category']].head())\n\nUsing numpy for Conditional Logic: For more complex conditional logic, numpy.where is very useful.\n\nimport numpy as np\n\n# Create a 'wealth_category' based on GDP\ngapminder['wealth_category'] = np.where(gapminder['gdp'] &gt; 1000000000000, 'Very High',\n                                        np.where(gapminder['gdp'] &gt; 100000000000, 'High', 'Medium/Low'))\nprint(gapminder[['gdp', 'wealth_category']].head())\n\n\n\nCombining Datasets\nOften, the data you need for an analysis is spread across multiple files or DataFrames. pandas provides powerful functions to combine them. - Concatenating DataFrames (pd.concat()): Used to stack DataFrames vertically (rows) or horizontally (columns).\n# Create two small DataFrames for demonstration\ndf1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']}, index=[0, 1])\ndf2 = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']}, index=[2, 3])\n\n# Concatenate vertically (default)\nresult_concat_rows = pd.concat([df1, df2])\nprint(\"Concatenated Rows:\\n\", result_concat_rows)\n\n# Concatenate horizontally\ndf3 = pd.DataFrame({'C': ['C0', 'C1'], 'D': ['D0', 'D1']}, index=[0, 1])\nresult_concat_cols = pd.concat([df1, df3], axis=1)\nprint(\"\\nConcatenated Columns:\\n\", result_concat_cols)\n\nMerging DataFrames (pd.merge()): Similar to SQL joins, merge() combines DataFrames based on common columns (keys). This is crucial for combining related information from different sources.\n\n# Create two sample DataFrames: one with country info, one with additional stats\ncountry_info = pd.DataFrame({\n    'country': ['Afghanistan', 'Albania', 'Algeria', 'Angola'],\n    'capital': ['Kabul', 'Tirana', 'Algiers', 'Luanda']\n})\n\ncountry_stats = pd.DataFrame({\n    'country': ['Afghanistan', 'Albania', 'Algeria', 'Argentina'],\n    'population_2020': [38928346, 2877797, 43851044, 45195774]\n})\n\n# Inner merge: only includes rows where the 'country' key exists in both DataFrames\nmerged_data_inner = pd.merge(country_info, country_stats, on='country', how='inner')\nprint(\"\\nInner Merge:\\n\", merged_data_inner)\n\n# Left merge: includes all rows from the left DataFrame and matching rows from the right\nmerged_data_left = pd.merge(country_info, country_stats, on='country', how='left')\nprint(\"\\nLeft Merge:\\n\", merged_data_left)\n\n# Right merge: includes all rows from the right DataFrame and matching rows from the left\nmerged_data_right = pd.merge(country_info, country_stats, on='country', how='right')\nprint(\"\\nRight Merge:\\n\", merged_data_right)\n\n# Outer merge: includes all rows from both DataFrames, filling missing with NaN\nmerged_data_outer = pd.merge(country_info, country_stats, on='country', how='outer')\nprint(\"\\nOuter Merge:\\n\", merged_data_outer)\nChoosing the correct how argument (inner, left, right, outer) is essential depending on how you want to handle non-matching keys.\n\n\n\nReshaping Data (Pivoting and Melting)\nReshaping data involves changing the layout of your DataFrame. This is particularly useful when you need to transform data from a “long” format to a “wide” format, or vice-versa, for analysis or visualization.\nPivoting (df.pivot_table() or df.pivot()) Pivoting transforms unique values from one or more columns into new columns, typically aggregating data in the process. pivot_table is more flexible as it can handle duplicate entries in the index and allows for aggregation.\nImagine your gapminder data has multiple entries for a country across different years, and you want to see life expectancy for each continent as columns, with years as rows.\n# Let's create a simplified DataFrame for pivoting example\npivot_df = gapminder[['year', 'continent', 'life_exp']]\n# Calculate the mean life expectancy per continent per year\navg_life_exp_pivot = pivot_df.pivot_table(index='year',\n                                          columns='continent',\n                                          values='life_exp',\n                                          aggfunc='mean')\nprint(\"Pivoted Average Life Expectancy:\\n\", avg_life_exp_pivot.head())\nMelting (pd.melt())\nMelting is the opposite of pivoting; it transforms data from a “wide” format into a “long” format. This is useful for reshaping dataframes for certain types of analysis or plotting libraries (like Seaborn, which often prefers long-format data).\nConsider our avg_life_exp_pivot from above. If we wanted to “unpivot” it back into a long format.\n# Reset index to make 'year' a regular column\navg_life_exp_pivot_reset = avg_life_exp_pivot.reset_index()\n\n# Melt the DataFrame\nmelted_life_exp = pd.melt(avg_life_exp_pivot_reset,\n                         id_vars=['year'], # Columns to keep as identifier variables\n                         var_name='continent', # Name for the new column storing melted column headers\n                         value_name='average_life_expectancy') # Name for the new column storing values\nprint(\"\\nMelted Average Life Expectancy:\\n\", melted_life_exp.head())"
  },
  {
    "objectID": "Examples.html#group-and-aggregate-data",
    "href": "Examples.html#group-and-aggregate-data",
    "title": "Examples",
    "section": "6. Group and Aggregate Data",
    "text": "6. Group and Aggregate Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nThe groupby() Method\nCommon Aggregation Functions\nGrouping by Multiple Columns\nApplying Multiple Aggregations\nUsing agg() with Custom Functions\nPivot Tables Revisited for Aggregation\n\n\n\n\nAfter you’ve cleaned and manipulated your data, you often need to summarize it to uncover patterns, trends, or characteristics within different categories. This is where grouping and aggregation become indispensable. These techniques allow you to perform calculations on subsets of your data, rather than on the entire dataset at once.\nThink of it like getting summary reports for different departments in a company, rather than looking at every single transaction. You’re combining rows based on shared characteristics and then applying functions (like sum, average, count) to those groups.\nThis section will introduce you to groupby() and various aggregation functions in pandas, enabling you to derive meaningful insights from your prepared data\n\n\nThe groupby() Method\nThe core of grouping in pandas is the groupby() method. It allows you to split your DataFrame into groups based on one or more columns, apply a function to each group independently, and then combine the results into a single DataFrame. This process is often referred to as the “split-apply-combine” strategy.\nBasic Grouping Let’s use the gapminder dataset. A common task might be to find the average life expectancy for each continent.\nimport pandas as pd\n\n# Assuming 'gapminder' DataFrame is loaded and 'continent' column is clean\n# (e.g., gapminder = pd.read_csv(\"filepath-to-your-dataset\"))\n# If 'continent' has NaNs, ensure they are handled as per the previous chapter, e.g.,\n# gapminder['continent'] = gapminder['continent'].fillna('Unknown')\n\n# Group by 'continent' and calculate the mean of 'life_exp'\navg_life_exp_by_continent = gapminder.groupby('continent')['life_exp'].mean()\nprint(\"Average Life Expectancy by Continent:\\n\", avg_life_exp_by_continent)\nIn this example:\n\ngapminder.groupby(‘continent’) splits the DataFrame into separate groups for each unique continent (e.g., ‘Asia’, ‘Europe’, ‘Africa’).\n[‘life_exp’] selects the life_exp column from each of these groups.\n.mean() applies the mean function to the life_exp values within each continent group.\nPandas then combines these results into a new Series.\n\n\n\n\nCommon Aggregation Functions\nAfter grouping, you can apply various aggregation functions. Some of the most common include: - .mean(): Calculates the average. - .sum(): Calculates the total. - .count(): Counts non-null observations. - .median(): Calculates the median. - .min(): Finds the minimum value. - .max(): Finds the maximum value. - .std(): Calculates the standard deviation. - .var(): Calculates the variance. - .size(): Counts the total number of rows in each group (including NaNs).\n# Group by 'continent' and find the sum of 'population'\ntotal_pop_by_continent = gapminder.groupby('continent')['population'].sum()\nprint(\"\\nTotal Population by Continent:\\n\", total_pop_by_continent)\n\n# Group by 'year' and find the median 'gdp'\nmedian_gdp_by_year = gapminder.groupby('year')['gdp'].median()\nprint(\"\\nMedian GDP by Year:\\n\", median_gdp_by_year.head())\n\n# Group by 'continent' and find the number of unique countries\ncountry_count_by_continent = gapminder.groupby('continent')['country'].nunique()\nprint(\"\\nNumber of Unique Countries by Continent:\\n\", country_count_by_continent)\n\n\n\nGrouping by Multiple Columns\nYou can group your data using more than one column. This creates hierarchical groups, allowing for more granular analysis.\n# Group by 'year' AND 'continent' to find average life expectancy\navg_life_exp_year_continent = gapminder.groupby(['year', 'continent'])['life_exp'].mean()\nprint(\"\\nAverage Life Expectancy by Year and Continent:\\n\", avg_life_exp_year_continent.head(10))\n\n# The result is a Series with a MultiIndex. You can access specific levels:\nprint(\"\\nLife expectancy for Europe in 1952:\", avg_life_exp_year_continent.loc[(1952, 'Europe')])\nThis multi-level index (MultiIndex) is very powerful for slicing and dicing your data further.\n\n\n\nApplying Multiple Aggregations\nSometimes, you need to calculate several summary statistics for each group simultaneously. The .agg() method (or .aggregate()) is perfect for this.\nAggregating a Single Column with Multiple Functions\n# Group by 'continent' and calculate mean, min, and max of 'life_exp'\nmulti_agg_life_exp = gapminder.groupby('continent')['life_exp'].agg(['mean', 'min', 'max', 'std'])\nprint(\"\\nMultiple Aggregations of Life Expectancy by Continent:\\n\", multi_agg_life_exp)\nAggregating Multiple Columns with Multiple Functions You can specify different aggregation functions for different columns using a dictionary.\n# Group by 'continent' and aggregate 'life_exp' and 'gdp'\nmulti_col_agg = gapminder.groupby('continent').agg(\n    avg_life_exp=('life_exp', 'mean'), # Renames the output column to 'avg_life_exp'\n    total_gdp=('gdp', 'sum'),\n    min_pop=('population', 'min')\n)\nprint(\"\\nMulti-Column Aggregation by Continent:\\n\", multi_col_agg)\nThis gives you fine-grained control over your summary statistics, allowing you to name the resulting columns clearly.\n\n\n\nUsing agg() with Custom Functions\nThe agg() method is highly flexible and can even apply custom functions you define.\n# Define a custom function to calculate the range (max - min)\ndef data_range(series):\n    return series.max() - series.min()\n\n# Group by 'continent' and apply the custom 'data_range' function to 'life_exp'\ncustom_agg_life_exp = gapminder.groupby('continent')['life_exp'].agg(data_range)\nprint(\"\\nLife Expectancy Range by Continent (Custom Function):\\n\", custom_agg_life_exp)\n\n# You can also pass a lambda function directly\nlambda_agg_gdp = gapminder.groupby('continent')['gdp'].agg(lambda x: x.quantile(0.75) - x.quantile(0.25))\nprint(\"\\nInterquartile Range of GDP by Continent (Lambda Function):\\n\", lambda_agg_gdp)\n\n\n\nPivot Tables Revisited for Aggregation\nYou’ve seen pivot_table used for reshaping. It’s also incredibly powerful for aggregation, often providing a more intuitive syntax than groupby().agg() for certain types of summaries.\nRecall our example of average life expectancy by year and continent:\n# Calculate the mean life expectancy using pivot_table\n# index: the column(s) to become the new DataFrame index (rows)\n# columns: the column(s) whose unique values will become the new DataFrame columns\n# values: the column(s) to aggregate\n# aggfunc: the aggregation function (default is 'mean')\navg_life_exp_pivot_table = pd.pivot_table(gapminder,\n                                          index='year',\n                                          columns='continent',\n                                          values='life_exp',\n                                          aggfunc='mean')\nprint(\"\\nAverage Life Expectancy by Year and Continent (using pivot_table):\\n\", avg_life_exp_pivot_table.head())\nThe result is the same as the groupby([‘year’, ‘continent’])[‘life_exp’].mean() with an unstack() operation, but pivot_table often makes the intention clearer when you want a cross-tabulated summary.\nYou can also apply multiple aggregation functions in pivot_table:\n# Pivot table with multiple aggregation functions for 'life_exp'\nmulti_agg_pivot_table = pd.pivot_table(gapminder,\n                                       index='continent',\n                                       values='life_exp',\n                                       aggfunc=['mean', 'median', 'std'])\nprint(\"\\nMultiple Aggregations of Life Expectancy (using pivot_table):\\n\", multi_agg_pivot_table)\nMastering grouping and aggregation allows you to transform large datasets into concise, insightful summaries that are essential for reporting, making decisions, and preparing data for more advanced analytical techniques."
  },
  {
    "objectID": "Examples.html#intro",
    "href": "Examples.html#intro",
    "title": "Examples",
    "section": "Introduction to Statistical Inference",
    "text": "Introduction to Statistical Inference\nThis section will introduce the core ideas behind statistical inference, explaining how we move from simply describing data to making informed judgments about larger populations.\nWhat is Statistical Inference? - The goal is to predict and forecast values of population parameters, test hypotheses about them, and make decisions based on sample statistics derived from limited and incomplete sample information. - Population vs. Sample: Descriptive statistics describe the whole population (its properties are called parameters, e.g., μ, σ). Inferential statistics use a random sample to infer something about the population; properties calculated from samples are called sample statistics (estimators, e.g. \\(\\bar{x}\\), s), which are used to estimate population parameters. Different samples will typically have different statistics."
  },
  {
    "objectID": "Examples.html#probdist",
    "href": "Examples.html#probdist",
    "title": "Examples",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nProbability distributions are fundamental to statistical inference. They describe how data is spread and form the basis for many statistical tests.\nWhat are Probability Distributions? - A probability distribution is an expression that describes the relative frequency (height of the curve) for every possible outcome or a range of outcomes. - A probability model (e.g., the Normal distribution) attempts to capture the essential structure of the real world by asking what the world might look like if an infinite number of observations were obtained. - The main purpose of a distribution is to indicate error, not accuracy; no single observation is a perfect example of generality."
  },
  {
    "objectID": "Examples.html#Normal",
    "href": "Examples.html#Normal",
    "title": "Examples",
    "section": "Normal (Gaussian) Distribution",
    "text": "Normal (Gaussian) Distribution\n\nProperties: Bell-shaped curve.\nStandard Normal Distribution (Z-distribution): A special case where the mean (μ) is 0 and the standard deviation (σ) is 1, as shown below.\n\n\n\nZ-score: Represents the number of standard deviations an observation falls above or below the mean, hense Z = (observation−mean)/SD\n\n\n\nApproximately 95% of observations fall within ±1.96 standard deviations of the mean in a Normal distribution. Observations with a Z-score &gt; 2 or &lt; -2 are usually considered unusual.\n\n\n\nCaution: Not all continuous variables are normally distributed; some are skewed, and the Normal distribution allows for negative values, which may not be appropriate for all real-world data.\n\nThe Normal is just 1 distributions, there are many such distributions including Uniform, Binomial, Exponential, and Log-Normal distributions (But we will not be looking at any of these other distributions)"
  },
  {
    "objectID": "Examples.html#confidenceIntervals",
    "href": "Examples.html#confidenceIntervals",
    "title": "Examples",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nThis section focuses on interval estimation, providing a range within which the true population parameter is likely to fall, along with a specified level of confidence.\nWhat is a Confidence Interval? - Confidence Intervals: Provide a range within which the true population parameter is likely to fall. - Confidence Level: Typically 90%, 95%, or 99% (e.g., “We are 95% confident that the true population mean lies within this interval”). - Confidence level + signficiance level = 100% - So if we have a 95% confidence level, we can say we have a significance level a 5% OR a significance level of 2.5% in each tail.\n\nConstructing Confidence Intervals for Means:\n\nDistribution: It uses the t-distribution with degrees of freedom (df) equal to \\(n-1\\).\nConditions: The conditions for using this method are:\n\nThe sample size (\\(n\\)) is greater than or equal to 30 (\\(n \\ge 30\\)).\nOR the data is approximately normally distributed.\n\nStandard Error (SE): The standard error for the mean is calculated as:\n\n\\[ \\text{SE} = \\sqrt{\\frac{s^2}{n}} = \\frac{s}{\\sqrt{n}} \\]\nwhere \\(s\\) is the sample standard deviation and \\(n\\) is the sample size.\nConstructing Confidence Intervals for Proportions:\n\nDistribution: It typically uses the Normal distribution.\nConditions: The conditions for using this method (often called the “success-failure condition”) are met when:\n\nThe number of expected “successes” (\\(np\\)) is at least 10 (\\(np \\ge 10\\)).\nAND the number of expected “failures” (\\(n(1-p)\\)) is also at least 10 (\\(n(1-p) \\ge 10\\)).\nNote: In practice, we often use the sample proportion \\(\\hat{p}\\) instead of the true proportion \\(p\\) to check these conditions, i.e., \\(n\\hat{p} \\ge 10\\) and \\(n(1-\\hat{p}) \\ge 10\\).\n\nStandard Error (SE): The standard error for the proportion is calculated using the sample proportion (\\(\\hat{p}\\)):\n\n\\[ \\text{SE} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\]\nwhere \\(\\hat{p}\\) is the sample proportion and \\(n\\) is the sample size.\nBootstrapping Bootstrap Estimation: Can be used for confidence intervals. The process involves specifying variables, generating data (via bootstrapping), calculating a statistic, and visualizing. In Python, this is achieved using NumPy for data manipulation and random sampling, SciPy’s bootstrap function for efficient resampling and confidence interval calculation, and Matplotlib or Seaborn for visualizing the bootstrap distribution."
  },
  {
    "objectID": "Examples.html#HypTesting",
    "href": "Examples.html#HypTesting",
    "title": "Examples",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nThis is the core of statistical inference, where we formally test claims or assumptions about a population using evidence from our sample.\n\nNull Hypothesis (\\(H_0\\)): This is a statement of no effect or no difference. It represents a model of the world under the assumption that the effect we are investigating is not real (e.g., there is no difference between two groups, or a new drug has no effect). We assume the null hypothesis is true until we find sufficient evidence to the contrary.\nAlternative Hypothesis (\\(H_A\\) or \\(H_1\\)): This is the claim we are trying to find evidence for. It proposes that there is an effect or a difference (e.g., there is a difference between groups, or the new drug does have an effect).\nSignificance Level (\\(\\alpha\\)): This is the predetermined threshold for how improbable the observed data must be, assuming the null hypothesis is true, for us to reject the null hypothesis. It represents the maximum probability of making a Type I error that we are willing to accept.\n\nCommonly set at 5% (0.05).\nCan be set lower, such as 1% (0.01) or less, particularly in fields where false positives are costly (e.g., medicine, particle physics).\nThis concept is analogous to the legal principle of “beyond reasonable doubt” in a court of law.\n\np-value: The p-value is the probability of observing data as extreme as, or more extreme than, what was actually observed, assuming that the null hypothesis (\\(H_0\\)) is true. It quantifies the strength of evidence against the null hypothesis.\n\n\nInterpreting the p-value\n\nIf the p-value &lt; \\(\\alpha\\): This means the observed data is highly improbable if the null hypothesis were true. We consider this a “surprising” effect, suggesting that the data is not just random noise. In this case, we reject the null hypothesis (\\(H_0\\)). This implies there is statistically significant evidence to support the alternative hypothesis.\nIf the p-value &gt; \\(\\alpha\\): This means the observed data is not surprising under the assumption that the null hypothesis is true. The data is consistent with random variation. In this case, we fail to reject the null hypothesis (\\(H_0\\)). This means there isn’t enough statistically significant evidence to support the alternative hypothesis.\n\n\n\nTypes of Errors\nWhen making a decision about the null hypothesis, there’s always a possibility of making an error:\n\nType I Error (\\(\\alpha\\)): This occurs when we reject a true null hypothesis. It’s a “false positive” – concluding there’s an effect when there isn’t one. The significance level (\\(\\alpha\\)) directly controls the probability of making a Type I error.\nType II Error (\\(\\beta\\)): This occurs when we fail to reject a false null hypothesis. It’s a “false negative” – concluding there’s no effect when there actually is one.\n\n\n\nDrawing Conclusions\nIn hypothesis testing, our conclusions are precise:\n\nWe either Reject \\(H_0\\) or Fail to Reject \\(H_0\\).\nIt’s crucial to avoid language that implies absolute proof or disproof. For example, we should never say, “The person is innocent,” but rather, “There is not sufficient evidence to show that the person is guilty.” Similarly, we don’t “accept” the null hypothesis; we simply state that we “fail to reject” it.\n\n\n\nTests for Comparing Groups\nWhen comparing two independent groups, different statistical tests and formulas for standard error are used based on whether you’re comparing proportions or means.\n\nDifference in Means\n\nDistribution: Uses the t-distribution.\nDegrees of Freedom (df): The degrees of freedom are conservatively estimated as the smaller of \\(n_1-1\\) or \\(n_2-1\\). More precise methods (like Welch’s t-test) use a more complex formula for df.\nConditions:\n\n\\(n_1 \\ge 30\\) or data from group 1 is approximately normal.\nAND \\(n_2 \\ge 30\\) or data from group 2 is approximately normal.\n\nStandard Error (SE): The standard error for the difference in means is:\n\n\\[ \\text{SE} = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}} \\]\nwhere \\(s_1^2\\) and \\(s_2^2\\) are the sample variances for group 1 and group 2, respectively, and \\(n_1\\) and \\(n_2\\) are their respective sample sizes.\n\n\nDifference in Proportions\n\nDistribution: Uses the Normal distribution.\nConditions: All counts for both groups must be at least 10. That is, for Group 1: \\(n_1\\hat{p}_1 \\ge 10\\) and \\(n_1(1-\\hat{p}_1) \\ge 10\\); and for Group 2: \\(n_2\\hat{p}_2 \\ge 10\\) and \\(n_2(1-\\hat{p}_2) \\ge 10\\).\nStandard Error (SE): The standard error for the difference in proportions is:\n\n\\[ \\text{SE} = \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\]\nwhere \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\) are the sample proportions for group 1 and group 2, respectively, and \\(n_1\\) and \\(n_2\\) are their respective sample sizes."
  },
  {
    "objectID": "Examples.html#regression-applications",
    "href": "Examples.html#regression-applications",
    "title": "Examples",
    "section": "Examples of Regression Applications",
    "text": "Examples of Regression Applications\nRegression models are widely used. Examples include:\n\nBrexit Data Models: Analyzing factors influencing “leave share” in the Brexit referendum, considering variables like born_in_uk, degree, and age_18to24. These models often involve assessing correlations between variables.\nEarnings and Height/Gender: Investigating if height and gender impact earnings. This highlights the importance of considering confounding variables that might bias estimates if omitted from the model.\nPick-up Time Analysis: Modelling the relationship between average pick-up times, number of drivers, and surge price in a mobility platform."
  },
  {
    "objectID": "Assignments.html",
    "href": "Assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment details\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nWeekly Homeworks\nFinal Project\nFinal Exam\n\n\n\n\n\n\nWeekly Homeworks\nTo practice writing R code, exploring datasets, and running statistical models you will complete a series of weekly homeworks.\nThere are 3 weekly homeworks to be undertaken with your group and these will be graded using a check system:\n\n5/5: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often.\n3/5: Problem set is 70–99% complete and most answers are correct. This is the expected level of performance.\n1/5: Problem set is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often.\n\n\n\n\nFinal Project\nAt the end of the course, you will demonstrate your knowledge by working on a group assignment… Complete details for the final project will be made available here.\n\n\n\nFinal Exam\nThere will be a final exam that will cover: * 1. Programming in R and the tidyverse (about a third of total marks) * the core statistical tools of inferential statistics and linear models.\nYou will take the final exam in-person in a lecture theatre. This is a closed-book, 1.5-hour exam. A formula sheet will be provided, but you will not have online access to Google.\n\n\n\n\nHomeworks\n\n\nHomework 1\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nhomework1.Rmd\nInstruction\nRubric\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nhomework1.Rmd\nInstruction\nRubric\n\n\n\n\n\n\n\nHomework 3\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nhomework1.Rmd\nInstruction\nRubric\n\n\n\n\n\n\n\n\nProjects\n\n\nFinal Group Project\n\n\n\n\n\n\nContents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Exam\n\n\n\n\n\n\nContents\n\n\n\n\n\n\n\n\n\n\n\n\nDue by 11:59 PM on Friday, September 20, 2024"
  },
  {
    "objectID": "Content.html",
    "href": "Content.html",
    "title": "Content",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before the lecture or working through the lesson.\nI have included a set of questions that might guide your reflection response. You should not try to respond to all of these (or any of them if you don’t want to)– they’ll just help you know what to look for and think about as you read."
  },
  {
    "objectID": "Content.html#workshop-1-import-visualise-and-manipulate-data",
    "href": "Content.html#workshop-1-import-visualise-and-manipulate-data",
    "title": "Content",
    "section": "Workshop 1: Import, visualise, and manipulate data",
    "text": "Workshop 1: Import, visualise, and manipulate data\n\nRead before class on Wednesday, August 28, 2024\n\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nReadings\n\nRecommended\n\nQuestions to Reflect on\nSession Files\n\n\n\n\n\nReadings\n\nHans Rosling, “200 Countries, 200 Years, 4 Minutes”\nChapter 2 in Claus Wilke, Fundamentals of Data Visualization\nChapter 3 in Kieran Healy, Data Visualization\n\n\nRecommended\n\nThe fullest look yet at the racial inequality of Coronavirus\n\n\n\n\nQuestions to Reflect on\n\nWhat data was mapped to which aesthetics in Rosling’s video?\nWhat data would you need to create the bar plot in NYT’s article?\n\n\n\nSession Files\n\nSlides for today’s session are available on Canvas.\nYou can download all session files (data, code, etc.) by pulling from course Github repo.\n\ngit clone https://github.com/kostis-christodoulou/am01.git\nAlternatively, download the zipped files directly from here. Then unzip and open them in your Python IDE (e.g., Jupyter, VS Code, or Spyder)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics",
    "section": "",
    "text": "Applied Statistics with Python\n\n\nThis website is your comprehensive guide to understanding, analyzing, and leveraging data effectively. Whether you’re a beginner curious about data or looking to deepen your analytical skills, you’ll find the tools and knowledge you need right here."
  },
  {
    "objectID": "index.html#what-youll-explore",
    "href": "index.html#what-youll-explore",
    "title": "Applied Statistics",
    "section": "What You’ll Explore",
    "text": "What You’ll Explore\n\n\n\n\n📊\n\n\nExploratory Data Analysis\n\n\nLearn how to inspect, clean, and visualize your data to uncover hidden patterns, identify anomalies, and formulate initial hypotheses. Get to know your data inside and out.\n\n\n\n\n\n🧠\n\n\nStatistical Inference\n\n\nDive into making informed decisions about populations based on sample data. Master probability distributions, confidence intervals, and hypothesis testing.\n\n\n\n\n\n💻\n\n\nData Modelling\n\n\nBuild and interpret statistical models that explain relationships and make predictions. Master correlation, linear regression, and real-world problem solving.\n\n\n\n\n\n\n\n\n\n\n👨‍🏫\n\n\nInstructor\n\n\n———————–\n\n\n\n\n\n📚\n\n\nCourse Details\n\n\n———————————-\n\n\n\n\n\n🚀\n\n\nContact Me\n\n\n——————————————"
  },
  {
    "objectID": "Schedule.html",
    "href": "Schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the course\n\nContent (): This contains the readings, slides, data files, etc. for each session. These will also be added on Canvas on the day of each session. It helps to read the material before each session.\nExample () : This page contains worked examples of fully annotated R code that you can use as a reference. This is only a reference page—you don’t have to necessarily do anything here.\nExercise (): These are interactive exercises where you have to provide R code in your browser to solve a problem, much like Datacamp. These are not graded, but are always there for your reference.\nAssignment (): This page contains instructions for the three workshop exercises (3-4 brief tasks plus a challenge), for the individual portfolio website project, and the final group project.\n\nAssignments are due by 11:59 PM UTC on the day they’re listed.\n\n\n\nWeek\nDate\nSession\nContent\nExample\nExercise\nAssignment\n\n\n\n\nFoundations: EDA and Intro to Data Science\n\n\n\n\n\n\n\n\n1\n28th Aug\nLecture 1: Exploratory Data Analysis\n()\n()\n()\n-\n\n\n2\n28th Aug\nWorkshop 1: Import, visualise, and manipulate data\n()\n()\n()\n-\n\n\n\n29th Aug\nHomework 1 due\n-\n-\n-\n()\n\n\nInferential Statistics\n\n\n\n\n\n\n\n\n3\n30th Aug\nLecture 2: Sampling and Probability Distributions\n()\n()\n()\n-\n\n\n4\n30th Aug\nWorkshop 2: Confidence Intervals; reshape data\n()\n()\n()\n-\n\n\n\n03rd Sep\nHomework 2 due\n-\n-\n-\n()\n\n\n5\n04th Sep\nLecture 3: Hypothesis Testing; there is only one test\n()\n()\n()\n-\n\n\n6\n04th Sep\nWorkshop 3: Hypothesis testing; A/B testing; simulating\n()\n()\n-\n-\n\n\n\n08th Sep\nHomework 3 due\n-\n-\n-\n()\n\n\nRegression Modelling\n\n\n\n\n\n\n\n\n7\n09th Sep\nLecture 4: Introduction to regression models\n()\n()\n-\n-\n\n\n8\n09th Sep\nWorkshop 4: Workshop on regression\n()\n()\n-\n-\n\n\n9\n11th Sep\nLecture 5: Further regression; regression diagnostics\n()\n()\n-\n-\n\n\n10\n11th Sep\nFinal group project\n()\n-\n-\n-\n\n\n\n11th Sep\nFinal Group Project due\n-\n-\n-\n()\n\n\n\n18th Sep\nRevision Session\n-\n-\n-\n-\n\n\n\n20th Sep\nFinal Exam\n-\n-\n-\n()"
  }
]