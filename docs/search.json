[
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course is an introduction to using R/Rstudio to learn from data. It aims to teach useful skills: importing, data filtering and manipulation, visualization, inferential statistics, and data modelling.\nBusiness decisions are often too complex to be made by intuition alone. We need to communicate the structure of our reasoning, defend it to adversarial challenge and deliver presentations that show we have done a thorough analysis. We also need to understand and make use of various sources of data, organise the inputs of experts and colleagues, and use R/RStudio to provide analytical support to our reasoning. The overall objective of this course is to equip you with analytical thinking and techniques that help you be more effective in these tasks. The goal is to teach you how to perform data analysis to support decision-making, build simple but powerful models that test your intuitive reasoning, develop managerial thinking and facilitate the communication of your recommendations.\nBy the end of the course you should be able to identify the areas where data analytics can add the most value, select appropriate types of analyses and apply them in a small-scale, quick-turnaround but high-impact fashion.\n\nLearning Objectives\n\nArticulate, extract and analyse valuable information from data\nUnderstand and quantify the accuracy of sample evidence\nBuild regression models to describe and predict complicated outcomes\nCommunicate quantitative analysis and recommendations effectively with RMarkdown\nBe able to use R and R studio effectively for data analysis and decision making\n\n\n\nRequired Texts or Readings\nWe will be drawing on the following online Textbooks:\n\nModernDrive: Statistical Interface Via Data Science\nR for Data Science\nGetting Used to R, RStudio, and RMarkdown\n\n\n\nAssessment Policy\nGrades will be based on the following:\n\n\n\n\n\n\n\n\n\nAssessment Type\nDue\nWeight\nGroup/Individual\n\n\n\n\nHomework in R Markdown (3*5%)\n29th Aug, 3rd Sep, 8th Sep\n15%\nGroup\n\n\nGroup Project\n11 Sep 2024\n30%\nGroup\n\n\nFinal Exam, online ?, timed\n20 Sep 2024\n55%\nIndividual\n\n\n\nAssignments are due by 11:59 PM UTC on the day they are due.\n\n\nDetailed Course Structure\nHere is a detailed schedule for this class.\n\n\nWords of Encouragement\nLearning R can be challenging at first— it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2—made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors.\n # The 15 Minute rule\nIt’s good practice to follow the 15 minute rule. If you encounter a problem in your work, spend 15 minutes troubleshooting the problem on your own; Google, RStudio Support, and StackOverflow are good places to look for answers. So if you google your error message, you will find that 99% of the time someone has had the same error message and the solution is on stackoverflow.\nHowever, if after 15 minutes you still cannot solve the problem, please ask for help – post a question on Slack, email me, reach out to a friend.\n\n\n15 min rule: when stuck, you HAVE to try on your own for 15 min; after 15 min, you HAVE to ask for help. - Brain AMA\n\n— Rachel Thomas (@math_rachel), August 15, 2016"
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Resources",
    "section": "",
    "text": "Contents\n\n\n\n\n\n\nInstalling Python\nInstalling vs Code\nOther tools\n\n\n\n\n\n\nTo install Python:\n\nDownload from the official website: python.org\nRecommended version: 3.10 or higher\nFor Mac: Consider using Homebrew\n\nFor Windows: Use the Microsoft Store or installer\n\n\n\n\nSteps to install VS Code:\n\nDownload from: code.visualstudio.com\nRecommended extensions:\n\nPython\nQuarto\nGitLens\nJupyter\n\n\n\n\n\nComing soon…"
  },
  {
    "objectID": "Resources.html#installing",
    "href": "Resources.html#installing",
    "title": "Resources",
    "section": "",
    "text": "To install Python:\n\nDownload from the official website: python.org\nRecommended version: 3.10 or higher\nFor Mac: Consider using Homebrew\n\nFor Windows: Use the Microsoft Store or installer"
  },
  {
    "objectID": "Resources.html#code",
    "href": "Resources.html#code",
    "title": "Resources",
    "section": "",
    "text": "Steps to install VS Code:\n\nDownload from: code.visualstudio.com\nRecommended extensions:\n\nPython\nQuarto\nGitLens\nJupyter"
  },
  {
    "objectID": "Resources.html#tools",
    "href": "Resources.html#tools",
    "title": "Resources",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "Resources.html#github",
    "href": "Resources.html#github",
    "title": "Resources",
    "section": "Using GitHub",
    "text": "Using GitHub\nTo get started with GitHub:\n\nSign up at github.com\nLearn basic commands:\n\ngit clone\ngit add\ngit commit\ngit push\n\nTry GitHub Desktop if you’re not comfortable with the command line interface (CLI)"
  },
  {
    "objectID": "Resources.html#textbooks",
    "href": "Resources.html#textbooks",
    "title": "Resources",
    "section": "Textbooks",
    "text": "Textbooks\n\ntest"
  },
  {
    "objectID": "Resources.html#videos",
    "href": "Resources.html#videos",
    "title": "Resources",
    "section": "Videos",
    "text": "Videos\n\ntest"
  },
  {
    "objectID": "Resources.html#finance",
    "href": "Resources.html#finance",
    "title": "Resources",
    "section": "Finance Data",
    "text": "Finance Data\n\nYahoo Finance: finance.yahoo.com\nAlpha Vantage: alphavantage.co\nQuandl: quandl.com"
  },
  {
    "objectID": "Resources.html#worldBank",
    "href": "Resources.html#worldBank",
    "title": "Resources",
    "section": "World Bank",
    "text": "World Bank\n\nWorld Bank Open Data: data.worldbank.org\nWDI R package: WDI on CRAN"
  },
  {
    "objectID": "Examples.html",
    "href": "Examples.html",
    "title": "Examples",
    "section": "",
    "text": "Contents\n\n\n\n\n\n\nExploratory Data Analysis\nData Modelling\n\n\n\n\nThis section contains worked examples, most of them with fully annotated Python code that you can use as reference.\n\n\nBefore starting any analysis, we first need to import a dataset, understand its variables, visualize it, and manipulate it systematically using tools like pandas, matplotlib, and seaborn. This might seem like a tedious step, but it’s a critical foundation that must precede any form of statistical modelling.\n\n\n\nData modelling allows us to move beyond describing individual variables — instead, we use models to learn from data. At the core of this is understanding the relationship:\n# General form of a predictive model\noutcome = f(features) + error\nWe begin with Exploratory Data Analysis tailored for modelling, and then proceed with three key approaches:\n\nEDA for Modelling After importing and cleaning the data (using pandas), we start looking at summary statistics and plots that will be useful in framing our modelling approach\nTesting for Differences in Means across samples: How do we know whether there is a statistically significant difference between two groups A and B? E.g., between those who took a drug versus those than a placebo? Or whether there is a difference in the percentage of people who approve of Donald Trump is lower than those who disapprove of him?\nFitting a Linear Regression Model To understand which features are associated with a numerical outcome Y, we use Linear Regression from scikit-learn. We try to explain the effect that specific explanatory variables, X, have on Y\nFitting a Binary Classification Model where the difference is that the outcome variable, Y, is binary (0/1). Again we want to use our model primarily for explanation, e.g., what is the effect of different explanatory variables X’s on e.g., the probability that someone with Covid-19 will die?"
  },
  {
    "objectID": "Examples.html#import-data",
    "href": "Examples.html#import-data",
    "title": "Examples",
    "section": "1. Import Data",
    "text": "1. Import Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\n\nImporting CSV files: pandas.read_csv()\nImporting CSV files saved locally\n\nNeed for speed: dask and vaex\nOther data formats\nNever work directly on the raw data\nother Links\n\n\n\n\n\nLearning Objectives\n\nLoad external data from a .csv file into a data frame.\nDescribe what a data frame is.\nUse indexing to subset specific portions of data frames.\nDescribe what a factor is.\nReorder and rename factors.\nFormat dates.\n\n\n\nOverview\nWhen working with Python, data importation is generally achieved using libraries like pandas, which provides powerful tools for data manipulation, including importing data from various file formats.\n\n\n\nImporting CSV files: pandas.read_csv()\nCSV files can be imported using the read_csv() function from the pandas library. This function is fast and user-friendly, allowing you to read flat data without complex configurations.\nimport pandas as pd\n\n# Importing CSV file from a URL\nurl = \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\"\nweather = pd.read_csv(url, skiprows=1, na_values=\"***\")\nOptions Used: * skiprows=1: This skips the first row, assuming the actual data starts from the second row. * na_values= \"***\" : This treats astriks’ as missing values, converting them to NaN.\nTo view the structure of the dataframe, you can use:\nprint(weather.info())\nprint(weather.head())\n\n\n\nImporting CSV files saved locally\nTo import a CSV file saved locally, simply provide the file path to the read_csv() function:\nweather_local = pd.read_csv(\"path/to/your/localfile.csv\")\n\n\n\nNeed for speed: dask and vaex\nFor handling large datasets, libraries like dask and vaex can be used, which offer faster data processing capabilities compared to pandas.\nimport dask.dataframe as dd\n\nweather_large = dd.read_csv(\"path/to/largefile.csv\")\n\n\n\nOther data formats\nPython offers several libraries for reading and writing various data formats:\n\nExcel files: pandas.read_excel()\nJSON: json library\nWeb APIs: requests library\nDatabases: sqlite3\nBig Data: pyspark\n\n\n\n\nNever work directly on the raw data\nIn 2012 Cecilia Giménez, an 83-year-old widow and amateur painter, attempted to restore a century-old fresco of Jesus crowned with thorns in her local church in Borja, Spain. The restoration didn’t go very well, but, surprisingly, the botched restoration of Jesus fresco miraculously saved the Spanish Town.\n\nAs a most important rule, please do not work on the raw data; it’s unlikely you will have Cecilia Giménez’s good fortune to become (in)famous for your not-so-brilliant work.\nMake sure you always work on a copy of your raw data. Use Python’s data manipulation libraries to clean and transform your data, and save the results to a new file, ensuring the original data remains intact.\nweather_cleaned = weather.dropna()  # Example of cleaning data\nweather_cleaned.to_csv(\"cleaned_data.csv\", index=False)\n\n\n\nOther Links\n\nFor Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights\nData Wrangling with pandas: Pandas Documentation\nBig Data Processing with dask and vaex: Dask Documentation, Vaex Documentation"
  },
  {
    "objectID": "Examples.html#inspect-data",
    "href": "Examples.html#inspect-data",
    "title": "Examples",
    "section": "2. Inspect Data",
    "text": "2. Inspect Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\nViewing Data\nDetailed Inspection\nKey Questions\n\n\n\n\n\n\nOverview\nOnce you have loaded your dataset into Python, it’s essential to inspect and understand the data. Typically, you want to know:\n\nThe dimensions of the dataset (number of rows and columns).\nThe types of variables (integer, string, boolean, etc.).\nThe number of missing values.\nSummary statistics for numeric data.\n\n\n\n\nViewing Data\nIn Python, you can use pandas to view and inspect your data. The pandas library provides several functions to achieve this.\npandas.DataFrame.info() and pandas.DataFrame.describe()\nThese functions help you understand the structure and summary statistics of your data.\nimport pandas as pd\n\n# Load data\ngapminder = pd.read_csv(\"path/to/gapminder.csv\")\n\n# View the structure of the dataframe\nprint(gapminder.info())\n\n# Summary statistics\nprint(gapminder.describe())\nUsing info(): Provides the number of rows, columns, and data types of each column. Using describe() : Offers summary statistics for numeric columns, including count, mean, standard deviation, min, and max values.\n\n\n\nDetailed Inspection\nFor a detailed inspection, you can use the pandas_profiling library, which provides an extensive report on your dataframe.\nfrom pandas_profiling import ProfileReport\n\n# Generate a report\nprofile = ProfileReport(gapminder)\nprofile.to_file(\"gapminder_report.html\")\nExample Analysis on London Bikes Data\nbikes = pd.read_csv(\"path/to/londonBikes.csv\")\n\n# Use pandas_profiling for a detailed report\nbikes_profile = ProfileReport(bikes)\nbikes_profile.to_file(\"london_bikes_report.html\")\n\n\n\nKey Questions\nusing the London Bikes Data from above\n\nWhat kind of variable is date?\n\ndate is typically a string or datetime variable. You can convert it using pd.to_datetime().\n\nbikes['date'] = pd.to_datetime(bikes['date'])\nWhat kind of variable is season?\n\nseason is likely a categorical variable. You can convert it using pd.Categorical().\n\nbikes['season'] = pd.Categorical(bikes['season'])\nHow often does it rain in London?\n\nCount the occurrences in the rain column.\n\nrain_count = bikes['rain'].sum()\nprint(f\"It rains {rain_count} times in the dataset.\")\nWhat is the average annual temperature (in degrees C)?\n\nCalculate the mean of the temperature columns.\n\navg_temp = bikes[['max_temp', 'min_temp', 'avg_temp']].mean().mean()\nprint(f\"The average annual temperature is {avg_temp:.2f} degrees C.\")\nWhat is the maximum rainfall?\n\nFind the maximum value in the rainfall_mm column.\n\nmax_rainfall = bikes['rainfall_mm'].max()\nprint(f\"The maximum rainfall recorded is {max_rainfall} mm.\")"
  },
  {
    "objectID": "Examples.html#clean-data",
    "href": "Examples.html#clean-data",
    "title": "Examples",
    "section": "3. Clean Data",
    "text": "3. Clean Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\nCleaning Variable Names with pandas\nCode Quality\nOther Links\n\n\n\n\n\n\nOverview\nWhen creating data files, it’s common to use variable names and formats that are human-readable but not ideal for computational processing. In Python, the pandas library can be used to clean and standardise variable names for easier manipulation.\n\n\n\nCleaning Variable Names with pandas\nIn Python, we can use pandas and custom code to read data and clean column names to make them more suitable for analysis.\nimport pandas as pd\n\n# Load Excel file\nroster = pd.read_excel(\"path/to/dirty_data.xlsx\")\n\n# Clean column names using pandas string methods\nroster.columns = (\n    roster.columns\n        .str.strip()  # Remove leading and trailing spaces\n        .str.lower()  # Convert to lowercase\n        .str.replace(' ', '_')  # Replace spaces with underscores\n        .str.replace('%', 'percent')  # Replace '%' with 'percent'\n        .str.replace('[^a-zA-Z0-9_]', '', regex=True)  # Remove special characters\n)\n\n# Inspect cleaned dataframe\nprint(roster.head())\n\nThe custom code directly modifies the column names using pandas string methods. It removes spaces, converts to lowercase, replaces spaces with underscores, and removes special characters using a regular expression.\nRegular Expression: [^a-zA-Z0-9_] is used to remove any character that is not alphanumeric or an underscore.\npandas.read_excel(): Used to read Excel files into a DataFrame.\n\n\n\n\nCode Quality\nAccording to Phil Karlton, there are only two hard things in Computer Science: cache invalidation and naming things. It’s crucial to write code that is not only functional but also maintainable and readable. Use meaningful names for variables and dataframes, and include comments to explain complex logic.\n\n\n\nOther Links\n\nFor Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights"
  },
  {
    "objectID": "Examples.html#visualise-data",
    "href": "Examples.html#visualise-data",
    "title": "Examples",
    "section": "4. Visualise Data",
    "text": "4. Visualise Data\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nOverview\nLayers\nFaucetting\nAnimated Graphs\n\nmatplotlib\n\nWhy you should always plot your data\nFurther resources\n\n\n\n\n\n\nLearning Objectives\n\n\n\nProduce scatter plots, boxplots, and time series plots using matplotlib and seaborn.\nSet universal plot settings.\nDescribe what faceting is and apply faceting using seaborn\nModify the aesthetics of an existing plot (including axis labels and colour).\nBuild complex and customised plots from data in a DataFrame.\n\n\n\n\nOverview\n\n“Visualization is a powerful tool for understanding data. You can often discover patterns, spot anomalies, and develop an intuition for the data just by looking at it.”\n— Wes McKinney, creator of pandas\n\nWe will explore how to create insightful and aesthetically pleasing data visualisations using two powerful Python libraries: matplotlib and seaborn.\nWe will work through examples of various plot types - scatter plots, boxplots, and histograms - and learn to customise them for clarity and impact.\nIt may seem verbose and unwieldy, but the idea of building a plot on a layer-by-layer basis is very powerful.\n\nYou begin a plot by defining the dataset you will use.\nThen, specify aesthetics, namely (x, y) coordinates, colour, size, etc.\nFinally, choose the geometric shape to represent your data, and add more layers like legends, labels, facets, etc.\n\nFor example, using the Gapminder dataset with data on life expectancy (life_exp), human development index (hdi_index), and GDP (gdp) for a number of countries, we can build a graph that shows the relationship between GDP and life expectancy.\nAs we said, first we define the dataset we are using\nimport pandas as pd\ngapminder = pd.read_csv(r\"filepath-to-your-dataset\")\nThe next thing is to map aesthetics. In our case, we will map gdpPercap to the x-axis, and lifeExp to the y-axis.\n# Basic Plot Setup\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=gapminder, x='gdp', y='life_exp')\nplt.title('Life Expectancy vs GDP')\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.show()\n\n\nplt.figure(figsize=(10, 6)): Defines the size of the plot.\nsns.scatterplot(...): Uses Seaborn to draw a scatter plot from the gapminder dataset.\nplt.title(...), plt.xlabel(...), plt.ylabel(...): Add meaningful labels and a title to make the chart easy to understand.\nplt.show(): Renders the plot in your output window or notebook.\n\nWhat if we wanted to colour the points by the continent each country is in? For this we will need to use the “continent” column from our gapminder dataset. Seaborn makes this easy using the hue parameter.\n# Colored Scatter Plot by Continent\nplt.figure(figsize=(10, 6))  # Set figure size\nsns.scatterplot(data=gapminder, x='gdp', y='life_exp', hue='continent')  # Color points by continent\nplt.title('Life Expectancy vs GDP by Continent')  # Updated title\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='Continent')  # Add a legend with a title\nplt.show()\n\n\nhue='continent': Tells Seaborn to color the points based on the continent each country belongs to.\nplt.legend(...): Makes sure the legend is clear and labeled.\n\nWhat if instead of a scatter plot we wanted to create a line plot? For this we need to:\n\nchange the sns.scatterplot to sns.lineplot\n\n# Colored Line Plot by Continent\nplt.figure(figsize=(10, 6))  # Set figure size\nsns.lineplot(data=gapminder, x='gdp', y='life_exp', hue='continent')  # Color points by continent\nplt.title('Life Expectancy vs GDP by Continent')  # Updated title\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='Continent')  # Add a legend with a title\nplt.show()\n\nHowever, this is not a particularly useful plot, so let us go back to our scatter plot.\nWhat if we wanted to have the size of each point correspond to the HDI index of the country? For this we will :\n\nadd the size='hdi_index' parameter in the scatterplot() function\nadd the sizes=(20, 200) parameter in the scatterplot() function to specify the range of sizes of scatter points we want to use.\n\n# going back to scatter plot but now the size of points coresspond to the population of the country\n# Basic Plot Setup with size mapped to population\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=gapminder,\n    x='gdp',\n    y='life_exp',\n    size='hdi_index',                  # map size to population\n    sizes=(20, 200),             # scale point size (min, max)\n)\n\nplt.title('Life Expectancy vs GDP (Point Size = hdi_index)')\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))\nplt.tight_layout()\nplt.show()\n But the points above are overlapping with each other. We can set the alpha variable between 0 and 1 to specify how transparent each point will be. This will let us see all the points better.\n# Adding Alpha\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=gapminder,\n    x='gdp',\n    y='life_exp',\n    size='hdi_index',                  # map size to population\n    sizes=(20, 200),             # scale point size (min, max)\n    alpha=0.4                    # make points a bit transparent\n)\n\nplt.title('Life Expectancy vs GDP (Point Size = hdi_index)')\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))\nplt.tight_layout()\nplt.show()\n\nOur graph is still not very clear after adding the alpha this is because all the points are bunched up in only a small section of the graph. We can log the x-axis to make the points more spread out across the GDP axis.\n# Logging the axis\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=gapminder,\n    x='gdp',\n    y='life_exp',\n    size='hdi_index',                  # map size to population\n    sizes=(20, 200),             # scale point size (min, max)\n    alpha=0.4                    # make points a bit transparent\n)\n\nplt.title('Life Expectancy vs GDP (Point Size = hdi_index)')\nplt.xlabel('GDP')\nplt.ylabel('Life Expectancy')\nplt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))\nplt.tight_layout()\nplt.xscale('log')\nplt.show()\n We will now add the colour by continent parameter back.\nFor this notice that we are splitting the legends’ code into 2 separate sections.\n# Adding color by continent parameter back \nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(\n    data=gapminder,\n    x='gdp',\n    y='life_exp',\n    size='hdi_index',                  # map size to population\n    sizes=(20, 200),             # scale point size (min, max)\n    alpha=0.4,                       # make points a bit transparent\n    hue='continent'\n)\nplt.title('Life Expectancy vs GDP (Point Size = hdi_index)')\nplt.xlabel('GDP per capita (USD)')\nplt.ylabel('Life Expectancy (years)')\n\n# Separate legends for hue and size\nplt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))\nsns.move_legend(scatter, title='Continent', loc='upper left', bbox_to_anchor=(0.7, 0.65))\n\nplt.tight_layout # making sure the legends are in the frame\nplt.xscale('log')\nplt.show()\n\n\n\n\nLayers\nOnce you define your data and aesthetics (such as (x, y) coordinates, colour, size, etc.), you can add more layers to modify and enhance your plots.\n\nGeometric Objects: These are the graphical objects to be drawn, such as histograms, boxplots, density plots, etc.\nStatistics: These can be applied to your data, like calculating density or fitting a regression line.\nPosition Adjustments: These modify how elements are placed, such as jittering points or stacking bars.\n\nExample: Creating a Base plot - a histogram\n# Histogram with Position Adjustments\nplt.figure(figsize=(10, 6))\nsns.histplot(data=gapminder, x='life_exp', hue='continent', element='step', fill=True, alpha=0.3)\nplt.title('Histogram of Life Expectancy by Continent')\nplt.xlabel('Life Expectancy')\nplt.ylabel('Frequency')\nplt.show()\n\nthe “element” attribute can be one of “step”, “poly” or “bars” depending of what is needed. Try it out yourself.\nWe will now plot a density plot, a smoothed version of a histogram using geom_density; its default position is identity and both plots are equivalent.\n\nA histogram displays the frequency of data within bins, while a density plot represents the probability density function of the data, providing a smoothed continuous curve.\nTo convert a histogram to a density plot, you typically use sns.kdeplot, which computes and plots the kernel density estimate.\n\nIn our code below, the kdeplot function is used to create a density plot, and the common_norm=False ensures that the densities for each continent are normalized separately, allowing for a fair comparison of the distribution shapes without being influenced by the differing sizes of the groups.\n# filled Density Plot\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=gapminder, x='life_exp', hue='continent', fill=True, common_norm=False, alpha=0.3)\nplt.title('Density Plot of Life Expectancy by Continent')\nplt.xlabel('Life Expectancy')\nplt.ylabel('Density')\nplt.show()\n\nThe “Multiple Parameter” The multiple Parameter can be set to any of these values:\n\nlayer - Overlays categories, allowing overlap.\nstack - Stacks categories cumulatively.\nfill - Stacks and scales categories to equal height.\ndodge - Positions categories side by side.\n\ne.g. using the dodge version:\nplt.figure(figsize=(10, 6))\nsns.histplot(data=gapminder, x='life_exp', hue='continent', multiple='dodge', alpha=0.3)\nplt.title('Life Expectancy Distribution by Continent')\nplt.xlabel('Life Expectancy')\nplt.ylabel('Frequency')\nplt.show()\n \n\n\nFaucetting\nFacetting is a powerful technique in data visualisation that allows you to split one plot into multiple plots based on a factor included in the dataset. Python’s seaborn library provides this functionality.\nIn the Gapminder scatterplot example, we can use faceting to produce one scatter plot for each continent separately, using FacetGrid.\n\nDefine the Core Scatter Plot\nFirst, let’s define the core scatter plot of life expectancy vs GDP and store it in an object for easy reuse:\n# Base graph\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=gapminder, x='gdp', y='life_exp', hue='continent', alpha=0.5)\nplt.xscale('log')\nplt.title('Life Expectancy vs GDP per capita, 1998-2002')\nplt.xlabel('GDP per capita')\nplt.ylabel('Life Expectancy')\nplt.show()\n\nNow, let’s add a new layer to our base plot using FacetGrid to facet by continent:\n# Ensure 'continent' is treated as a categorical variable\ngapminder['continent'] = gapminder['continent'].astype('category')\n\n# Facet the scatter plot by continent with hue\ng = sns.FacetGrid(gapminder, col='continent', col_wrap=3, height=4)\ng.map_dataframe(sns.scatterplot, x='gdp', y='life_exp', hue='continent', alpha=0.5)\ng.set_titles(\"{col_name}\")\ng.set_axis_labels('GDP per capita', 'Life Expectancy')\ng.add_legend()\nplt.show()\n\ng.map_dataframe(...): Maps the sns.scatterplot function to each facet, passing the data explicitly and ensuring that\nhue='continent' colours the points appropriately in each facet.\n\n\n\nFacetGrid: This is used to create a grid of plots, allowing you to facet by a specified variable (continent in this case).\ncol_wrap: This parameter controls the number of columns in the facet grid, making it adaptable to different screen sizes.\nmap: This method maps a plotting function (sns.scatterplot) to each facet.\n\nFinally, if you want to create a boxplot of life expectancy by continent instead of a scatter plot, you can use similar aesthetics with Python libraries like matplotlib and seaborn. The key difference is the type of plot you choose to represent your data.\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=gapminder, x='continent', y='life_exp', hue='continent')\n\n# Add labels and title\nplt.title(\"Life Expectancy among the continents, 1952-2007\")\nplt.xlabel(\" \")  # Empty, as the levels of the x-variable are the continents\nplt.ylabel(\"Life Expectancy\")\nplt.figtext(0.9, 0.01, \"Source: Gapminder\", horizontalalignment='right')\n\n# Apply a minimal theme\nsns.set_theme(style=\"whitegrid\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\nAnimated Graphs\nAnimated graphs are a powerful way to visualize how data changes over time. In Python, one of the most commonly used tools for creating animations is Matplotlib’s FuncAnimation class. This function updates a plot frame-by-frame, allowing you to illustrate dynamic processes such as moving trends, changing patterns, or simulations. The final animation can be displayed directly in a notebook or exported as a GIF or video using writers like pillow or ffmpeg. Keep in mind that generating animations may take a few seconds, especially for longer sequences, as each frame is rendered individually.\n\n\nmatplotlib.animation\nTo implement animations, we will need to understand the concept of functions in Python. These are reusable blocks of code that perform specific tasks and can be called with different inputs to produce different outputs.\nIn the context of animations, a function—like update() in the code below - is used to define how each frame of the animation should be rendered based on changing input (e.g., the year). This function is then repeatedly called by FuncAnimation, allowing us to dynamically update the plot for each time step.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n\n# Set up the figure and axis\nfig, ax = plt.subplots(figsize=(8, 6))\nplt.xticks(rotation=45)\n\n# Get unique years sorted\nyears = sorted(gapminder['year'].unique())\n\ndef update(year_index):\n    ax.clear()  # Clear previous frame\n    year = years[year_index]\n    yearly_data = gapminder[gapminder['year'] == year]\n\n    # Create boxplot grouped by continent\n    yearly_data.boxplot(column='life_exp', by='continent', ax=ax)\n\n    # Customize plot appearance\n    ax.set_title(f'Life Expectancy by Continent ({year})')\n    ax.set_xlabel('Continent')\n    ax.set_ylabel('Life Expectancy')\n    plt.suptitle('')  # Remove automatic title added by Pandas boxplot\n\n# Create animation\nani = animation.FuncAnimation(fig, update, frames=len(years), interval=800, repeat=True)\n\nplt.tight_layout()\nplt.show()\n\n# Optional: Save as GIF (requires 'pillow')\nani.save('life_expectancy_boxplot.gif', writer='pillow')\n\nSimilarly, we can create an animated visualisation showing the relationship between GDP and life expectancy over time for different countries, segmented by continent.\n\nX-axis (GDP): Logarithmic scale representing each country’s GDP.\nY-axis (Life Expectancy): Life expectancy of each country’s population.\nData Points\n\nEach point represents a country for a specific year.\nColour-coded by continent:\n\nAsia: Red\nEurope: Blue\nAfrica: Green\nAmericas: Yellow\nOceania: Purple\nUnknown: Grey\n\n\nAnimation\n\nDisplays changes over time, with each frame representing a different year.\nTitle updates dynamically to reflect the current year.\n\n\n# Fill NaN values in the 'continent' column with a default value\ngapminder['continent'] = gapminder['continent'].fillna('Unknown')\n\n# Set up the figure and axis\nfig, ax = plt.subplots(figsize=(6, 6))\nplt.xticks(rotation=45)\n\n# Get unique years sorted\nyears = sorted(gapminder['year'].unique())\n\ndef update(year_index):\n    ax.clear()  # Clear previous frame\n    year = years[year_index]\n    yearly_data = gapminder[gapminder['year'] == year]\n\n    # Ensure there are no NaN values in the data used for plotting\n    yearly_data = yearly_data.dropna(subset=['gdp', 'life_exp'])\n\n    # Map continents to colours, handling 'Unknown' as grey\n    continent_colors = yearly_data['continent'].map({\n        'Asia': 'red', 'Europe': 'blue', 'Africa': 'green', \n        'Americas': 'yellow', 'Oceania': 'purple', 'Unknown': 'grey'\n    }).fillna('grey')  # Fill any remaining NaN values with 'grey'\n\n    # Create scatter plot\n    scatter = ax.scatter(yearly_data['gdp'], yearly_data['life_exp'], \n                         c=continent_colors, alpha=0.5)\n\n    # Customize plot appearance\n    ax.set_title(f'Year: {year}')\n    ax.set_xlabel('GDP')\n    ax.set_ylabel('Life Expectancy')\n    ax.set_xscale('log')  # Use logarithmic scale for GDP\n    ax.set_xlim(gapminder['gdp'].min(), gapminder['gdp'].max())\n    ax.set_ylim(gapminder['life_exp'].min(), gapminder['life_exp'].max())\n\n# Create animation\nani = animation.FuncAnimation(fig, update, frames=len(years), interval=800, repeat=True)\n\nplt.tight_layout()\nplt.show()\n\n# Optional: Save as GIF (requires 'pillow')\nani.save('life_expectancy_vs_gdp.gif', writer='pillow', dpi=80)\n\n\n\n\n\nWhy you should always plot your data\nWe have touched on the basics of python visualisations, but in this section we wanted to discuss why one should always plot the data and not just rely on tables of summary statistics.\nLet us consider thirteen datasets all of which have 142 observations of (x,y) values. The table below shows the average value of X and Y, the standard deviation of X and Y, as well as the correlation coefficient between X and Y.\n\n\n\nid\nn\nmean_x\nmean_y\nsd_x\nsd_y\ncorrelation\n\n\n\n\n1\n142\n54.3\n47.8\n6.82\n6.9\n-0.064\n\n\n2\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n3\n142\n54.3\n47.8\n6.82\n6.9\n-0.068\n\n\n4\n142\n54.3\n47.8\n6.82\n6.9\n-0.064\n\n\n5\n142\n54.3\n47.8\n6.82\n6.9\n-0.060\n\n\n6\n142\n54.3\n47.8\n6.82\n6.9\n-0.062\n\n\n7\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n8\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n9\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n10\n142\n54.3\n47.8\n6.82\n6.9\n-0.063\n\n\n11\n142\n54.3\n47.8\n6.82\n6.9\n-0.069\n\n\n12\n142\n54.3\n47.8\n6.82\n6.9\n-0.067\n\n\n13\n142\n54.3\n47.8\n6.82\n6.9\n-0.066\n\n\n\nSince our datasets contain values for X and Y, we can estimate 13 regression models and plot the values for each of the 13 intercepts and slope for X.\n\nIf we just looked at either the summary statistics table, or the plots of intercepts and slopes, we may be tempted to conclude that the 13 datasets are either identical or very much alike. However, this is far from the truth, as this is what the 13 individual datasets look like.\n\nYou can read more about why you should never trust summary statistics alone and should always visualize your data.\n\n\n\nFurther resources\n\nOfficial seaborn Tutorial\nmatplotlib Pyplot Tutorial\nDataCamp - Data Visualization with Seaborn\nDatacamp - Introduction to Data Visualization with Matplotlib\nseaborn Gallery\nmatplotlib Examples"
  },
  {
    "objectID": "Examples.html#manipulate-data",
    "href": "Examples.html#manipulate-data",
    "title": "Examples",
    "section": "5. Manipulate Data",
    "text": "5. Manipulate Data"
  },
  {
    "objectID": "Examples.html#reshape-data",
    "href": "Examples.html#reshape-data",
    "title": "Examples",
    "section": "6. Reshape Data",
    "text": "6. Reshape Data"
  },
  {
    "objectID": "Examples.html#eda-for-modelling",
    "href": "Examples.html#eda-for-modelling",
    "title": "Examples",
    "section": "7. EDA for Modelling",
    "text": "7. EDA for Modelling"
  },
  {
    "objectID": "Assignments.html",
    "href": "Assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment details\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nWeekly Homeworks\nFinal Project\nFinal Exam\n\n\n\n\n\n\nWeekly Homeworks\nTo practice writing R code, exploring datasets, and running statistical models you will complete a series of weekly homeworks.\nThere are 3 weekly homeworks to be undertaken with your group and these will be graded using a check system:\n\n5/5: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often.\n3/5: Problem set is 70–99% complete and most answers are correct. This is the expected level of performance.\n1/5: Problem set is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often.\n\n\n\n\nFinal Project\nAt the end of the course, you will demonstrate your knowledge by working on a group assignment… Complete details for the final project will be made available here.\n\n\n\nFinal Exam\nThere will be a final exam that will cover: * 1. Programming in R and the tidyverse (about a third of total marks) * the core statistical tools of inferential statistics and linear models.\nYou will take the final exam in-person in a lecture theatre. This is a closed-book, 1.5-hour exam. A formula sheet will be provided, but you will not have online access to Google.\n\n\n\n\nHomeworks\n\n\nHomework 1\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nhomework1.Rmd\nInstruction\nRubric\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nhomework1.Rmd\nInstruction\nRubric\n\n\n\n\n\n\n\nHomework 3\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nhomework1.Rmd\nInstruction\nRubric\n\n\n\n\n\n\n\n\nProjects\n\n\nFinal Group Project\n\n\n\n\n\n\nContents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Exam\n\n\n\n\n\n\nContents\n\n\n\n\n\n\n\n\n\n\n\n\nDue by 11:59 PM on Friday, September 20, 2024"
  },
  {
    "objectID": "Content.html",
    "href": "Content.html",
    "title": "Content",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before the lecture or working through the lesson.\nI have included a set of questions that might guide your reflection response. You should not try to respond to all of these (or any of them if you don’t want to)– they’ll just help you know what to look for and think about as you read."
  },
  {
    "objectID": "Content.html#workshop-1-import-visualise-and-manipulate-data",
    "href": "Content.html#workshop-1-import-visualise-and-manipulate-data",
    "title": "Content",
    "section": "Workshop 1: Import, visualise, and manipulate data",
    "text": "Workshop 1: Import, visualise, and manipulate data\n\nRead before class on Wednesday, August 28, 2024\n\n\n\n\n\n\n\nContents\n\n\n\n\n\n\nReadings\n\nRecommended\n\nQuestions to Reflect on\nSession Files\n\n\n\n\n\nReadings\n\nHans Rosling, “200 Countries, 200 Years, 4 Minutes”\nChapter 2 in Claus Wilke, Fundamentals of Data Visualization\nChapter 3 in Kieran Healy, Data Visualization\n\n\nRecommended\n\nThe fullest look yet at the racial inequality of Coronavirus\n\n\n\n\nQuestions to Reflect on\n\nWhat data was mapped to which aesthetics in Rosling’s video?\nWhat data would you need to create the bar plot in NYT’s article?\n\n\n\nSession Files\n\nSlides for today’s session are available on Canvas.\nYou can download all session files (data, code, etc.) by pulling from course Github repo.\n\ngit clone https://github.com/kostis-christodoulou/am01.git\nAlternatively, download the zipped files directly from here. Then unzip and open them in your Python IDE (e.g., Jupyter, VS Code, or Spyder)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics with Python",
    "section": "",
    "text": "Instructor\n\n\n\n\nCourse details\n\n\n\n\nContact me"
  },
  {
    "objectID": "Schedule.html",
    "href": "Schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the course\n\nContent (): This contains the readings, slides, data files, etc. for each session. These will also be added on Canvas on the day of each session. It helps to read the material before each session.\nExample () : This page contains worked examples of fully annotated R code that you can use as a reference. This is only a reference page—you don’t have to necessarily do anything here.\nExercise (): These are interactive exercises where you have to provide R code in your browser to solve a problem, much like Datacamp. These are not graded, but are always there for your reference.\nAssignment (): This page contains instructions for the three workshop exercises (3-4 brief tasks plus a challenge), for the individual portfolio website project, and the final group project.\n\nAssignments are due by 11:59 PM UTC on the day they’re listed.\n\n\n\nWeek\nDate\nSession\nContent\nExample\nExercise\nAssignment\n\n\n\n\nFoundations: EDA and Intro to Data Science\n\n\n\n\n\n\n\n\n1\n28th Aug\nLecture 1: Exploratory Data Analysis\n()\n()\n()\n-\n\n\n2\n28th Aug\nWorkshop 1: Import, visualise, and manipulate data\n()\n()\n()\n-\n\n\n\n29th Aug\nHomework 1 due\n-\n-\n-\n()\n\n\nInferential Statistics\n\n\n\n\n\n\n\n\n3\n30th Aug\nLecture 2: Sampling and Probability Distributions\n()\n()\n()\n-\n\n\n4\n30th Aug\nWorkshop 2: Confidence Intervals; reshape data\n()\n()\n()\n-\n\n\n\n03rd Sep\nHomework 2 due\n-\n-\n-\n()\n\n\n5\n04th Sep\nLecture 3: Hypothesis Testing; there is only one test\n()\n()\n()\n-\n\n\n6\n04th Sep\nWorkshop 3: Hypothesis testing; A/B testing; simulating\n()\n()\n-\n-\n\n\n\n08th Sep\nHomework 3 due\n-\n-\n-\n()\n\n\nRegression Modelling\n\n\n\n\n\n\n\n\n7\n09th Sep\nLecture 4: Introduction to regression models\n()\n()\n-\n-\n\n\n8\n09th Sep\nWorkshop 4: Workshop on regression\n()\n()\n-\n-\n\n\n9\n11th Sep\nLecture 5: Further regression; regression diagnostics\n()\n()\n-\n-\n\n\n10\n11th Sep\nFinal group project\n()\n-\n-\n-\n\n\n\n11th Sep\nFinal Group Project due\n-\n-\n-\n()\n\n\n\n18th Sep\nRevision Session\n-\n-\n-\n-\n\n\n\n20th Sep\nFinal Exam\n-\n-\n-\n()"
  }
]