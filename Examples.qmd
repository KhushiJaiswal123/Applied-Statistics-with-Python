---
title: "Examples"
toc-title: "Overview"
toc-location: left
jupyter: python3
---

# Worked Examples

::: {.callout-tip collapse=false}
## Contents
- [Exploratory Data Analysis](#EDA)
- [Data Modelling](#Modelling)
:::

This section contains worked examples, most of them with fully annotated Python code that you can use as reference.

### Exploratory Data Analysis {#EDA .unnumbered .unlisted}
Before starting any analysis, we first need to import a dataset, understand its variables, visualize it, and manipulate it systematically using tools like `pandas`, `matplotlib`, and `seaborn.` This might seem like a tedious step, but it's a critical foundation that must precede any form of statistical modelling.

### Data Modelling {#Modelling .unnumbered .unlisted}
Data modelling allows us to move beyond describing individual variables — instead, we use models to learn from data. At the core of this is understanding the relationship:
```python
# General form of a predictive model
outcome = f(features) + error

```
We begin with Exploratory Data Analysis tailored for modelling, and then proceed with three key approaches:

1. **EDA for Modelling**
After importing and cleaning the data (using pandas), we start looking at summary statistics and plots that will be useful in framing our modelling approach

2. **Testing for Differences in Means** across samples: How do we know whether there is a **statistically significant difference** between two groups A and B? E.g., between those who took a drug versus those than a placebo? Or whether there is a difference in the percentage of people who approve of Donald Trump is lower than those who disapprove of him?

3. **Fitting a Linear Regression Model**
To understand which features are associated with a numerical outcome **Y**, we use Linear Regression from `scikit-learn`.
    We try to explain the effect that specific explanatory variables, **X**, have on **Y**

4. **Fitting a Binary Classification Model**
where the difference is that the outcome variable, **Y**, is binary (0/1). Again we want to use our model primarily for explanation, e.g., what is the effect of different explanatory variables **X**'s on e.g., the probability that someone with Covid-19 will die?

<!--#################################################-->

# **Exploratory Data Analysis**

<!--#################################################-->

## 1. Import Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overview)
    - [Importing CSV files: pandas.read_csv()](#importing)
    - [Importing CSV files saved locally](#localImport)
- [Need for speed: `dask` and `vaex`](#speed)
- [Other data formats](#OtherFormats)
- [Never work directly on the raw data](#rawData)
- [other Links](#otherLinks)

:::

> Learning Objectives
>
> 1. Load external data from a .csv file into a data frame.
>
> 2. Describe what a data frame is.
>
> 3. Use indexing to subset specific portions of data frames.
>
> 4. Describe what a factor is.
>
> 5. Reorder and rename factors.
>
> 6. Format dates.


### Overview {#overview .unnumbered .unlisted}

When working with Python, data importation is generally achieved using libraries like `pandas`, which provides powerful tools for data manipulation, including importing data from various file formats.

<!--#################################################-->

### Importing CSV files: pandas.read_csv() {#importing .unnumbered .unlisted}

CSV files can be imported using the `read_csv()` function from the `pandas` library. This function is fast and user-friendly, allowing you to read flat data without complex configurations.
```python
import pandas as pd

# Importing CSV file from a URL
url = "https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv"
weather = pd.read_csv(url, skiprows=1, na_values="***")
```
Options Used:
  * `skiprows=1`: This skips the first row, assuming the actual data starts from the second row.
  * `na_values= "***"` : This treats astriks' as missing values, converting them to NaN.

To view the structure of the dataframe, you can use:
```python
print(weather.info())
print(weather.head())
```

<!--#################################################-->

### Importing CSV files saved locally {#localImport .unnumbered .unlisted}

To import a CSV file saved locally, simply provide the file path to the `read_csv()` function:
```python 
weather_local = pd.read_csv("path/to/your/localfile.csv")
```


<!--#################################################-->

### Need for speed: `dask` and `vaex` {#speed .unnumbered .unlisted}
For handling large datasets, libraries like dask and vaex can be used, which offer faster data processing capabilities compared to pandas.
```python 
import dask.dataframe as dd

weather_large = dd.read_csv("path/to/largefile.csv")
```


<!--#################################################-->

### Other data formats {#OtherFormats .unnumbered .unlisted}

Python offers several libraries for reading and writing various data formats:

  * Excel files: `pandas.read_excel()`
  * JSON: `json` library
  * Web APIs: `requests` library
  * Databases: `sqlite3`
  * Big Data: `pyspark`

<!--#################################################-->

### Never work directly on the raw data {#rawData .unnumbered .unlisted}

In 2012 Cecilia Giménez, an 83-year-old widow and amateur painter, attempted to restore a century-old fresco of Jesus crowned with thorns in her local church in Borja, Spain. The restoration didn’t go very well, but, surprisingly, the [botched restoration of Jesus fresco miraculously saved the Spanish Town](https://news.artnet.com/art-world/botched-restoration-of-jesus-fresco-miraculously-saves-spanish-town-197057).

![](Images/raw_data_example.png)

As a most important rule, please do not work on the raw data; it’s unlikely you will have Cecilia Giménez’s good fortune to become (in)famous for your not-so-brilliant work.


Make sure you always work on a copy of your raw data. Use Python's data manipulation libraries to clean and transform your data, and save the results to a new file, ensuring the original data remains intact.
```python 
weather_cleaned = weather.dropna()  # Example of cleaning data
weather_cleaned.to_csv("cleaned_data.csv", index=False)
```


<!--#################################################-->

### Other Links {#otherLinks .unnumbered .unlisted}

  * [For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)
  * Data Wrangling with `pandas`: [Pandas Documentation](https://pandas.pydata.org/docs/)
  * Big Data Processing with `dask` and `vaex`: [Dask Documentation](https://dask.org/), [Vaex Documentation](https://vaex.io/docs/)

<!--#################################################-->

## 2. Inspect Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overviewof2)
- [Viewing Data](#viewing)
- [Detailed Inspection](#inspections)
- [Key Questions](#questions)

:::

<!--#################################################-->

### Overview {#overviewof2 .unnumbered .unlisted}
Once you have loaded your dataset into Python, it's essential to inspect and understand the data. Typically, you want to know:

  * The dimensions of the dataset (number of rows and columns).
  * The types of variables (integer, string, boolean, etc.).
  * The number of missing values.
  * Summary statistics for numeric data.

<!--#################################################-->

### Viewing Data {#viewing .unnumbered .unlisted}

In Python, you can use pandas to view and inspect your data. The pandas library provides several functions to achieve this.

`pandas.DataFrame.info()` and `pandas.DataFrame.describe()`

These functions help you understand the structure and summary statistics of your data.

```python 
import pandas as pd

# Load data
gapminder = pd.read_csv("path/to/gapminder.csv")

# View the structure of the dataframe
print(gapminder.info())

# Summary statistics
print(gapminder.describe())
```

Using `info()`: Provides the number of rows, columns, and data types of each column. Using `describe()` : Offers summary statistics for numeric columns, including count, mean, standard deviation, min, and max values.

<!--#################################################-->

### Detailed Inspection {#inspections .unnumbered .unlisted}

For a detailed inspection, you can use the pandas_profiling library, which provides an extensive report on your dataframe.

```python 
from pandas_profiling import ProfileReport

# Generate a report
profile = ProfileReport(gapminder)
profile.to_file("gapminder_report.html")
```

Example Analysis on London Bikes Data
```python 
bikes = pd.read_csv("path/to/londonBikes.csv")

# Use pandas_profiling for a detailed report
bikes_profile = ProfileReport(bikes)
bikes_profile.to_file("london_bikes_report.html")
```

<!--#################################################-->

### Key Questions {#questions .unnumbered .unlisted}

using the **London Bikes Data** from above

1. What kind of variable is **date**?
    * date is typically a string or datetime variable. You can convert it using `pd.to_datetime()`.
    
    ```python
    bikes['date'] = pd.to_datetime(bikes['date'])
    ```
2. What kind of variable is **season**?
    * season is likely a categorical variable. You can convert it using `pd.Categorical()`.
    
    ```python 
    bikes['season'] = pd.Categorical(bikes['season'])
    ```

3. How often does it rain in London?
    * Count the occurrences in the rain column.
    
    ```python 
    rain_count = bikes['rain'].sum()
print(f"It rains {rain_count} times in the dataset.")
```

4. What is the average annual temperature (in degrees C)?
    * Calculate the mean of the temperature columns.
    
    ```python 
    avg_temp = bikes[['max_temp', 'min_temp', 'avg_temp']].mean().mean()
print(f"The average annual temperature is {avg_temp:.2f} degrees C.")
```

5. What is the maximum rainfall?
    * Find the maximum value in the `rainfall_mm` column.
    
    ```python
    max_rainfall = bikes['rainfall_mm'].max()
print(f"The maximum rainfall recorded is {max_rainfall} mm.")
    ```

<!--#################################################-->

## 3. Clean Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overviewof3)
- [Cleaning Variable Names with pandas](#cleaning)
- [Code Quality](#codeQUality)
- [Other Links](#otherLinks)

:::

<!--#################################################-->

### Overview {#overviewof3 .unnumbered .unlisted}

When creating data files, it's common to use variable names and formats that are human-readable but not ideal for computational processing. In Python, the `pandas` library can be used to clean and standardise variable names for easier manipulation.

<!--#################################################-->

### Cleaning Variable Names with pandas {#cleaning .unnumbered .unlisted}

In `Python`, we can use pandas and custom code to read data and clean column names to make them more suitable for analysis.
```python 
import pandas as pd

# Load Excel file
roster = pd.read_excel("path/to/dirty_data.xlsx")

# Clean column names using pandas string methods
roster.columns = (
    roster.columns
        .str.strip()  # Remove leading and trailing spaces
        .str.lower()  # Convert to lowercase
        .str.replace(' ', '_')  # Replace spaces with underscores
        .str.replace('%', 'percent')  # Replace '%' with 'percent'
        .str.replace('[^a-zA-Z0-9_]', '', regex=True)  # Remove special characters
)

# Inspect cleaned dataframe
print(roster.head())

```

  * The custom code directly modifies the column names using pandas string methods. It removes spaces, converts to lowercase, replaces spaces with underscores, and removes special characters using a regular expression.
  * Regular Expression: [^a-zA-Z0-9_] is used to remove any character that is not alphanumeric or an underscore.
  * `pandas.read_excel()`: Used to read Excel files into a DataFrame.


<!--#################################################-->

### Code Quality {#codeQUality .unnumbered .unlisted}

According to Phil Karlton, there are only two [hard things in Computer Science: cache invalidation and naming things](https://martinfowler.com/bliki/TwoHardThings.html). It's crucial to write code that is not only functional but also maintainable and readable. Use meaningful names for variables and dataframes, and include comments to explain complex logic.

<!--#################################################-->

### Other Links {#otherLinks .unnumbered .unlisted}

* [For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)

<!--#################################################-->

## 4. Visualise Data

[gapminder Dataset](https://www.kaggle.com/datasets/albertovidalrod/gapminder-dataset?resource=download)

<!--#################################################-->

## 5. Manipulate Data

<!--#################################################-->

## 6. Reshape Data 

<!--#################################################-->

## 7. EDA for Modelling




<!--#################################################-->

# **Statistical Inference**




<!--#################################################-->


# **Data Modelling**
