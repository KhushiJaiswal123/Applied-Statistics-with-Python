---
title: "Examples"
toc-title: "Overview"
toc-location: left
jupyter: python3
---

# Worked Examples

::: {.callout-tip collapse=false}
## Contents
- [Exploratory Data Analysis](#EDA)
- [Data Modelling](#Modelling)
:::

This section contains worked examples, most of them with fully annotated Python code that you can use as reference.

### Exploratory Data Analysis {#EDA .unnumbered .unlisted}
Before starting any analysis, we first need to import a dataset, understand its variables, visualize it, and manipulate it systematically using tools like `pandas`, `matplotlib`, and `seaborn.` This might seem like a tedious step, but it's a critical foundation that must precede any form of statistical modelling.

### Data Modelling {#Modelling .unnumbered .unlisted}
Data modelling allows us to move beyond describing individual variables — instead, we use models to learn from data. At the core of this is understanding the relationship:
```python
# General form of a predictive model
outcome = f(features) + error

```
We begin with Exploratory Data Analysis tailored for modelling, and then proceed with three key approaches:

1. **EDA for Modelling**
After importing and cleaning the data (using pandas), we start looking at summary statistics and plots that will be useful in framing our modelling approach

2. **Testing for Differences in Means** across samples: How do we know whether there is a **statistically significant difference** between two groups A and B? E.g., between those who took a drug versus those than a placebo? Or whether there is a difference in the percentage of people who approve of Donald Trump is lower than those who disapprove of him?

3. **Fitting a Linear Regression Model**
To understand which features are associated with a numerical outcome **Y**, we use Linear Regression from `scikit-learn`.
    We try to explain the effect that specific explanatory variables, **X**, have on **Y**

4. **Fitting a Binary Classification Model**
where the difference is that the outcome variable, **Y**, is binary (0/1). Again we want to use our model primarily for explanation, e.g., what is the effect of different explanatory variables **X**'s on e.g., the probability that someone with Covid-19 will die?

<!--#################################################-->

# **Exploratory Data Analysis**

<!--#################################################-->

## 1. Import Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overview)
    - [Importing CSV files: pandas.read_csv()](#importing)
    - [Importing CSV files saved locally](#localImport)
- [Need for speed: `dask` and `vaex`](#speed)
- [Other data formats](#OtherFormats)
- [Never work directly on the raw data](#rawData)
- [other Links](#otherLinks)

:::

> Learning Objectives
>
> 1. Load external data from a .csv file into a data frame.
>
> 2. Describe what a data frame is.
>
> 3. Use indexing to subset specific portions of data frames.
>
> 4. Describe what a factor is.
>
> 5. Reorder and rename factors.
>
> 6. Format dates.


### Overview {#overview .unnumbered .unlisted}

When working with Python, data importation is generally achieved using libraries like `pandas`, which provides powerful tools for data manipulation, including importing data from various file formats.

<!--#################################################-->

### Importing CSV files: pandas.read_csv() {#importing .unnumbered .unlisted}

CSV files can be imported using the `read_csv()` function from the `pandas` library. This function is fast and user-friendly, allowing you to read flat data without complex configurations.
```python
import pandas as pd

# Importing CSV file from a URL
url = "https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv"
weather = pd.read_csv(url, skiprows=1, na_values="***")
```
Options Used:
  * `skiprows=1`: This skips the first row, assuming the actual data starts from the second row.
  * `na_values= "***"` : This treats astriks' as missing values, converting them to NaN.

To view the structure of the dataframe, you can use:
```python
print(weather.info())
print(weather.head())
```

<!--#################################################-->

### Importing CSV files saved locally {#localImport .unnumbered .unlisted}

To import a CSV file saved locally, simply provide the file path to the `read_csv()` function:
```python 
weather_local = pd.read_csv("path/to/your/localfile.csv")
```


<!--#################################################-->

### Need for speed: `dask` and `vaex` {#speed .unnumbered .unlisted}
For handling large datasets, libraries like dask and vaex can be used, which offer faster data processing capabilities compared to pandas.
```python 
import dask.dataframe as dd

weather_large = dd.read_csv("path/to/largefile.csv")
```


<!--#################################################-->

### Other data formats {#OtherFormats .unnumbered .unlisted}

Python offers several libraries for reading and writing various data formats:

  * Excel files: `pandas.read_excel()`
  * JSON: `json` library
  * Web APIs: `requests` library
  * Databases: `sqlite3`
  * Big Data: `pyspark`

<!--#################################################-->

### Never work directly on the raw data {#rawData .unnumbered .unlisted}

In 2012 Cecilia Giménez, an 83-year-old widow and amateur painter, attempted to restore a century-old fresco of Jesus crowned with thorns in her local church in Borja, Spain. The restoration didn’t go very well, but, surprisingly, the [botched restoration of Jesus fresco miraculously saved the Spanish Town](https://news.artnet.com/art-world/botched-restoration-of-jesus-fresco-miraculously-saves-spanish-town-197057).

![](Images/raw_data_example.png)

As a most important rule, please do not work on the raw data; it’s unlikely you will have Cecilia Giménez’s good fortune to become (in)famous for your not-so-brilliant work.


Make sure you always work on a copy of your raw data. Use Python's data manipulation libraries to clean and transform your data, and save the results to a new file, ensuring the original data remains intact.
```python 
weather_cleaned = weather.dropna()  # Example of cleaning data
weather_cleaned.to_csv("cleaned_data.csv", index=False)
```


<!--#################################################-->

### Other Links {#otherLinks .unnumbered .unlisted}

  * [For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)
  * Data Wrangling with `pandas`: [Pandas Documentation](https://pandas.pydata.org/docs/)
  * Big Data Processing with `dask` and `vaex`: [Dask Documentation](https://dask.org/), [Vaex Documentation](https://vaex.io/docs/)

<!--#################################################-->

## 2. Inspect Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overviewof2)
- [Viewing Data](#viewing)
- [Detailed Inspection](#inspections)
- [Key Questions](#questions)

:::

<!--#################################################-->

### Overview {#overviewof2 .unnumbered .unlisted}
Once you have loaded your dataset into Python, it's essential to inspect and understand the data. Typically, you want to know:

  * The dimensions of the dataset (number of rows and columns).
  * The types of variables (integer, string, boolean, etc.).
  * The number of missing values.
  * Summary statistics for numeric data.

<!--#################################################-->

### Viewing Data {#viewing .unnumbered .unlisted}

In Python, you can use pandas to view and inspect your data. The pandas library provides several functions to achieve this.

`pandas.DataFrame.info()` and `pandas.DataFrame.describe()`

These functions help you understand the structure and summary statistics of your data.

```python 
import pandas as pd

# Load data
gapminder = pd.read_csv("path/to/gapminder.csv")

# View the structure of the dataframe
print(gapminder.info())

# Summary statistics
print(gapminder.describe())
```

Using `info()`: Provides the number of rows, columns, and data types of each column. Using `describe()` : Offers summary statistics for numeric columns, including count, mean, standard deviation, min, and max values.

<!--#################################################-->

### Detailed Inspection {#inspections .unnumbered .unlisted}

For a detailed inspection, you can use the pandas_profiling library, which provides an extensive report on your dataframe.

```python 
from pandas_profiling import ProfileReport

# Generate a report
profile = ProfileReport(gapminder)
profile.to_file("gapminder_report.html")
```

Example Analysis on London Bikes Data
```python 
bikes = pd.read_csv("path/to/londonBikes.csv")

# Use pandas_profiling for a detailed report
bikes_profile = ProfileReport(bikes)
bikes_profile.to_file("london_bikes_report.html")
```

<!--#################################################-->

### Key Questions {#questions .unnumbered .unlisted}

using the **London Bikes Data** from above

1. What kind of variable is **date**?
    * date is typically a string or datetime variable. You can convert it using `pd.to_datetime()`.
    
    ```python
    bikes['date'] = pd.to_datetime(bikes['date'])
    ```
2. What kind of variable is **season**?
    * season is likely a categorical variable. You can convert it using `pd.Categorical()`.
    
    ```python 
    bikes['season'] = pd.Categorical(bikes['season'])
    ```

3. How often does it rain in London?
    * Count the occurrences in the rain column.
    
    ```python 
    rain_count = bikes['rain'].sum()
print(f"It rains {rain_count} times in the dataset.")
```

4. What is the average annual temperature (in degrees C)?
    * Calculate the mean of the temperature columns.
    
    ```python 
    avg_temp = bikes[['max_temp', 'min_temp', 'avg_temp']].mean().mean()
print(f"The average annual temperature is {avg_temp:.2f} degrees C.")
```

5. What is the maximum rainfall?
    * Find the maximum value in the `rainfall_mm` column.
    
    ```python
    max_rainfall = bikes['rainfall_mm'].max()
print(f"The maximum rainfall recorded is {max_rainfall} mm.")
    ```

<!--#################################################-->

## 3. Clean Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overviewof3)
- [Cleaning Variable Names with pandas](#cleaning)
- [Code Quality](#codeQUality)
- [Other Links](#otherLinks)

:::

<!--#################################################-->

### Overview {#overviewof3 .unnumbered .unlisted}

When creating data files, it's common to use variable names and formats that are human-readable but not ideal for computational processing. In Python, the `pandas` library can be used to clean and standardise variable names for easier manipulation.

<!--#################################################-->

### Cleaning Variable Names with pandas {#cleaning .unnumbered .unlisted}

In `Python`, we can use pandas and custom code to read data and clean column names to make them more suitable for analysis.
```python 
import pandas as pd

# Load Excel file
roster = pd.read_excel("path/to/dirty_data.xlsx")

# Clean column names using pandas string methods
roster.columns = (
    roster.columns
        .str.strip()  # Remove leading and trailing spaces
        .str.lower()  # Convert to lowercase
        .str.replace(' ', '_')  # Replace spaces with underscores
        .str.replace('%', 'percent')  # Replace '%' with 'percent'
        .str.replace('[^a-zA-Z0-9_]', '', regex=True)  # Remove special characters
)

# Inspect cleaned dataframe
print(roster.head())

```

  * The custom code directly modifies the column names using pandas string methods. It removes spaces, converts to lowercase, replaces spaces with underscores, and removes special characters using a regular expression.
  * Regular Expression: [^a-zA-Z0-9_] is used to remove any character that is not alphanumeric or an underscore.
  * `pandas.read_excel()`: Used to read Excel files into a DataFrame.


<!--#################################################-->

### Code Quality {#codeQUality .unnumbered .unlisted}

According to Phil Karlton, there are only two [hard things in Computer Science: cache invalidation and naming things](https://martinfowler.com/bliki/TwoHardThings.html). It's crucial to write code that is not only functional but also maintainable and readable. Use meaningful names for variables and dataframes, and include comments to explain complex logic.

<!--#################################################-->

### Other Links {#otherLinks .unnumbered .unlisted}

* [For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)

<!--#################################################-->

## 4. Visualise Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overviewof4)
- [Layers](#layers)
- [Faucetting](#faucetting)
- [Animated Graphs](#animated)
- [Why you should always plot your data](#whyPlot)
    - [what data Patterns can lie behind a correlation coefficient?](#hiddenPatterns)
- [Further resources](#furtherResources4)

:::
<!--#################################################-->
> Learning Objectives
> 1. Produce scatter plots, boxplots, and time series plots using matplotlib and seaborn.
> 2. Set universal plot settings.
> 3. Describe what faceting is and apply faceting using seaborn
> 4. Modify the aesthetics of an existing plot (including axis labels and colour).
> 5. Build complex and customised plots from data in a DataFrame.

<!--#################################################-->

### Overview {#overviewof4 .unnumbered .unlisted}

> “Visualization is a powerful tool for understanding data. You can often discover patterns, spot anomalies, and develop an intuition for the data just by looking at it.”  
>  
> — Wes McKinney, creator of pandas  

We will explore how to create insightful and aesthetically pleasing data visualisations using two powerful Python libraries: `matplotlib` and `seaborn`.  
We will work through examples of various plot types - scatter plots, boxplots, and histograms - and learn to customise them for clarity and impact.  

It may seem verbose and unwieldy, but the idea of building a plot on a layer-by-layer basis is very powerful.

* You begin a plot by defining the dataset you will use. 
* Then, specify aesthetics, namely (x, y) coordinates, colour, size, etc. 
* Finally, choose the geometric shape to represent your data, and add more layers like legends, labels, facets, etc.

For example, using the [Gapminder](https://www.kaggle.com/datasets/albertovidalrod/gapminder-dataset?resource=download) dataset with data on life expectancy (life_exp), human development index (hdi_index), and GDP (gdp) for a number of countries, we can build a graph that shows the relationship between GDP and life expectancy.

As we said, first we define the dataset we are using
```python
import pandas as pd
gapminder = pd.read_csv(r"filepath-to-your-dataset")
```

The next thing is to map aesthetics. In our case, we will map gdpPercap to the x-axis, and lifeExp to the y-axis.

```python 
# Basic Plot Setup
plt.figure(figsize=(10, 6))
sns.scatterplot(data=gapminder, x='gdp', y='life_exp')
plt.title('Life Expectancy vs GDP')
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.show()
```
![](Images/BasicPlotSetup.png)

* `plt.figure(figsize=(10, 6))`: Defines the size of the plot.
* `sns.scatterplot(...)`: Uses Seaborn to draw a scatter plot from the gapminder dataset.
* `plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`: Add meaningful labels and a title to make the chart easy to understand.
* `plt.show()`: Renders the plot in your output window or notebook.


What if we wanted to colour the points by the continent each country is in? For this we will need to use the "continent" column from our gapminder dataset. Seaborn makes this easy using the hue parameter.


```python 
# Colored Scatter Plot by Continent
plt.figure(figsize=(10, 6))  # Set figure size
sns.scatterplot(data=gapminder, x='gdp', y='life_exp', hue='continent')  # Color points by continent
plt.title('Life Expectancy vs GDP by Continent')  # Updated title
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='Continent')  # Add a legend with a title
plt.show()
```

![](Images/ColouredScatterPlot.png)

* `hue='continent'`: Tells Seaborn to color the points based on the continent each country belongs to.
* `plt.legend(...)`: Makes sure the legend is clear and labeled.

What if instead of a scatter plot we wanted to create a line plot? For this we need to: 

* change the `sns.scatterplot` to `sns.lineplot`


```python 
# Colored Line Plot by Continent
plt.figure(figsize=(10, 6))  # Set figure size
sns.lineplot(data=gapminder, x='gdp', y='life_exp', hue='continent')  # Color points by continent
plt.title('Life Expectancy vs GDP by Continent')  # Updated title
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='Continent')  # Add a legend with a title
plt.show()
```


![](Images/LineGraph.png)

However, this is not a particularly useful plot, so let us go back to our scatter plot.

What if we wanted to have the size of each point correspond to the HDI index of the country?
For this we will :

* add the `size='hdi_index'` parameter in the `scatterplot()` function
* add the `sizes=(20, 200)` parameter in the `scatterplot()` function to specify the range of sizes of scatter points we want to use. 

```python
# going back to scatter plot but now the size of points coresspond to the population of the country
# Basic Plot Setup with size mapped to population
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=gapminder,
    x='gdp',
    y='life_exp',
    size='hdi_index',                  # map size to population
    sizes=(20, 200),             # scale point size (min, max)
)

plt.title('Life Expectancy vs GDP (Point Size = hdi_index)')
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))
plt.tight_layout()
plt.show()
```

![](Images/hdi_index.png)
But the points above are overlapping with each other. We can set the `alpha` variable between 0 and 1 to specify how transparent each point will be. This will let us see all the points better.

```python 
# Adding Alpha
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=gapminder,
    x='gdp',
    y='life_exp',
    size='hdi_index',                  # map size to population
    sizes=(20, 200),             # scale point size (min, max)
    alpha=0.4                    # make points a bit transparent
)

plt.title('Life Expectancy vs GDP (Point Size = hdi_index)')
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))
plt.tight_layout()
plt.show()

```
![](Images/Alpha.png)

Our graph is still not very clear after adding the `alpha` this is because all the points are bunched up in only a small section of the graph. We can *log* the x-axis to make the points more spread out across the GDP axis.

```python 
# Logging the axis
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=gapminder,
    x='gdp',
    y='life_exp',
    size='hdi_index',                  # map size to population
    sizes=(20, 200),             # scale point size (min, max)
    alpha=0.4                    # make points a bit transparent
)

plt.title('Life Expectancy vs GDP (Point Size = hdi_index)')
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))
plt.tight_layout()
plt.xscale('log')
plt.show()

```
![](Images/AxisLogged.png)
We will now add the colour by continent parameter back. 

For this notice that we are splitting the legends' code into 2 separate sections. 
```python 
# Adding color by continent parameter back 
plt.figure(figsize=(10, 6))
scatter = sns.scatterplot(
    data=gapminder,
    x='gdp',
    y='life_exp',
    size='hdi_index',                  # map size to population
    sizes=(20, 200),             # scale point size (min, max)
    alpha=0.4,                       # make points a bit transparent
    hue='continent'
)
plt.title('Life Expectancy vs GDP (Point Size = hdi_index)')
plt.xlabel('GDP per capita (USD)')
plt.ylabel('Life Expectancy (years)')

# Separate legends for hue and size
plt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))
sns.move_legend(scatter, title='Continent', loc='upper left', bbox_to_anchor=(0.7, 0.65))

plt.tight_layout # making sure the legends are in the frame
plt.xscale('log')
plt.show()
```
![](Images/twoLegends.png)

<!--#################################################-->

### Layers {#layers .unnumbered .unlisted}

Once you define your data and aesthetics (such as (x, y) coordinates, colour, size, etc.), you can add more layers to modify and enhance your plots.

* *Geometric Object*s: These are the graphical objects to be drawn, such as histograms, boxplots, density plots, etc.
* *Statistics*: These can be applied to your data, like calculating density or fitting a regression line.
* *Position Adjustments*: These modify how elements are placed, such as jittering points or stacking bars.

Example: Creating a Base plot - a histogram

```python 
# Histogram with Position Adjustments
plt.figure(figsize=(10, 6))
sns.histplot(data=gapminder, x='life_exp', hue='continent', element='step', fill=True, alpha=0.3)
plt.title('Histogram of Life Expectancy by Continent')
plt.xlabel('Life Expectancy')
plt.ylabel('Frequency')
plt.show()

```
![](Images/Histogram.png)

the "element" attribute can be one of "step", "poly" or "bars" depending of what is needed. Try it out yourself.

We will now plot a density plot, a smoothed version of a histogram using geom_density; its default position is identity and both plots are equivalent.

* A histogram displays the frequency of data within bins, while a density plot represents the probability density function of the data, providing a smoothed continuous curve.
* To convert a histogram to a density plot, you typically use `sns.kdeplot`, which computes and plots the kernel density estimate.

In our code below, the kdeplot function is used to create a density plot, and the common_norm=False ensures that the densities for each continent are normalized separately, allowing for a fair comparison of the distribution shapes without being influenced by the differing sizes of the groups.
```python 
# filled Density Plot
plt.figure(figsize=(10, 6))
sns.kdeplot(data=gapminder, x='life_exp', hue='continent', fill=True, common_norm=False, alpha=0.3)
plt.title('Density Plot of Life Expectancy by Continent')
plt.xlabel('Life Expectancy')
plt.ylabel('Density')
plt.show()
```

![](Images/densityPlot.png)

The "Multiple Parameter"
The multiple Parameter can be set to any of these values: 
* `layer` - Overlays categories, allowing overlap.
* `stack` - Stacks categories cumulatively.
* `fill` - Stacks and scales categories to equal height.
* `dodge` - Positions categories side by side.

e.g. using the dodge version: 

```python 
plt.figure(figsize=(10, 6))
sns.histplot(data=gapminder, x='life_exp', hue='continent', multiple='dodge', alpha=0.3)
plt.title('Life Expectancy Distribution by Continent')
plt.xlabel('Life Expectancy')
plt.ylabel('Frequency')
plt.show()
```
![](Images/Dodged.png)
<!--#################################################-->

### Faucetting {#faucetting .unnumbered .unlisted}

Facetting is a powerful technique in data visualisation that allows you to split one plot into multiple plots based on a factor included in the dataset. Python's `seaborn` library provides this functionality.


In the Gapminder scatterplot example, we can use faceting to produce one scatter plot for each continent separately, using `FacetGrid.`

#### Define the Core Scatter Plot

First, let's define the core scatter plot of life expectancy vs GDP and store it in an object for easy reuse:
```python 
# Base graph
plt.figure(figsize=(10, 6))
sns.scatterplot(data=gapminder, x='gdp', y='life_exp', hue='continent', alpha=0.5)
plt.xscale('log')
plt.title('Life Expectancy vs GDP per capita, 1998-2002')
plt.xlabel('GDP per capita')
plt.ylabel('Life Expectancy')
plt.show()
```
![](Images/baseGraph.png)

Now, let's add a new layer to our base plot using FacetGrid to facet by continent:
```python 
# Ensure 'continent' is treated as a categorical variable
gapminder['continent'] = gapminder['continent'].astype('category')

# Facet the scatter plot by continent with hue
g = sns.FacetGrid(gapminder, col='continent', col_wrap=3, height=4)
g.map_dataframe(sns.scatterplot, x='gdp', y='life_exp', hue='continent', alpha=0.5)
g.set_titles("{col_name}")
g.set_axis_labels('GDP per capita', 'Life Expectancy')
g.add_legend()
plt.show()
```

* `g.map_dataframe(...)`: Maps the sns.scatterplot function to each facet, passing the data explicitly and ensuring that 
* `hue='continent'` colours the points appropriately in each facet.

![](Images/faucetGraph.png)

* `FacetGrid`: This is used to create a grid of plots, allowing you to facet by a specified variable (continent in this case).
* `col_wrap`: This parameter controls the number of columns in the facet grid, making it adaptable to different screen sizes.
* `map`: This method maps a plotting function (sns.scatterplot) to each facet.


Finally, if you want to create a boxplot of life expectancy by continent instead of a scatter plot, you can use similar aesthetics with Python libraries like matplotlib and seaborn. The key difference is the type of plot you choose to represent your data.

```python 
plt.figure(figsize=(10, 6))
sns.boxplot(data=gapminder, x='continent', y='life_exp', hue='continent')

# Add labels and title
plt.title("Life Expectancy among the continents, 1952-2007")
plt.xlabel(" ")  # Empty, as the levels of the x-variable are the continents
plt.ylabel("Life Expectancy")
plt.figtext(0.9, 0.01, "Source: Gapminder", horizontalalignment='right')

# Apply a minimal theme
sns.set_theme(style="whitegrid")

# Show the plot
plt.show()
```
![](Images/boxAndWhiskers.png)

<!--#################################################-->

### Animated Graphs {#animated .unnumbered .unlisted}


<!--#################################################-->

### Why you should always plot your data {#whyPlot .unnumbered .unlisted}

<!--#################################################-->

### what data Patterns can lie behind a correlation coefficient? {#hiddenPatterns .unnumbered .unlisted}


<!--#################################################-->

### Further resources {#furtherResources4 .unnumbered .unlisted}

<!--#################################################-->


<!--#################################################-->


<!--#################################################-->

<!--#################################################-->

## 5. Manipulate Data

<!--#################################################-->

## 6. Reshape Data 

<!--#################################################-->

## 7. EDA for Modelling




<!--#################################################-->

# **Statistical Inference**




<!--#################################################-->


# **Data Modelling**
