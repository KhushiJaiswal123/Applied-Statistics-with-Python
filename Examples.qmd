---
title: "Examples"
toc-title: "Overview"
toc-location: left
jupyter: python3
---

# Worked Examples

::: {.callout-tip collapse=false}
## Contents
- [The Basics](#basics)
- [Exploratory Data Analysis](#EDA)
- [Data Modelling](#Modelling)
:::

<!--#################################################-->

### The Basics {#basics .unnumbered .unlisted}

<!--#################################################-->

::: {.callout-tip collapse=false}
## Contents
- [Variables and Data Types](#varAndData)
- [Basic Operations](#basicOperations)
- [Data Structures](#dataStructs)
- [Control flow](#control flow)


:::

This section covers fundamental Python concepts that are essential for working with data and understanding the examples in the rest of this course. If you're new to Python, spend some time here to get comfortable with the syntax and common operations.


<!--#################################################-->

#### Variables and Data Types {#varAndData}
* **What are variables?** (Containers for storing data)
* **Common Data Types:**
    * `int` (Integers: `5`, `-10`)
    * `float` (Floating-point numbers: `3.14`, `-0.5`)
    * `str` (Strings: `'hello'`, `"Python Course"`)
    * `bool` (Booleans: `True`, `False`)
* **How to assign variables:** `my_number = 10`, `my_text = "Data"`
* **Checking data type:** `type(my_number)`

```python
# Example
age = 30
temperature = 25.5
name = "Alice"
is_student = True

print(f"Age: {age}, Type: {type(age)}")
print(f"Temperature: {temperature}, Type: {type(temperature)}")
print(f"Name: {name}, Type: {type(name)}")
print(f"Is Student: {is_student}, Type: {type(is_student)}")

```
#### Basic Operations {#basicOperations}
* Arithmetic Operators: `+`, `-`, `*`, `/`, `**` (exponentiation), `%` (modulo)
* Comparison Operators: `==`, `!=`, `<`, `>`, `<=`, `>=`
* Logical Operators: and, or, not

```python
# Example
result = 10 + 5 * 2
print(f"Calculation: {result}") # Output: 20

is_greater = (result > 15) and (age < 40)
print(f"Is greater and age within range: {is_greater}")
```









































<!--#################################################-->


This section next section contains worked examples, most of them with fully annotated Python code that you can use as reference.

### Exploratory Data Analysis {#EDA .unnumbered .unlisted}
Before starting any analysis, we first need to import a dataset, understand its variables, visualize it, and manipulate it systematically using tools like `pandas`, `matplotlib`, and `seaborn.` This might seem like a tedious step, but it's a critical foundation that must precede any form of statistical modelling.

### Data Modelling {#Modelling .unnumbered .unlisted}
Data modelling allows us to move beyond describing individual variables — instead, we use models to learn from data. At the core of this is understanding the relationship:
```python
# General form of a predictive model
outcome = f(features) + error

```
We begin with Exploratory Data Analysis tailored for modelling, and then proceed with three key approaches:

1. **EDA for Modelling**
After importing and cleaning the data (using pandas), we start looking at summary statistics and plots that will be useful in framing our modelling approach

2. **Testing for Differences in Means** across samples: How do we know whether there is a **statistically significant difference** between two groups A and B? E.g., between those who took a drug versus those than a placebo? Or whether there is a difference in the percentage of people who approve of Donald Trump is lower than those who disapprove of him?

3. **Fitting a Linear Regression Model**
To understand which features are associated with a numerical outcome **Y**, we use Linear Regression from `scikit-learn`.
    We try to explain the effect that specific explanatory variables, **X**, have on **Y**

4. **Fitting a Binary Classification Model**
where the difference is that the outcome variable, **Y**, is binary (0/1). Again we want to use our model primarily for explanation, e.g., what is the effect of different explanatory variables **X**'s on e.g., the probability that someone with Covid-19 will die?

<!--#################################################-->

# **Exploratory Data Analysis**

<!--#################################################-->

## 1. Import Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overview)
    - [Importing CSV files: pandas.read_csv()](#importing)
    - [Importing CSV files saved locally](#localImport)
- [Need for speed: `dask` and `vaex`](#speed)
- [Other data formats](#OtherFormats)
- [Never work directly on the raw data](#rawData)
- [other Links](#otherLinks)

:::

> Learning Objectives
>
> 1. Load external data from a .csv file into a data frame.
>
> 2. Describe what a data frame is.
>
> 3. Use indexing to subset specific portions of data frames.
>
> 4. Describe what a factor is.
>
> 5. Reorder and rename factors.
>
> 6. Format dates.


### Overview {#overview .unnumbered .unlisted}

When working with Python, data importation is generally achieved using libraries like `pandas`, which provides powerful tools for data manipulation, including importing data from various file formats.

<!--#################################################-->

### Importing CSV files: pandas.read_csv() {#importing .unnumbered .unlisted}

CSV files can be imported using the `read_csv()` function from the `pandas` library. This function is fast and user-friendly, allowing you to read flat data without complex configurations.
```python
import pandas as pd

# Importing CSV file from a URL
url = "https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv"
weather = pd.read_csv(url, skiprows=1, na_values="***")
```
Options Used:
  * `skiprows=1`: This skips the first row, assuming the actual data starts from the second row.
  * `na_values= "***"` : This treats astriks' as missing values, converting them to NaN.

To view the structure of the dataframe, you can use:
```python
print(weather.info())
print(weather.head())
```

<!--#################################################-->

### Importing CSV files saved locally {#localImport .unnumbered .unlisted}

To import a CSV file saved locally, simply provide the file path to the `read_csv()` function:
```python 
weather_local = pd.read_csv("path/to/your/localfile.csv")
```


<!--#################################################-->

### Need for speed: `dask` and `vaex` {#speed .unnumbered .unlisted}
For handling large datasets, libraries like dask and vaex can be used, which offer faster data processing capabilities compared to pandas.
```python 
import dask.dataframe as dd

weather_large = dd.read_csv("path/to/largefile.csv")
```


<!--#################################################-->

### Other data formats {#OtherFormats .unnumbered .unlisted}

Python offers several libraries for reading and writing various data formats:

  * Excel files: `pandas.read_excel()`
  * JSON: `json` library
  * Web APIs: `requests` library
  * Databases: `sqlite3`
  * Big Data: `pyspark`

<!--#################################################-->

### Never work directly on the raw data {#rawData .unnumbered .unlisted}

In 2012 Cecilia Giménez, an 83-year-old widow and amateur painter, attempted to restore a century-old fresco of Jesus crowned with thorns in her local church in Borja, Spain. The restoration didn’t go very well, but, surprisingly, the [botched restoration of Jesus fresco miraculously saved the Spanish Town](https://news.artnet.com/art-world/botched-restoration-of-jesus-fresco-miraculously-saves-spanish-town-197057).

![](Images/raw_data_example.png)

As a most important rule, please do not work on the raw data; it’s unlikely you will have Cecilia Giménez’s good fortune to become (in)famous for your not-so-brilliant work.


Make sure you always work on a copy of your raw data. Use Python's data manipulation libraries to clean and transform your data, and save the results to a new file, ensuring the original data remains intact.
```python 
weather_cleaned = weather.dropna()  # Example of cleaning data
weather_cleaned.to_csv("cleaned_data.csv", index=False)
```


<!--#################################################-->

### Other Links {#otherLinks .unnumbered .unlisted}

  * [For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)
  * Data Wrangling with `pandas`: [Pandas Documentation](https://pandas.pydata.org/docs/)
  * Big Data Processing with `dask` and `vaex`: [Dask Documentation](https://dask.org/), [Vaex Documentation](https://vaex.io/docs/)

<!--#################################################-->

## 2. Inspect Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overviewof2)
- [Viewing Data](#viewing)
- [Detailed Inspection](#inspections)
- [Key Questions](#questions)

:::

<!--#################################################-->

### Overview {#overviewof2 .unnumbered .unlisted}
Once you have loaded your dataset into Python, it's essential to inspect and understand the data. Typically, you want to know:

  * The dimensions of the dataset (number of rows and columns).
  * The types of variables (integer, string, boolean, etc.).
  * The number of missing values.
  * Summary statistics for numeric data.

<!--#################################################-->

### Viewing Data {#viewing .unnumbered .unlisted}

In Python, you can use pandas to view and inspect your data. The pandas library provides several functions to achieve this.

`pandas.DataFrame.info()` and `pandas.DataFrame.describe()`

These functions help you understand the structure and summary statistics of your data.

```python 
import pandas as pd

# Load data
gapminder = pd.read_csv("path/to/gapminder.csv")

# View the structure of the dataframe
print(gapminder.info())

# Summary statistics
print(gapminder.describe())
```

Using `info()`: Provides the number of rows, columns, and data types of each column. Using `describe()` : Offers summary statistics for numeric columns, including count, mean, standard deviation, min, and max values.

<!--#################################################-->

### Detailed Inspection {#inspections .unnumbered .unlisted}

For a detailed inspection, you can use the pandas_profiling library, which provides an extensive report on your dataframe.

```python 
from pandas_profiling import ProfileReport

# Generate a report
profile = ProfileReport(gapminder)
profile.to_file("gapminder_report.html")
```

Example Analysis on London Bikes Data
```python 
bikes = pd.read_csv("path/to/londonBikes.csv")

# Use pandas_profiling for a detailed report
bikes_profile = ProfileReport(bikes)
bikes_profile.to_file("london_bikes_report.html")
```

<!--#################################################-->

### Key Questions {#questions .unnumbered .unlisted}

using the **London Bikes Data** from above

1. What kind of variable is **date**?
    * date is typically a string or datetime variable. You can convert it using `pd.to_datetime()`.
    
    ```python
    bikes['date'] = pd.to_datetime(bikes['date'])
    ```
2. What kind of variable is **season**?
    * season is likely a categorical variable. You can convert it using `pd.Categorical()`.
    
    ```python 
    bikes['season'] = pd.Categorical(bikes['season'])
    ```

3. How often does it rain in London?
    * Count the occurrences in the rain column.
    
    ```python 
    rain_count = bikes['rain'].sum()
print(f"It rains {rain_count} times in the dataset.")
```

4. What is the average annual temperature (in degrees C)?
    * Calculate the mean of the temperature columns.
    
    ```python 
    avg_temp = bikes[['max_temp', 'min_temp', 'avg_temp']].mean().mean()
print(f"The average annual temperature is {avg_temp:.2f} degrees C.")
```

5. What is the maximum rainfall?
    * Find the maximum value in the `rainfall_mm` column.
    
    ```python
    max_rainfall = bikes['rainfall_mm'].max()
print(f"The maximum rainfall recorded is {max_rainfall} mm.")
    ```

<!--#################################################-->

## 3. Clean Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overviewof3)
- [Cleaning Variable Names with pandas](#cleaning)
- [Code Quality](#codeQUality)
- [Other Links](#otherLinks)

:::

<!--#################################################-->

### Overview {#overviewof3 .unnumbered .unlisted}

When creating data files, it's common to use variable names and formats that are human-readable but not ideal for computational processing. In Python, the `pandas` library can be used to clean and standardise variable names for easier manipulation.

<!--#################################################-->

### Cleaning Variable Names with pandas {#cleaning .unnumbered .unlisted}

In `Python`, we can use pandas and custom code to read data and clean column names to make them more suitable for analysis.
```python 
import pandas as pd

# Load Excel file
roster = pd.read_excel("path/to/dirty_data.xlsx")

# Clean column names using pandas string methods
roster.columns = (
    roster.columns
        .str.strip()  # Remove leading and trailing spaces
        .str.lower()  # Convert to lowercase
        .str.replace(' ', '_')  # Replace spaces with underscores
        .str.replace('%', 'percent')  # Replace '%' with 'percent'
        .str.replace('[^a-zA-Z0-9_]', '', regex=True)  # Remove special characters
)

# Inspect cleaned dataframe
print(roster.head())

```

  * The custom code directly modifies the column names using pandas string methods. It removes spaces, converts to lowercase, replaces spaces with underscores, and removes special characters using a regular expression.
  * Regular Expression: [^a-zA-Z0-9_] is used to remove any character that is not alphanumeric or an underscore.
  * `pandas.read_excel()`: Used to read Excel files into a DataFrame.


<!--#################################################-->

### Code Quality {#codeQUality .unnumbered .unlisted}

According to Phil Karlton, there are only two [hard things in Computer Science: cache invalidation and naming things](https://martinfowler.com/bliki/TwoHardThings.html). It's crucial to write code that is not only functional but also maintainable and readable. Use meaningful names for variables and dataframes, and include comments to explain complex logic.

<!--#################################################-->

### Other Links {#otherLinks .unnumbered .unlisted}

* [For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)

<!--#################################################-->

## 4. Visualise Data
::: {.callout-tip collapse=false}
## Contents
- [Overview](#overviewof4)
- [Layers](#layers)
- [Faucetting](#faucetting)
- [Animated Graphs](#animated)
    - [matplotlib](#matplotLibrary)
- [Why you should always plot your data](#whyPlot)
- [Further resources](#furtherResources4)

:::
<!--#################################################-->

> Learning Objectives

> 1. Produce scatter plots, boxplots, and time series plots using matplotlib and seaborn.
> 2. Set universal plot settings.
> 3. Describe what faceting is and apply faceting using seaborn
> 4. Modify the aesthetics of an existing plot (including axis labels and colour).
> 5. Build complex and customised plots from data in a DataFrame.

<!--#################################################-->

### Overview {#overviewof4 .unnumbered .unlisted}

> “Visualization is a powerful tool for understanding data. You can often discover patterns, spot anomalies, and develop an intuition for the data just by looking at it.”  
>  
> — Wes McKinney, creator of pandas  

We will explore how to create insightful and aesthetically pleasing data visualisations using two powerful Python libraries: `matplotlib` and `seaborn`.  
We will work through examples of various plot types - scatter plots, boxplots, and histograms - and learn to customise them for clarity and impact.  

It may seem verbose and unwieldy, but the idea of building a plot on a layer-by-layer basis is very powerful.

* You begin a plot by defining the dataset you will use. 
* Then, specify aesthetics, namely (x, y) coordinates, colour, size, etc. 
* Finally, choose the geometric shape to represent your data, and add more layers like legends, labels, facets, etc.

For example, using the <a href="https://www.kaggle.com/datasets/albertovidalrod/gapminder-dataset?resource=download" target="_blank">Gapminder dataset</a> with data on life expectancy (life_exp), human development index (hdi_index), and GDP (gdp) for a number of countries, we can build a graph that shows the relationship between GDP and life expectancy.

As we said, first we define the dataset we are using
```python
import pandas as pd
gapminder = pd.read_csv(r"filepath-to-your-dataset")
```

The next thing is to map aesthetics. In our case, we will map gdpPercap to the x-axis, and lifeExp to the y-axis.

```python 
# Basic Plot Setup
plt.figure(figsize=(10, 6))
sns.scatterplot(data=gapminder, x='gdp', y='life_exp')
plt.title('Life Expectancy vs GDP')
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.show()
```
![](Images/BasicPlotSetup.png)

* `plt.figure(figsize=(10, 6))`: Defines the size of the plot.
* `sns.scatterplot(...)`: Uses Seaborn to draw a scatter plot from the gapminder dataset.
* `plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`: Add meaningful labels and a title to make the chart easy to understand.
* `plt.show()`: Renders the plot in your output window or notebook.


What if we wanted to colour the points by the continent each country is in? For this we will need to use the "continent" column from our gapminder dataset. Seaborn makes this easy using the hue parameter.


```python 
# Colored Scatter Plot by Continent
plt.figure(figsize=(10, 6))  # Set figure size
sns.scatterplot(data=gapminder, x='gdp', y='life_exp', hue='continent')  # Color points by continent
plt.title('Life Expectancy vs GDP by Continent')  # Updated title
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='Continent')  # Add a legend with a title
plt.show()
```

![](Images/ColouredScatterPlot.png)

* `hue='continent'`: Tells Seaborn to color the points based on the continent each country belongs to.
* `plt.legend(...)`: Makes sure the legend is clear and labeled.

What if instead of a scatter plot we wanted to create a line plot? For this we need to: 

* change the `sns.scatterplot` to `sns.lineplot`


```python 
# Colored Line Plot by Continent
plt.figure(figsize=(10, 6))  # Set figure size
sns.lineplot(data=gapminder, x='gdp', y='life_exp', hue='continent')  # Color points by continent
plt.title('Life Expectancy vs GDP by Continent')  # Updated title
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='Continent')  # Add a legend with a title
plt.show()
```


![](Images/LineGraph.png)

However, this is not a particularly useful plot, so let us go back to our scatter plot.

What if we wanted to have the size of each point correspond to the HDI index of the country?
For this we will :

* add the `size='hdi_index'` parameter in the `scatterplot()` function
* add the `sizes=(20, 200)` parameter in the `scatterplot()` function to specify the range of sizes of scatter points we want to use. 

```python
# going back to scatter plot but now the size of points coresspond to the population of the country
# Basic Plot Setup with size mapped to population
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=gapminder,
    x='gdp',
    y='life_exp',
    size='hdi_index',                  # map size to population
    sizes=(20, 200),             # scale point size (min, max)
)

plt.title('Life Expectancy vs GDP (Point Size = hdi_index)')
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))
plt.tight_layout()
plt.show()
```

![](Images/hdi_index.png)
But the points above are overlapping with each other. We can set the `alpha` variable between 0 and 1 to specify how transparent each point will be. This will let us see all the points better.

```python 
# Adding Alpha
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=gapminder,
    x='gdp',
    y='life_exp',
    size='hdi_index',                  # map size to population
    sizes=(20, 200),             # scale point size (min, max)
    alpha=0.4                    # make points a bit transparent
)

plt.title('Life Expectancy vs GDP (Point Size = hdi_index)')
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))
plt.tight_layout()
plt.show()

```
![](Images/Alpha.png)

Our graph is still not very clear after adding the `alpha` this is because all the points are bunched up in only a small section of the graph. We can *log* the x-axis to make the points more spread out across the GDP axis.

```python 
# Logging the axis
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=gapminder,
    x='gdp',
    y='life_exp',
    size='hdi_index',                  # map size to population
    sizes=(20, 200),             # scale point size (min, max)
    alpha=0.4                    # make points a bit transparent
)

plt.title('Life Expectancy vs GDP (Point Size = hdi_index)')
plt.xlabel('GDP')
plt.ylabel('Life Expectancy')
plt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))
plt.tight_layout()
plt.xscale('log')
plt.show()

```
![](Images/AxisLogged.png)
We will now add the colour by continent parameter back. 

For this notice that we are splitting the legends' code into 2 separate sections. 
```python 
# Adding color by continent parameter back 
plt.figure(figsize=(10, 6))
scatter = sns.scatterplot(
    data=gapminder,
    x='gdp',
    y='life_exp',
    size='hdi_index',                  # map size to population
    sizes=(20, 200),             # scale point size (min, max)
    alpha=0.4,                       # make points a bit transparent
    hue='continent'
)
plt.title('Life Expectancy vs GDP (Point Size = hdi_index)')
plt.xlabel('GDP per capita (USD)')
plt.ylabel('Life Expectancy (years)')

# Separate legends for hue and size
plt.legend(title='hdi_index', loc='upper left', bbox_to_anchor=(1, 1))
sns.move_legend(scatter, title='Continent', loc='upper left', bbox_to_anchor=(0.7, 0.65))

plt.tight_layout # making sure the legends are in the frame
plt.xscale('log')
plt.show()
```
![](Images/twoLegends.png)

<!--#################################################-->

### Layers {#layers .unnumbered .unlisted}

Once you define your data and aesthetics (such as (x, y) coordinates, colour, size, etc.), you can add more layers to modify and enhance your plots.

* *Geometric Object*s: These are the graphical objects to be drawn, such as histograms, boxplots, density plots, etc.
* *Statistics*: These can be applied to your data, like calculating density or fitting a regression line.
* *Position Adjustments*: These modify how elements are placed, such as jittering points or stacking bars.

Example: Creating a Base plot - a histogram

```python 
# Histogram with Position Adjustments
plt.figure(figsize=(10, 6))
sns.histplot(data=gapminder, x='life_exp', hue='continent', element='step', fill=True, alpha=0.3)
plt.title('Histogram of Life Expectancy by Continent')
plt.xlabel('Life Expectancy')
plt.ylabel('Frequency')
plt.show()

```
![](Images/Histogram.png)

the "element" attribute can be one of "step", "poly" or "bars" depending of what is needed. Try it out yourself.

We will now plot a density plot, a smoothed version of a histogram using geom_density; its default position is identity and both plots are equivalent.

* A histogram displays the frequency of data within bins, while a density plot represents the probability density function of the data, providing a smoothed continuous curve.
* To convert a histogram to a density plot, you typically use `sns.kdeplot`, which computes and plots the kernel density estimate.

In our code below, the `kdeplot` function is used to create a density plot, and the `common_norm=False` ensures that the densities for each continent are normalized separately, allowing for a fair comparison of the distribution shapes without being influenced by the differing sizes of the groups.
```python 
# filled Density Plot
plt.figure(figsize=(10, 6))
sns.kdeplot(data=gapminder, x='life_exp', hue='continent', fill=True, common_norm=False, alpha=0.3)
plt.title('Density Plot of Life Expectancy by Continent')
plt.xlabel('Life Expectancy')
plt.ylabel('Density')
plt.show()
```

![](Images/densityPlot.png)

The "Multiple Parameter"
The multiple Parameter can be set to any of these values: 

* `layer` - Overlays categories, allowing overlap.
* `stack` - Stacks categories cumulatively.
* `fill` - Stacks and scales categories to equal height.
* `dodge` - Positions categories side by side.

e.g. using the dodge version: 

```python 
plt.figure(figsize=(10, 6))
sns.histplot(data=gapminder, x='life_exp', hue='continent', multiple='dodge', alpha=0.3)
plt.title('Life Expectancy Distribution by Continent')
plt.xlabel('Life Expectancy')
plt.ylabel('Frequency')
plt.show()
```
![](Images/Dodged.png)
<!--#################################################-->

### Faucetting {#faucetting .unnumbered .unlisted}

Facetting is a powerful technique in data visualisation that allows you to split one plot into multiple plots based on a factor included in the dataset. Python's `seaborn` library provides this functionality.


In the Gapminder scatterplot example, we can use faceting to produce one scatter plot for each continent separately, using `FacetGrid.`

#### Define the Core Scatter Plot

First, let's define the core scatter plot of life expectancy vs GDP and store it in an object for easy reuse:
```python 
# Base graph
plt.figure(figsize=(10, 6))
sns.scatterplot(data=gapminder, x='gdp', y='life_exp', hue='continent', alpha=0.5)
plt.xscale('log')
plt.title('Life Expectancy vs GDP per capita, 1998-2002')
plt.xlabel('GDP per capita')
plt.ylabel('Life Expectancy')
plt.show()
```
![](Images/baseGraph.png)

Now, let's add a new layer to our base plot using FacetGrid to facet by continent:
```python 
# Ensure 'continent' is treated as a categorical variable
gapminder['continent'] = gapminder['continent'].astype('category')

# Facet the scatter plot by continent with hue
g = sns.FacetGrid(gapminder, col='continent', col_wrap=3, height=4)
g.map_dataframe(sns.scatterplot, x='gdp', y='life_exp', hue='continent', alpha=0.5)
g.set_titles("{col_name}")
g.set_axis_labels('GDP per capita', 'Life Expectancy')
g.add_legend()
plt.show()
```

* `g.map_dataframe(...)`: Maps the sns.scatterplot function to each facet, passing the data explicitly and ensuring that 
* `hue='continent'` colours the points appropriately in each facet.

![](Images/faucetGraph.png)

* `FacetGrid`: This is used to create a grid of plots, allowing you to facet by a specified variable (continent in this case).
* `col_wrap`: This parameter controls the number of columns in the facet grid, making it adaptable to different screen sizes.
* `map`: This method maps a plotting function (sns.scatterplot) to each facet.


Finally, if you want to create a boxplot of life expectancy by continent instead of a scatter plot, you can use similar aesthetics with Python libraries like matplotlib and seaborn. The key difference is the type of plot you choose to represent your data.

```python 
plt.figure(figsize=(10, 6))
sns.boxplot(data=gapminder, x='continent', y='life_exp', hue='continent')

# Add labels and title
plt.title("Life Expectancy among the continents, 1952-2007")
plt.xlabel(" ")  # Empty, as the levels of the x-variable are the continents
plt.ylabel("Life Expectancy")
plt.figtext(0.9, 0.01, "Source: Gapminder", horizontalalignment='right')

# Apply a minimal theme
sns.set_theme(style="whitegrid")

# Show the plot
plt.show()
```
![](Images/boxAndWhiskers.png)

<!--#################################################-->

### Animated Graphs {#animated .unnumbered .unlisted}

Animated graphs are a powerful way to visualize how data changes over time. In Python, one of the most commonly used tools for creating animations is Matplotlib's `FuncAnimation` class. This function updates a plot frame-by-frame, allowing you to illustrate dynamic processes such as moving trends, changing patterns, or simulations. The final animation can be displayed directly in a notebook or exported as a GIF or video using writers like `pillow` or `ffmpeg.` Keep in mind that generating animations may take a few seconds, especially for longer sequences, as each frame is rendered individually.

<!--#################################################-->
#### matplotlib.animation {#matplotLibrary .unnumbered .unlisted}

To implement animations, we will need to understand the concept of functions in Python. These are *reusable blocks of code that perform specific tasks and can be called with different inputs to produce different outputs*. 

In the context of animations, a function—like `update()` in the code below - is used to define how each frame of the animation should be rendered based on changing input (e.g., the year). This function is then repeatedly called by FuncAnimation, allowing us to dynamically update the plot for each time step.

```python 
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation as animation


# Set up the figure and axis
fig, ax = plt.subplots(figsize=(8, 6))
plt.xticks(rotation=45)

# Get unique years sorted
years = sorted(gapminder['year'].unique())

def update(year_index):
    ax.clear()  # Clear previous frame
    year = years[year_index]
    yearly_data = gapminder[gapminder['year'] == year]

    # Create boxplot grouped by continent
    yearly_data.boxplot(column='life_exp', by='continent', ax=ax)

    # Customize plot appearance
    ax.set_title(f'Life Expectancy by Continent ({year})')
    ax.set_xlabel('Continent')
    ax.set_ylabel('Life Expectancy')
    plt.suptitle('')  # Remove automatic title added by Pandas boxplot

# Create animation
ani = animation.FuncAnimation(fig, update, frames=len(years), interval=800, repeat=True)

plt.tight_layout()
plt.show()

# Optional: Save as GIF (requires 'pillow')
ani.save('life_expectancy_boxplot.gif', writer='pillow')
```

![](GIFs/life_expectancy_boxplot.gif)

Similarly, we can create an animated visualisation showing the relationship between GDP and life expectancy over time for different countries, segmented by continent.

* *X-axis (GDP)*: Logarithmic scale representing each country's GDP.
* *Y-axis (Life Expectancy)*: Life expectancy of each country's population.
* *Data Points*
    * Each point represents a country for a specific year.
    * Colour-coded by continent:
        * Asia: Red
        * Europe: Blue
        * Africa: Green
        * Americas: Yellow
        * Oceania: Purple
        * Unknown: Grey
* *Animation*
    * Displays changes over time, with each frame representing a different year.
    * Title updates dynamically to reflect the current year.



```python 
# Fill NaN values in the 'continent' column with a default value
gapminder['continent'] = gapminder['continent'].fillna('Unknown')

# Set up the figure and axis
fig, ax = plt.subplots(figsize=(6, 6))
plt.xticks(rotation=45)

# Get unique years sorted
years = sorted(gapminder['year'].unique())

def update(year_index):
    ax.clear()  # Clear previous frame
    year = years[year_index]
    yearly_data = gapminder[gapminder['year'] == year]

    # Ensure there are no NaN values in the data used for plotting
    yearly_data = yearly_data.dropna(subset=['gdp', 'life_exp'])

    # Map continents to colours, handling 'Unknown' as grey
    continent_colors = yearly_data['continent'].map({
        'Asia': 'red', 'Europe': 'blue', 'Africa': 'green', 
        'Americas': 'yellow', 'Oceania': 'purple', 'Unknown': 'grey'
    }).fillna('grey')  # Fill any remaining NaN values with 'grey'

    # Create scatter plot
    scatter = ax.scatter(yearly_data['gdp'], yearly_data['life_exp'], 
                         c=continent_colors, alpha=0.5)

    # Customize plot appearance
    ax.set_title(f'Year: {year}')
    ax.set_xlabel('GDP')
    ax.set_ylabel('Life Expectancy')
    ax.set_xscale('log')  # Use logarithmic scale for GDP
    ax.set_xlim(gapminder['gdp'].min(), gapminder['gdp'].max())
    ax.set_ylim(gapminder['life_exp'].min(), gapminder['life_exp'].max())

# Create animation
ani = animation.FuncAnimation(fig, update, frames=len(years), interval=800, repeat=True)

plt.tight_layout()
plt.show()

# Optional: Save as GIF (requires 'pillow')
ani.save('life_expectancy_vs_gdp.gif', writer='pillow', dpi=80)
```
![](GIFs/life_expectancy_vs_gdp.gif)

<!--#################################################-->

### Why you should always plot your data {#whyPlot .unnumbered .unlisted}

We have touched on the basics of **python** visualisations, but in this section we wanted to discuss why one should **always plot the data** and not just rely on tables of summary statistics.

Let us consider thirteen datasets all of which have 142 observations of (x,y) values. The table below shows the average value of X and Y, the standard deviation of X and Y, as well as the correlation coefficient between X and Y.

| id | n | mean_x | mean_y | sd_x | sd_y | correlation |
|:---|:--|:-------|:-------|:-----|:-----|:------------|
| 1  | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.064      |
| 2  | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.069      |
| 3  | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.068      |
| 4  | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.064      |
| 5  | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.060      |
| 6  | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.062      |
| 7  | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.069      |
| 8  | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.069      |
| 9  | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.069      |
| 10 | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.063      |
| 11 | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.069      |
| 12 | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.067      |
| 13 | 142 | 54.3   | 47.8   | 6.82 | 6.9  | -0.066      |

Since our datasets contain values for X and Y, we can estimate 13 regression models and plot the values for each of the 13 intercepts and slope for X.

![](Images/LinearRegression_with95Confidence.png)

If we just looked at either the summary statistics table, or the plots of intercepts and slopes, we may be tempted to conclude that the 13 datasets are either identical or very much alike. However, this is far from the truth, as this is what the 13 individual datasets look like.

![](Images/13_correlation_plots.png)

You can read more about why you <a href="https://www.research.autodesk.com/publications/same-stats-different-graphs/" target="_blank">should never trust summary statistics alone and should always visualize your data</a>.

<!--#################################################-->

### Further resources {#furtherResources4 .unnumbered .unlisted}

* <a href="https://seaborn.pydata.org/tutorial.html" target="_blank">Official `seaborn` Tutorial</a>
* <a href="https://matplotlib.org/stable/tutorials/introductory/pyplot.html" target="_blank">`matplotlib` Pyplot Tutorial</a>
* <a href="https://www.datacamp.com/courses/introduction-to-data-visualization-with-seaborn?dc_referrer=https%3A%2F%2Fam01-sep24.netlify.app%2F" target="_blank">DataCamp - Data Visualization with Seaborn</a>
* <a href="https://www.datacamp.com/courses/introduction-to-data-visualization-with-matplotlib" target="_blank">Datacamp - Introduction to Data Visualization with Matplotlib</a>
* <a href="https://seaborn.pydata.org/examples/index.html" target="_blank">`seaborn` Gallery</a>
* <a href="https://matplotlib.org/stable/gallery/index.html" target="_blank">`matplotlib` Examples</a>

<!--#################################################-->

<!--#################################################-->

<!--#################################################-->

## 5. Manipulate Data

::: {.callout-tip collapse=false}
## Contents
- [Selecting and Filtering Data](#select)
- [Handling Missing Values](#filter)
- [Creating New Variables (Feature Engineering Basics)](#FeatureEng)
- [Combining Datasets](#combiningDatasets)
- [Reshaping Data (Pivoting and Melting)](#groupData)

:::

Once you've imported and inspected your data, the next crucial step in any data science workflow is data manipulation. This involves transforming raw data into a clean, structured, and analysis-ready format. Think of it as refining raw materials before building something incredible – whether it's an insightful visualization or a powerful predictive model.

This section will introduce you to essential techniques using pandas, your go-to library for efficient data manipulation in Python. We'll cover how to select and filter data, handle missing values, create new variables, and combine datasets.

<!--#################################################-->

### Selecting and Filtering Data {#select .unnumbered .unlisted}

Effectively extracting specific subsets of your data is fundamental. This allows you to focus on relevant information, analyze particular groups, or prepare data for specific operations.

You can select one or multiple columns from a pandas DataFrame using single or double square brackets.

- Selecting a single column
```python
import pandas as pd

# Assuming 'gapminder' DataFrame is loaded from previous sections
# (e.g., gapminder = pd.read_csv("filepath-to-your-dataset"))

# Select the 'life_exp' column
life_expectancy = gapminder['life_exp']
print(life_expectancy.head())
print(type(life_expectancy)) # Output will be <class 'pandas.core.series.Series'>
```

- Selecting multiple columns:
``` python 
# Select 'gdp' and 'life_exp' columns
gdp_life_exp = gapminder[['gdp', 'life_exp']]
print(gdp_life_exp.head())
print(type(gdp_life_exp)) # Output will be <class 'pandas.core.frame.DataFrame'>
```

Notice the difference in output type: a single column selection returns a Series, while multiple columns return a DataFrame.

<!--#################################################-->

## Handling Missing Values {#filter .unnumbered .unlisted}

Filtering rows, also known as subsetting, allows you to select data based on specific conditions. This is often done using boolean indexing.

- Filtering by a single condition: Let's say we want to look at data only for the 'Europe' continent.
```python 
# Filter for data where continent is 'Europe'
europe_data = gapminder[gapminder['continent'] == 'Europe']
print(europe_data.head())
print(f"Number of rows for Europe: {len(europe_data)}")
```

- Filtering by multiple conditions: You can combine multiple conditions using logical operators (& for AND, | for OR, ~ for NOT). Remember to wrap each condition in parentheses.
```python
# Filter for data where continent is 'Asia' AND life expectancy is greater than 70
asia_high_life_exp = gapminder[(gapminder['continent'] == 'Asia') & (gapminder['life_exp'] > 70)]
print(asia_high_life_exp.head())
print(f"Number of rows for Asia with high life expectancy: {len(asia_high_life_exp)}")
```

Real-world datasets are rarely perfect and often contain missing values (represented as NaN - Not a Number in pandas). Dealing with these is a critical step in data cleaning, as they can lead to errors or biased results in your analysis.

You've briefly touched on dropna() when saving a cleaned file, but here we'll delve deeper into strategies.

- Identifying Missing Values: Use isnull() or isna() to get a boolean DataFrame indicating missing values, and sum() to count them per column.

```python 
# Check for missing values across the entire DataFrame
print(gapminder.isnull().sum())

# Check for missing values in a specific column, e.g., 'gdp'
print(f"Missing values in 'gdp' column: {gapminder['gdp'].isnull().sum()}")
```

- Dropping Missing Values: dropna() Removes rows or columns containing missing values.
```python 
# Drop rows with ANY missing values
df_cleaned_rows = gapminder.dropna()
print(f"Original rows: {len(gapminder)}, Rows after dropping NaNs: {len(df_cleaned_rows)}")

# Drop columns with ANY missing values
df_cleaned_cols = gapminder.dropna(axis=1) # axis=1 for columns
print(f"Original columns: {gapminder.shape[1]}, Columns after dropping NaNs: {df_cleaned_cols.shape[1]}")

# Drop rows only if ALL values are missing
df_cleaned_all = gapminder.dropna(how='all')

```
Caution: Be careful when dropping data, especially rows. This can lead to significant data loss and might not always be the best approach, particularly with small datasets or if missingness carries important information.

- Imputing Missing Values (Filling): fillna() Replaces missing values with a specified value or method. This is generally preferred over dropping when you want to retain as much data as possible.
```python 
# Fill missing 'gdp' values with the mean of the 'gdp' column
mean_gdp = gapminder['gdp'].mean()
gapminder_filled_mean = gapminder['gdp'].fillna(mean_gdp)
print(f"GDP after filling with mean:\n{gapminder_filled_mean.head()}")

# Fill missing values in 'continent' with 'Unknown' (as done in animation example)
gapminder['continent'] = gapminder['continent'].fillna('Unknown')
print(f"Continent column after filling NaNs:\n{gapminder['continent'].value_counts()}")

# Forward fill (propagates last valid observation forward to next valid)
gapminder_ffill = gapminder['life_exp'].fillna(method='ffill')

# Backward fill (propagates next valid observation backward to next valid)
gapminder_bfill = gapminder['life_exp'].fillna(method='bfill')

```
Choosing an imputation strategy depends heavily on the nature of your data and the reason for the missingness. Common strategies include using the mean, median, mode, or more advanced techniques like interpolation.


<!--#################################################-->


### Creating New Variables (Feature Engineering Basics) {#FeatureEng .unnumbered .unlisted}
Feature engineering is the process of creating new variables (features) from existing ones to improve the performance of predictive models or reveal new insights. This can be as simple as a mathematical transformation or as complex as extracting information from text.
- Mathematical Operations: You can easily perform arithmetic operations on existing columns to create new ones.
```python 
# Create a new 'gdp_per_person' column
gapminder['gdp_per_person'] = gapminder['gdp'] / gapminder['population']
print(gapminder[['gdp', 'population', 'gdp_per_person']].head())

```
- Applying Functions: Use the .apply() method to apply a custom function to a Series or DataFrame.
```python 
# Categorize countries into 'High' or 'Low' life expectancy based on a threshold
def categorize_life_exp(life_exp):
    return 'High' if life_exp > 70 else 'Low'

gapminder['life_exp_category'] = gapminder['life_exp'].apply(categorize_life_exp)
print(gapminder[['life_exp', 'life_exp_category']].head())

```

- Using numpy for Conditional Logic: For more complex conditional logic, numpy.where is very useful.
```python
import numpy as np

# Create a 'wealth_category' based on GDP
gapminder['wealth_category'] = np.where(gapminder['gdp'] > 1000000000000, 'Very High',
                                        np.where(gapminder['gdp'] > 100000000000, 'High', 'Medium/Low'))
print(gapminder[['gdp', 'wealth_category']].head())
```

<!--#################################################-->

### Combining Datasets {#combiningDatasets .unnumbered .unlisted}

Often, the data you need for an analysis is spread across multiple files or DataFrames. pandas provides powerful functions to combine them.
- Concatenating DataFrames (pd.concat()): Used to stack DataFrames vertically (rows) or horizontally (columns).
```python
# Create two small DataFrames for demonstration
df1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']}, index=[0, 1])
df2 = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']}, index=[2, 3])

# Concatenate vertically (default)
result_concat_rows = pd.concat([df1, df2])
print("Concatenated Rows:\n", result_concat_rows)

# Concatenate horizontally
df3 = pd.DataFrame({'C': ['C0', 'C1'], 'D': ['D0', 'D1']}, index=[0, 1])
result_concat_cols = pd.concat([df1, df3], axis=1)
print("\nConcatenated Columns:\n", result_concat_cols)
```

- Merging DataFrames (pd.merge()): Similar to SQL joins, merge() combines DataFrames based on common columns (keys). This is crucial for combining related information from different sources.
```python 
# Create two sample DataFrames: one with country info, one with additional stats
country_info = pd.DataFrame({
    'country': ['Afghanistan', 'Albania', 'Algeria', 'Angola'],
    'capital': ['Kabul', 'Tirana', 'Algiers', 'Luanda']
})

country_stats = pd.DataFrame({
    'country': ['Afghanistan', 'Albania', 'Algeria', 'Argentina'],
    'population_2020': [38928346, 2877797, 43851044, 45195774]
})

# Inner merge: only includes rows where the 'country' key exists in both DataFrames
merged_data_inner = pd.merge(country_info, country_stats, on='country', how='inner')
print("\nInner Merge:\n", merged_data_inner)

# Left merge: includes all rows from the left DataFrame and matching rows from the right
merged_data_left = pd.merge(country_info, country_stats, on='country', how='left')
print("\nLeft Merge:\n", merged_data_left)

# Right merge: includes all rows from the right DataFrame and matching rows from the left
merged_data_right = pd.merge(country_info, country_stats, on='country', how='right')
print("\nRight Merge:\n", merged_data_right)

# Outer merge: includes all rows from both DataFrames, filling missing with NaN
merged_data_outer = pd.merge(country_info, country_stats, on='country', how='outer')
print("\nOuter Merge:\n", merged_data_outer)
```
Choosing the correct how argument (inner, left, right, outer) is essential depending on how you want to handle non-matching keys.

<!--#################################################-->

### Reshaping Data (Pivoting and Melting) {#groupData .unnumbered .unlisted}

Reshaping data involves changing the layout of your DataFrame. This is particularly useful when you need to transform data from a "long" format to a "wide" format, or vice-versa, for analysis or visualization.

**Pivoting (df.pivot_table() or df.pivot())**
Pivoting transforms unique values from one or more columns into new columns, typically aggregating data in the process. pivot_table is more flexible as it can handle duplicate entries in the index and allows for aggregation.

Imagine your gapminder data has multiple entries for a country across different years, and you want to see life expectancy for each continent as columns, with years as rows.
```python 
# Let's create a simplified DataFrame for pivoting example
pivot_df = gapminder[['year', 'continent', 'life_exp']]
# Calculate the mean life expectancy per continent per year
avg_life_exp_pivot = pivot_df.pivot_table(index='year',
                                          columns='continent',
                                          values='life_exp',
                                          aggfunc='mean')
print("Pivoted Average Life Expectancy:\n", avg_life_exp_pivot.head())
```


**Melting (pd.melt())**

Melting is the opposite of pivoting; it transforms data from a "wide" format into a "long" format. This is useful for reshaping dataframes for certain types of analysis or plotting libraries (like Seaborn, which often prefers long-format data).

Consider our avg_life_exp_pivot from above. If we wanted to "unpivot" it back into a long format.
```python 
# Reset index to make 'year' a regular column
avg_life_exp_pivot_reset = avg_life_exp_pivot.reset_index()

# Melt the DataFrame
melted_life_exp = pd.melt(avg_life_exp_pivot_reset,
                         id_vars=['year'], # Columns to keep as identifier variables
                         var_name='continent', # Name for the new column storing melted column headers
                         value_name='average_life_expectancy') # Name for the new column storing values
print("\nMelted Average Life Expectancy:\n", melted_life_exp.head())
```

<!--#################################################-->

## 6. Group and Aggregate Data

::: {.callout-tip collapse=false}
## Contents
- [The groupby() Method](#groupBy)
- [Common Aggregation Functions](#agg)
- [Grouping by Multiple Columns](#multiColGroup)
- [Applying Multiple Aggregations](#multiArg)
- [Using agg() with Custom Functions](#Customfunctions)
- [Pivot Tables Revisited for Aggregation](#pivotTables)

:::

After you've cleaned and manipulated your data, you often need to summarize it to uncover patterns, trends, or characteristics within different categories. This is where grouping and aggregation become indispensable. These techniques allow you to perform calculations on subsets of your data, rather than on the entire dataset at once.

Think of it like getting summary reports for different departments in a company, rather than looking at every single transaction. You're combining rows based on shared characteristics and then applying functions (like sum, average, count) to those groups.

This section will introduce you to groupby() and various aggregation functions in pandas, enabling you to derive meaningful insights from your prepared data

<!--#################################################-->


### The groupby() Method {#groupBy .unnumbered .unlisted}

The core of grouping in pandas is the `groupby()` method. It allows you to split your DataFrame into groups based on one or more columns, apply a function to each group independently, and then combine the results into a single DataFrame. This process is often referred to as the "split-apply-combine" strategy.

**Basic Grouping**
Let's use the gapminder dataset. A common task might be to find the average life expectancy for each continent.

```python 
import pandas as pd

# Assuming 'gapminder' DataFrame is loaded and 'continent' column is clean
# (e.g., gapminder = pd.read_csv("filepath-to-your-dataset"))
# If 'continent' has NaNs, ensure they are handled as per the previous chapter, e.g.,
# gapminder['continent'] = gapminder['continent'].fillna('Unknown')

# Group by 'continent' and calculate the mean of 'life_exp'
avg_life_exp_by_continent = gapminder.groupby('continent')['life_exp'].mean()
print("Average Life Expectancy by Continent:\n", avg_life_exp_by_continent)

```

In this example:

1. gapminder.groupby('continent') splits the DataFrame into separate groups for each unique continent (e.g., 'Asia', 'Europe', 'Africa').
2. ['life_exp'] selects the life_exp column from each of these groups.
3. `.mean()` applies the mean function to the life_exp values within each continent group.
4. Pandas then combines these results into a new Series.

<!--#################################################-->

### Common Aggregation Functions {#agg .unnumbered .unlisted}

After grouping, you can apply various aggregation functions. Some of the most common include:
- `.mean()`: Calculates the average.
- `.sum()`: Calculates the total.
- `.count()`: Counts non-null observations.
- `.median()`: Calculates the median.
- `.min()`: Finds the minimum value.
- `.max()`: Finds the maximum value.
- `.std()`: Calculates the standard deviation.
- `.var()`: Calculates the variance.
- `.size()`: Counts the total number of rows in each group (including NaNs).

```python
# Group by 'continent' and find the sum of 'population'
total_pop_by_continent = gapminder.groupby('continent')['population'].sum()
print("\nTotal Population by Continent:\n", total_pop_by_continent)

# Group by 'year' and find the median 'gdp'
median_gdp_by_year = gapminder.groupby('year')['gdp'].median()
print("\nMedian GDP by Year:\n", median_gdp_by_year.head())

# Group by 'continent' and find the number of unique countries
country_count_by_continent = gapminder.groupby('continent')['country'].nunique()
print("\nNumber of Unique Countries by Continent:\n", country_count_by_continent)
```


<!--#################################################-->

### Grouping by Multiple Columns {#multiColGroup .unnumbered .unlisted}
You can group your data using more than one column. This creates hierarchical groups, allowing for more granular analysis.
```python 
# Group by 'year' AND 'continent' to find average life expectancy
avg_life_exp_year_continent = gapminder.groupby(['year', 'continent'])['life_exp'].mean()
print("\nAverage Life Expectancy by Year and Continent:\n", avg_life_exp_year_continent.head(10))

# The result is a Series with a MultiIndex. You can access specific levels:
print("\nLife expectancy for Europe in 1952:", avg_life_exp_year_continent.loc[(1952, 'Europe')])
```
This multi-level index (MultiIndex) is very powerful for slicing and dicing your data further.

<!--#################################################-->

### Applying Multiple Aggregations {#multiArg .unnumbered .unlisted}

Sometimes, you need to calculate several summary statistics for each group simultaneously. The `.agg()` method (or `.aggregate()`) is perfect for this.

**Aggregating a Single Column with Multiple Functions**
```python
# Group by 'continent' and calculate mean, min, and max of 'life_exp'
multi_agg_life_exp = gapminder.groupby('continent')['life_exp'].agg(['mean', 'min', 'max', 'std'])
print("\nMultiple Aggregations of Life Expectancy by Continent:\n", multi_agg_life_exp)

```
**Aggregating Multiple Columns with Multiple Functions**
You can specify different aggregation functions for different columns using a dictionary.
```python 
# Group by 'continent' and aggregate 'life_exp' and 'gdp'
multi_col_agg = gapminder.groupby('continent').agg(
    avg_life_exp=('life_exp', 'mean'), # Renames the output column to 'avg_life_exp'
    total_gdp=('gdp', 'sum'),
    min_pop=('population', 'min')
)
print("\nMulti-Column Aggregation by Continent:\n", multi_col_agg)

```

This gives you fine-grained control over your summary statistics, allowing you to name the resulting columns clearly.

<!--#################################################-->

### Using agg() with Custom Functions {#Customfunctions .unnumbered .unlisted}

The `agg()` method is highly flexible and can even apply custom functions you define.

```python 
# Define a custom function to calculate the range (max - min)
def data_range(series):
    return series.max() - series.min()

# Group by 'continent' and apply the custom 'data_range' function to 'life_exp'
custom_agg_life_exp = gapminder.groupby('continent')['life_exp'].agg(data_range)
print("\nLife Expectancy Range by Continent (Custom Function):\n", custom_agg_life_exp)

# You can also pass a lambda function directly
lambda_agg_gdp = gapminder.groupby('continent')['gdp'].agg(lambda x: x.quantile(0.75) - x.quantile(0.25))
print("\nInterquartile Range of GDP by Continent (Lambda Function):\n", lambda_agg_gdp)

```

<!--#################################################-->

### Pivot Tables Revisited for Aggregation {#pivotTables .unnumbered .unlisted}

You've seen pivot_table used for reshaping. It's also incredibly powerful for aggregation, often providing a more intuitive syntax than `groupby().agg()` for certain types of summaries.

Recall our example of average life expectancy by year and continent:

```python
# Calculate the mean life expectancy using pivot_table
# index: the column(s) to become the new DataFrame index (rows)
# columns: the column(s) whose unique values will become the new DataFrame columns
# values: the column(s) to aggregate
# aggfunc: the aggregation function (default is 'mean')
avg_life_exp_pivot_table = pd.pivot_table(gapminder,
                                          index='year',
                                          columns='continent',
                                          values='life_exp',
                                          aggfunc='mean')
print("\nAverage Life Expectancy by Year and Continent (using pivot_table):\n", avg_life_exp_pivot_table.head())

```

The result is the same as the groupby(['year', 'continent'])['life_exp'].mean() with an `unstack()` operation, but pivot_table often makes the intention clearer when you want a cross-tabulated summary.

You can also apply multiple aggregation functions in pivot_table:

```python 
# Pivot table with multiple aggregation functions for 'life_exp'
multi_agg_pivot_table = pd.pivot_table(gapminder,
                                       index='continent',
                                       values='life_exp',
                                       aggfunc=['mean', 'median', 'std'])
print("\nMultiple Aggregations of Life Expectancy (using pivot_table):\n", multi_agg_pivot_table)

```

Mastering grouping and aggregation allows you to transform large datasets into concise, insightful summaries that are essential for reporting, making decisions, and preparing data for more advanced analytical techniques.

<!--#################################################-->










<!--#################################################-->

# **Statistical Inference**

<!--#################################################-->

::: {.callout-tip collapse=false}
## Contents
- [Introduction to Statistical Inference](#intro)
- [Probability Distributions](#probdist)
- [Normal (Gaussian) Distribution](#Normal)
- [Estimation](#Estimation)
- [Confidence Intervals](#confidenceIntervals)
- [Hypothesis Testing](#HypTesting)

:::


## Introduction to Statistical Inference {#intro}

This section will introduce the core ideas behind statistical inference, explaining how we move from simply describing data to making informed judgments about larger populations.

**What is Statistical Inference?**
- The goal is to predict and forecast values of population parameters, test hypotheses about them, and make decisions based on sample statistics derived from limited and incomplete sample information.
- Population vs. Sample: Descriptive statistics describe the whole population (its properties are called parameters, e.g., μ, σ). Inferential statistics use a random sample to infer something about the population; properties calculated from samples are called sample statistics (estimators, e.g. $\bar{x}$, s), which are used to estimate population parameters. Different samples will typically have different statistics.


<!--#################################################-->

## Probability Distributions {#probdist}
Probability distributions are fundamental to statistical inference. They describe how data is spread and form the basis for many statistical tests.

**What are Probability Distributions?**
- A probability distribution is an expression that describes the relative frequency (height of the curve) for every possible outcome or a range of outcomes.
- A probability model (e.g., the Normal distribution) attempts to capture the essential structure of the real world by asking what the world might look like if an infinite number of observations were obtained.
- The main purpose of a distribution is to indicate error, not accuracy; no single observation is a perfect example of generality.

<!--#################################################-->

## Normal (Gaussian) Distribution {#Normal}

- Properties: Bell-shaped curve.
- Standard Normal Distribution (Z-distribution): A special case where the mean (μ) is 0 and the standard deviation (σ) is 1, as shown below.

![](Images/normal_or_z_dist.png)

- Z-score: Represents the number of standard deviations an observation falls above or below the mean, hense Z = (observation−mean)/SD

![](Images/zScoreFormula.png)

- Approximately 95% of observations fall within ±1.96 standard deviations of the mean in a Normal distribution. Observations with a Z-score > 2 or < -2 are usually considered unusual.


![](Images/normalDist_with_CI.jpg)

- Caution: Not all continuous variables are normally distributed; some are skewed, and the Normal distribution allows for negative values, which may not be appropriate for all real-world data.

The Normal is just 1 distributions, there are many such distributions including Uniform, Binomial, Exponential, and Log-Normal distributions (But we will not be looking at any of these other distributions)

<!--#################################################-->


## Confidence Intervals {#confidenceIntervals}

This section focuses on interval estimation, providing a range within which the true population parameter is likely to fall, along with a specified level of confidence.

**What is a Confidence Interval?**
- Confidence Intervals: Provide a range within which the true population parameter is likely to fall.
- Confidence Level: Typically 90%, 95%, or 99% (e.g., "We are 95% confident that the true population mean lies within this interval").
- Confidence level + signficiance level = 100%
- So if we have a 95% confidence level, we can say we have a significance level a 5% OR a significance level of 2.5% in each tail.

![](Images/sigLevels.png)

**Constructing Confidence Intervals for Means:**

- **Distribution:** It uses the **t-distribution** with degrees of freedom (df) equal to $n-1$.
- **Conditions:** The conditions for using this method are:
    * The sample size ($n$) is greater than or equal to 30 ($n \ge 30$).
    * **OR** the data is approximately normally distributed.
- **Standard Error (SE):** The standard error for the mean is calculated as:


$$ \text{SE} = \sqrt{\frac{s^2}{n}} = \frac{s}{\sqrt{n}} $$
  
  
  where $s$ is the sample standard deviation and $n$ is the sample size.


**Constructing Confidence Intervals for Proportions:**

- **Distribution:** It typically uses the **Normal distribution**.
- **Conditions:** The conditions for using this method (often called the "success-failure condition") are met when:
    * The number of expected "successes" ($np$) is at least 10 ($np \ge 10$).
    * **AND** the number of expected "failures" ($n(1-p)$) is also at least 10 ($n(1-p) \ge 10$).
    * *Note: In practice, we often use the sample proportion $\hat{p}$ instead of the true proportion $p$ to check these conditions, i.e., $n\hat{p} \ge 10$ and $n(1-\hat{p}) \ge 10$.*
- **Standard Error (SE):** The standard error for the proportion is calculated using the sample proportion ($\hat{p}$):


$$ \text{SE} = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} $$
    

where $\hat{p}$ is the sample proportion and $n$ is the sample size.



**Bootstrapping**
Bootstrap Estimation: Can be used for confidence intervals. The process involves specifying variables, generating data (via bootstrapping), calculating a statistic, and visualizing. In Python, this is achieved using NumPy for data manipulation and random sampling, SciPy's bootstrap function for efficient resampling and confidence interval calculation, and Matplotlib or Seaborn for visualizing the bootstrap distribution.


<!--#################################################-->

## Hypothesis Testing {#HypTesting}

This is the core of statistical inference, where we formally test claims or assumptions about a population using evidence from our sample.

- **Null Hypothesis ($H_0$):** This is a statement of no effect or no difference. It represents a model of the world under the assumption that the effect we are investigating is not real (e.g., there is no difference between two groups, or a new drug has no effect). We assume the null hypothesis is true until we find sufficient evidence to the contrary.

- **Alternative Hypothesis ($H_A$ or $H_1$):** This is the claim we are trying to find evidence for. It proposes that there *is* an effect or a difference (e.g., there *is* a difference between groups, or the new drug *does* have an effect).

- **Significance Level ($\alpha$):** This is the predetermined threshold for how improbable the observed data must be, assuming the null hypothesis is true, for us to reject the null hypothesis. It represents the maximum probability of making a Type I error that we are willing to accept.
    * Commonly set at 5% (0.05).
    * Can be set lower, such as 1% (0.01) or less, particularly in fields where false positives are costly (e.g., medicine, particle physics).
    * This concept is analogous to the legal principle of "beyond reasonable doubt" in a court of law.

- **p-value:** The p-value is the probability of observing data as extreme as, or more extreme than, what was actually observed, *assuming that the null hypothesis ($H_0$) is true*. It quantifies the strength of evidence against the null hypothesis.


### Interpreting the p-value

- **If the p-value < $\alpha$:** This means the observed data is highly improbable if the null hypothesis were true. We consider this a "surprising" effect, suggesting that the data is not just random noise. In this case, we **reject the null hypothesis ($H_0$)**. This implies there is statistically significant evidence to support the alternative hypothesis.

- **If the p-value > $\alpha$:** This means the observed data is *not* surprising under the assumption that the null hypothesis is true. The data is consistent with random variation. In this case, we **fail to reject the null hypothesis ($H_0$)**. This means there isn't enough statistically significant evidence to support the alternative hypothesis.


### Types of Errors

When making a decision about the null hypothesis, there's always a possibility of making an error:

- **Type I Error ($\alpha$):** This occurs when we **reject a true null hypothesis**. It's a "false positive" – concluding there's an effect when there isn't one. The significance level ($\alpha$) directly controls the probability of making a Type I error.

- **Type II Error ($\beta$):** This occurs when we **fail to reject a false null hypothesis**. It's a "false negative" – concluding there's no effect when there actually is one.


### Drawing Conclusions

In hypothesis testing, our conclusions are precise:

- We either **Reject $H_0$** or **Fail to Reject $H_0$**.
- It's crucial to avoid language that implies absolute proof or disproof. For example, we should never say, "The person is innocent," but rather, "There is not sufficient evidence to show that the person is guilty." Similarly, we don't "accept" the null hypothesis; we simply state that we "fail to reject" it.



### Tests for Comparing Groups

When comparing two independent groups, different statistical tests and formulas for standard error are used based on whether you're comparing proportions or means.


#### Difference in Means

- **Distribution:** Uses the **t-distribution**.
- **Degrees of Freedom (df):** The degrees of freedom are conservatively estimated as the smaller of $n_1-1$ or $n_2-1$. More precise methods (like Welch's t-test) use a more complex formula for df.
- **Conditions:**
    * $n_1 \ge 30$ or data from group 1 is approximately normal.
    * **AND** $n_2 \ge 30$ or data from group 2 is approximately normal.
- **Standard Error (SE):** The standard error for the difference in means is:

$$ \text{SE} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}} $$

where $s_1^2$ and $s_2^2$ are the sample variances for group 1 and group 2, respectively, and $n_1$ and $n_2$ are their respective sample sizes.


#### Difference in Proportions

- **Distribution:** Uses the **Normal distribution**.
- **Conditions:** All counts for both groups must be at least 10. That is, for Group 1: $n_1\hat{p}_1 \ge 10$ and $n_1(1-\hat{p}_1) \ge 10$; and for Group 2: $n_2\hat{p}_2 \ge 10$ and $n_2(1-\hat{p}_2) \ge 10$.
- **Standard Error (SE):** The standard error for the difference in proportions is:

$$ \text{SE} = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}} $$

where $\hat{p}_1$ and $\hat{p}_2$ are the sample proportions for group 1 and group 2, respectively, and $n_1$ and $n_2$ are their respective sample sizes.



<!--#################################################-->


# **Data Modelling**
